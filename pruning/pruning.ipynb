{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sparseml-nightly\n",
      "Version: 1.6.0.20231105\n",
      "Summary: Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models\n",
      "Home-page: https://github.com/neuralmagic/sparseml\n",
      "Author: Neuralmagic, Inc.\n",
      "Author-email: support@neuralmagic.com\n",
      "License: Apache\n",
      "Location: /home/rshaw/zephyr-training/sparseml-env/lib/python3.10/site-packages\n",
      "Requires: click, GPUtil, ipywidgets, jupyter, matplotlib, merge-args, numpy, onnx, packaging, pandas, progressbar2, protobuf, psutil, pydantic, pyyaml, requests, scikit-image, scikit-learn, scipy, setuptools, sparsezoo-nightly, toposort, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show sparseml-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Confirm Model Is Working Okay**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_id = \"HuggingFaceH4/mistral-7b-sft-beta\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset_id = \"HuggingFaceH4/ultrachat_200k\"\n",
    "# dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_template = tokenizer.apply_chat_template(dataset[\"train_sft\"][0][\"messages\"][:-1], tokenize=False, add_generation_prompt=True)\n",
    "# print(chat_template)\n",
    "\n",
    "# tokens = tokenizer(chat_template, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model.generate(**tokens, max_new_tokens=20)\n",
    "# tokenizer.batch_decode(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Construct Calib Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 17:58:34 sparseml.transformers WARNING  ****************************************************************\n",
      "WARNING: It appears that the Neural Magic fork of Transformers is not installed!\n",
      "This is CRITICAL for the proper application of quantization in SparseML flows.\n",
      "\n",
      "To resolve this, please run: `pip uninstall transformers;pip install nm-transformers`\n",
      "Failing to do so is UNSUPPORTED and may significantly affect model performance.\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from sparseml.transformers.data.base_llm import TransformersDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "system_prompt = {\n",
    "    \"content\": \"You are a friendly chatbot\",\n",
    "    \"role\": \"system\"\n",
    "}\n",
    "\n",
    "class ChatDataset(TransformersDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        seqlen,\n",
    "        nsamples,\n",
    "        path,\n",
    "        seed: int = 0,\n",
    "        split: str = \"train_sft\",\n",
    "        split_percent_to_use: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            seqlen=seqlen,\n",
    "            nsamples=nsamples,\n",
    "            path=path,\n",
    "            name=None,\n",
    "            seed=seed,\n",
    "            split=split,\n",
    "            use_max_tokens=False,\n",
    "            split_percent_to_use=split_percent_to_use,\n",
    "        )\n",
    "\n",
    "        tok = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "        processed_data = []\n",
    "        for sample in tqdm.tqdm(self._data):\n",
    "            assert \"messages\" in sample\n",
    "            messages_with_sys_prompt = [system_prompt] + sample[\"messages\"]\n",
    "            processed_data.append(\n",
    "                tok.apply_chat_template(\n",
    "                    messages_with_sys_prompt, \n",
    "                    tokenize=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # print(processed_data[-1])\n",
    "            \n",
    "        self.create_dataloader(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceH4/mistral-7b-sft-beta\"\n",
    "dataset_id = \"HuggingFaceH4/ultrachat_200k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-training/sparseml-env/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "100%|██████████| 512/512 [00:00<00:00, 12980.20it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = ChatDataset(\n",
    "    model=model_id,\n",
    "    seqlen=512,\n",
    "    nsamples=512,\n",
    "    path=dataset_id\n",
    ")\n",
    "\n",
    "calibration_data = dataset.loader\n",
    "tokenizer = dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fd91d841134e239f3324993ca7cca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "print(model.dtype)\n",
    "print(model.device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "SKIPPING MODEL.to(device)\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 17:59:29 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 1/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 17:59:44 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 0\n",
      "2023-11-06 17:59:46 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.31\n",
      "2023-11-06 17:59:46 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.05\n",
      "2023-11-06 18:00:00 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 0\n",
      "2023-11-06 18:00:01 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.24\n",
      "2023-11-06 18:00:01 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.12\n",
      "2023-11-06 18:00:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 0\n",
      "2023-11-06 18:00:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.26\n",
      "2023-11-06 18:00:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.30\n",
      "2023-11-06 18:00:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 0\n",
      "2023-11-06 18:00:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.28\n",
      "2023-11-06 18:00:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.07\n",
      "2023-11-06 18:00:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 0\n",
      "2023-11-06 18:00:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.31\n",
      "2023-11-06 18:00:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 533.56\n",
      "2023-11-06 18:01:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 0\n",
      "2023-11-06 18:01:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.31\n",
      "2023-11-06 18:01:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 482.15\n",
      "2023-11-06 18:01:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 0\n",
      "2023-11-06 18:01:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.67\n",
      "2023-11-06 18:01:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.16\n",
      "2023-11-06 18:02:00 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 2/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457424GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.798828GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:02:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 1\n",
      "2023-11-06 18:02:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.31\n",
      "2023-11-06 18:02:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 30.10\n",
      "2023-11-06 18:02:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 1\n",
      "2023-11-06 18:02:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.31\n",
      "2023-11-06 18:02:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 9.00\n",
      "2023-11-06 18:02:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 1\n",
      "2023-11-06 18:02:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:02:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5.26\n",
      "2023-11-06 18:03:11 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 1\n",
      "2023-11-06 18:03:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:03:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.28\n",
      "2023-11-06 18:03:29 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 1\n",
      "2023-11-06 18:03:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:03:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1690.74\n",
      "2023-11-06 18:03:47 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 1\n",
      "2023-11-06 18:03:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:03:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1497.88\n",
      "2023-11-06 18:04:18 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 1\n",
      "2023-11-06 18:04:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:04:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 57.27\n",
      "2023-11-06 18:04:40 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 3/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:04:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 2\n",
      "2023-11-06 18:04:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:04:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2555.46\n",
      "2023-11-06 18:05:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 2\n",
      "2023-11-06 18:05:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:05:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 901.78\n",
      "2023-11-06 18:05:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 2\n",
      "2023-11-06 18:05:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:05:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 210.92\n",
      "2023-11-06 18:05:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 2\n",
      "2023-11-06 18:05:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:05:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.35\n",
      "2023-11-06 18:06:09 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 2\n",
      "2023-11-06 18:06:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 18:06:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3687.54\n",
      "2023-11-06 18:06:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 2\n",
      "2023-11-06 18:06:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 18:06:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3264.32\n",
      "2023-11-06 18:06:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 2\n",
      "2023-11-06 18:07:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.65\n",
      "2023-11-06 18:07:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1.08\n",
      "2023-11-06 18:07:20 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 4/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:07:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 3\n",
      "2023-11-06 18:07:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:07:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2140.45\n",
      "2023-11-06 18:07:56 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 3\n",
      "2023-11-06 18:07:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:07:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 771.81\n",
      "2023-11-06 18:08:14 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 3\n",
      "2023-11-06 18:08:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:08:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 234.46\n",
      "2023-11-06 18:08:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 3\n",
      "2023-11-06 18:08:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:08:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.78\n",
      "2023-11-06 18:08:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 3\n",
      "2023-11-06 18:08:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:08:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5771.44\n",
      "2023-11-06 18:09:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 3\n",
      "2023-11-06 18:09:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:09:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5023.25\n",
      "2023-11-06 18:09:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 3\n",
      "2023-11-06 18:09:44 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 18:09:44 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2.06\n",
      "2023-11-06 18:10:01 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 5/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:10:18 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 4\n",
      "2023-11-06 18:10:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:10:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3627.70\n",
      "2023-11-06 18:10:36 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 4\n",
      "2023-11-06 18:10:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:10:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1286.52\n",
      "2023-11-06 18:10:54 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 4\n",
      "2023-11-06 18:10:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:10:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 440.39\n",
      "2023-11-06 18:11:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 4\n",
      "2023-11-06 18:11:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:11:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1.01\n",
      "2023-11-06 18:11:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 4\n",
      "2023-11-06 18:11:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:11:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 7956.79\n",
      "2023-11-06 18:11:49 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 4\n",
      "2023-11-06 18:11:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:11:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6552.94\n",
      "2023-11-06 18:12:19 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 4\n",
      "2023-11-06 18:12:24 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 18:12:24 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3.82\n",
      "2023-11-06 18:12:41 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 6/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:12:58 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 5\n",
      "2023-11-06 18:12:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:12:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5885.69\n",
      "2023-11-06 18:13:16 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 5\n",
      "2023-11-06 18:13:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:13:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2182.52\n",
      "2023-11-06 18:13:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 5\n",
      "2023-11-06 18:13:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:13:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 586.41\n",
      "2023-11-06 18:13:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 5\n",
      "2023-11-06 18:13:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:13:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2.66\n",
      "2023-11-06 18:14:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 5\n",
      "2023-11-06 18:14:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:14:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 11329.21\n",
      "2023-11-06 18:14:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 5\n",
      "2023-11-06 18:14:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:14:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 8771.77\n",
      "2023-11-06 18:14:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 5\n",
      "2023-11-06 18:15:04 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:15:04 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6.88\n",
      "2023-11-06 18:15:21 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 7/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:15:38 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 6\n",
      "2023-11-06 18:15:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:15:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4999.64\n",
      "2023-11-06 18:15:56 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 6\n",
      "2023-11-06 18:15:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:15:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1878.91\n",
      "2023-11-06 18:16:14 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 6\n",
      "2023-11-06 18:16:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:16:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 557.31\n",
      "2023-11-06 18:16:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 6\n",
      "2023-11-06 18:16:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:16:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2.85\n",
      "2023-11-06 18:16:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 6\n",
      "2023-11-06 18:16:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:16:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 13286.23\n",
      "2023-11-06 18:17:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 6\n",
      "2023-11-06 18:17:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:17:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 10427.98\n",
      "2023-11-06 18:17:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 6\n",
      "2023-11-06 18:17:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 18:17:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 10.13\n",
      "2023-11-06 18:18:00 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 8/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:18:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 7\n",
      "2023-11-06 18:18:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:18:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5964.08\n",
      "2023-11-06 18:18:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 7\n",
      "2023-11-06 18:18:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:18:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2250.06\n",
      "2023-11-06 18:18:54 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 7\n",
      "2023-11-06 18:18:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:18:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 707.32\n",
      "2023-11-06 18:19:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 7\n",
      "2023-11-06 18:19:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:19:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5.04\n",
      "2023-11-06 18:19:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 7\n",
      "2023-11-06 18:19:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:19:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 16235.30\n",
      "2023-11-06 18:19:48 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 7\n",
      "2023-11-06 18:19:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:19:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 12512.98\n",
      "2023-11-06 18:20:19 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 7\n",
      "2023-11-06 18:20:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:20:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 13.59\n",
      "2023-11-06 18:20:40 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 9/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:20:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 8\n",
      "2023-11-06 18:20:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:20:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6425.66\n",
      "2023-11-06 18:21:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 8\n",
      "2023-11-06 18:21:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:21:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2361.46\n",
      "2023-11-06 18:21:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 8\n",
      "2023-11-06 18:21:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:21:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 751.06\n",
      "2023-11-06 18:21:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 8\n",
      "2023-11-06 18:21:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:21:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5.81\n",
      "2023-11-06 18:22:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 8\n",
      "2023-11-06 18:22:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:22:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17331.82\n",
      "2023-11-06 18:22:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 8\n",
      "2023-11-06 18:22:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:22:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 13765.79\n",
      "2023-11-06 18:22:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 8\n",
      "2023-11-06 18:23:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:23:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 16.21\n",
      "2023-11-06 18:23:20 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 10/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:23:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 9\n",
      "2023-11-06 18:23:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:23:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 8542.04\n",
      "2023-11-06 18:23:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 9\n",
      "2023-11-06 18:23:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:23:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3270.70\n",
      "2023-11-06 18:24:13 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 9\n",
      "2023-11-06 18:24:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:24:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 995.81\n",
      "2023-11-06 18:24:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 9\n",
      "2023-11-06 18:24:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:24:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 7.31\n",
      "2023-11-06 18:24:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 9\n",
      "2023-11-06 18:24:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:24:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17979.39\n",
      "2023-11-06 18:25:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 9\n",
      "2023-11-06 18:25:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:25:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 14885.97\n",
      "2023-11-06 18:25:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 9\n",
      "2023-11-06 18:25:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.60\n",
      "2023-11-06 18:25:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 19.36\n",
      "2023-11-06 18:26:00 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 11/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:26:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 10\n",
      "2023-11-06 18:26:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:26:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 7801.41\n",
      "2023-11-06 18:26:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 10\n",
      "2023-11-06 18:26:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:26:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3253.01\n",
      "2023-11-06 18:26:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 10\n",
      "2023-11-06 18:26:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:26:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 826.11\n",
      "2023-11-06 18:27:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 10\n",
      "2023-11-06 18:27:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:27:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 9.96\n",
      "2023-11-06 18:27:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 10\n",
      "2023-11-06 18:27:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:27:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17523.68\n",
      "2023-11-06 18:27:48 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 10\n",
      "2023-11-06 18:27:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.39\n",
      "2023-11-06 18:27:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 14970.51\n",
      "2023-11-06 18:28:19 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 10\n",
      "2023-11-06 18:28:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 18:28:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 21.18\n",
      "2023-11-06 18:28:40 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 12/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:28:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 11\n",
      "2023-11-06 18:28:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:28:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 8717.14\n",
      "2023-11-06 18:29:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 11\n",
      "2023-11-06 18:29:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:29:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3448.88\n",
      "2023-11-06 18:29:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 11\n",
      "2023-11-06 18:29:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:29:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1139.76\n",
      "2023-11-06 18:29:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 11\n",
      "2023-11-06 18:29:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:29:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 14.23\n",
      "2023-11-06 18:30:09 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 11\n",
      "2023-11-06 18:30:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:30:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 18250.99\n",
      "2023-11-06 18:30:27 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 11\n",
      "2023-11-06 18:30:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:30:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 16092.48\n",
      "2023-11-06 18:30:58 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 11\n",
      "2023-11-06 18:31:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:31:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 24.49\n",
      "2023-11-06 18:31:19 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 13/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:31:36 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 12\n",
      "2023-11-06 18:31:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:31:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 9960.01\n",
      "2023-11-06 18:31:54 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 12\n",
      "2023-11-06 18:31:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:31:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3558.50\n",
      "2023-11-06 18:32:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 12\n",
      "2023-11-06 18:32:14 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:32:14 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1244.84\n",
      "2023-11-06 18:32:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 12\n",
      "2023-11-06 18:32:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:32:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 16.67\n",
      "2023-11-06 18:32:48 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 12\n",
      "2023-11-06 18:32:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:32:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 20568.92\n",
      "2023-11-06 18:33:07 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 12\n",
      "2023-11-06 18:33:08 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:33:08 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 18550.61\n",
      "2023-11-06 18:33:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 12\n",
      "2023-11-06 18:33:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:33:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 29.92\n",
      "2023-11-06 18:33:59 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 14/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:34:16 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 13\n",
      "2023-11-06 18:34:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:34:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 11112.79\n",
      "2023-11-06 18:34:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 13\n",
      "2023-11-06 18:34:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:34:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4720.46\n",
      "2023-11-06 18:34:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 13\n",
      "2023-11-06 18:34:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:34:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1435.06\n",
      "2023-11-06 18:35:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 13\n",
      "2023-11-06 18:35:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:35:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 20.89\n",
      "2023-11-06 18:35:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 13\n",
      "2023-11-06 18:35:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:35:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 22420.46\n",
      "2023-11-06 18:35:46 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 13\n",
      "2023-11-06 18:35:47 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:35:47 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 20954.68\n",
      "2023-11-06 18:36:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 13\n",
      "2023-11-06 18:36:22 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.66\n",
      "2023-11-06 18:36:22 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 36.82\n",
      "2023-11-06 18:36:38 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 15/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:36:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 14\n",
      "2023-11-06 18:36:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:36:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 10507.15\n",
      "2023-11-06 18:37:13 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 14\n",
      "2023-11-06 18:37:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:37:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3792.59\n",
      "2023-11-06 18:37:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 14\n",
      "2023-11-06 18:37:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:37:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2013.50\n",
      "2023-11-06 18:37:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 14\n",
      "2023-11-06 18:37:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:37:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 28.07\n",
      "2023-11-06 18:38:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 14\n",
      "2023-11-06 18:38:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.35\n",
      "2023-11-06 18:38:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 24259.43\n",
      "2023-11-06 18:38:26 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 14\n",
      "2023-11-06 18:38:27 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:38:27 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 23219.50\n",
      "2023-11-06 18:38:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 14\n",
      "2023-11-06 18:39:02 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 18:39:02 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 49.46\n",
      "2023-11-06 18:39:18 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 16/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:39:36 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 15\n",
      "2023-11-06 18:39:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:39:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 13886.56\n",
      "2023-11-06 18:39:54 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 15\n",
      "2023-11-06 18:39:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:39:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4490.59\n",
      "2023-11-06 18:40:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 15\n",
      "2023-11-06 18:40:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:40:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2519.37\n",
      "2023-11-06 18:40:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 15\n",
      "2023-11-06 18:40:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:40:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 38.07\n",
      "2023-11-06 18:40:48 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 15\n",
      "2023-11-06 18:40:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.37\n",
      "2023-11-06 18:40:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 29798.51\n",
      "2023-11-06 18:41:06 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 15\n",
      "2023-11-06 18:41:08 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 18:41:08 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 27489.35\n",
      "2023-11-06 18:41:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 15\n",
      "2023-11-06 18:41:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 18:41:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 68.47\n",
      "2023-11-06 18:41:59 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 17/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:42:16 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 16\n",
      "2023-11-06 18:42:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:42:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 15176.93\n",
      "2023-11-06 18:42:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 16\n",
      "2023-11-06 18:42:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:42:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5555.56\n",
      "2023-11-06 18:42:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 16\n",
      "2023-11-06 18:42:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:42:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2694.75\n",
      "2023-11-06 18:43:11 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 16\n",
      "2023-11-06 18:43:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:43:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 41.64\n",
      "2023-11-06 18:43:29 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 16\n",
      "2023-11-06 18:43:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:43:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 36975.43\n",
      "2023-11-06 18:43:47 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 16\n",
      "2023-11-06 18:43:48 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 18:43:48 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 32969.61\n",
      "2023-11-06 18:44:18 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 16\n",
      "2023-11-06 18:44:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.65\n",
      "2023-11-06 18:44:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 97.75\n",
      "2023-11-06 18:44:39 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 18/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:44:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 17\n",
      "2023-11-06 18:44:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:44:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 13043.64\n",
      "2023-11-06 18:45:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 17\n",
      "2023-11-06 18:45:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:45:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4639.60\n",
      "2023-11-06 18:45:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 17\n",
      "2023-11-06 18:45:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:45:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2200.51\n",
      "2023-11-06 18:45:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 17\n",
      "2023-11-06 18:45:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:45:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 44.39\n",
      "2023-11-06 18:46:09 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 17\n",
      "2023-11-06 18:46:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:46:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 43761.37\n",
      "2023-11-06 18:46:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 17\n",
      "2023-11-06 18:46:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:46:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 39205.86\n",
      "2023-11-06 18:46:58 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 17\n",
      "2023-11-06 18:47:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 18:47:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 129.72\n",
      "2023-11-06 18:47:20 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 19/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:47:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 18\n",
      "2023-11-06 18:47:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:47:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 15093.23\n",
      "2023-11-06 18:47:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 18\n",
      "2023-11-06 18:47:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:47:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4884.28\n",
      "2023-11-06 18:48:13 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 18\n",
      "2023-11-06 18:48:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:48:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2700.34\n",
      "2023-11-06 18:48:31 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 18\n",
      "2023-11-06 18:48:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:48:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 50.46\n",
      "2023-11-06 18:48:49 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 18\n",
      "2023-11-06 18:48:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:48:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 50902.71\n",
      "2023-11-06 18:49:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 18\n",
      "2023-11-06 18:49:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.39\n",
      "2023-11-06 18:49:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 45465.19\n",
      "2023-11-06 18:49:38 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 18\n",
      "2023-11-06 18:49:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.67\n",
      "2023-11-06 18:49:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 185.88\n",
      "2023-11-06 18:50:00 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 20/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:50:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 19\n",
      "2023-11-06 18:50:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:50:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17659.00\n",
      "2023-11-06 18:50:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 19\n",
      "2023-11-06 18:50:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:50:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5346.69\n",
      "2023-11-06 18:50:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 19\n",
      "2023-11-06 18:50:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:50:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3621.70\n",
      "2023-11-06 18:51:11 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 19\n",
      "2023-11-06 18:51:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:51:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 61.98\n",
      "2023-11-06 18:51:29 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 19\n",
      "2023-11-06 18:51:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:51:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 60886.28\n",
      "2023-11-06 18:51:48 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 19\n",
      "2023-11-06 18:51:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:51:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 52607.35\n",
      "2023-11-06 18:52:18 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 19\n",
      "2023-11-06 18:52:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.60\n",
      "2023-11-06 18:52:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 241.54\n",
      "2023-11-06 18:52:40 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 21/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:52:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 20\n",
      "2023-11-06 18:52:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:52:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17734.67\n",
      "2023-11-06 18:53:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 20\n",
      "2023-11-06 18:53:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:53:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5696.65\n",
      "2023-11-06 18:53:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 20\n",
      "2023-11-06 18:53:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:53:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3875.21\n",
      "2023-11-06 18:53:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 20\n",
      "2023-11-06 18:53:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:53:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 70.42\n",
      "2023-11-06 18:54:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 20\n",
      "2023-11-06 18:54:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.35\n",
      "2023-11-06 18:54:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 73768.74\n",
      "2023-11-06 18:54:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 20\n",
      "2023-11-06 18:54:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:54:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 60970.01\n",
      "2023-11-06 18:54:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 20\n",
      "2023-11-06 18:55:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:55:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 267.35\n",
      "2023-11-06 18:55:20 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 22/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:55:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 21\n",
      "2023-11-06 18:55:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:55:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 18332.68\n",
      "2023-11-06 18:55:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 21\n",
      "2023-11-06 18:55:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:55:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5851.79\n",
      "2023-11-06 18:56:14 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 21\n",
      "2023-11-06 18:56:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:56:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4031.51\n",
      "2023-11-06 18:56:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 21\n",
      "2023-11-06 18:56:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:56:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 64.39\n",
      "2023-11-06 18:56:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 21\n",
      "2023-11-06 18:56:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:56:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 90413.82\n",
      "2023-11-06 18:57:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 21\n",
      "2023-11-06 18:57:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:57:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 68921.45\n",
      "2023-11-06 18:57:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 21\n",
      "2023-11-06 18:57:44 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:57:44 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 270.28\n",
      "2023-11-06 18:58:01 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 23/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:58:18 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 22\n",
      "2023-11-06 18:58:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:58:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17970.44\n",
      "2023-11-06 18:58:36 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 22\n",
      "2023-11-06 18:58:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:58:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5665.70\n",
      "2023-11-06 18:58:54 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 22\n",
      "2023-11-06 18:58:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:58:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4354.91\n",
      "2023-11-06 18:59:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 22\n",
      "2023-11-06 18:59:14 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:59:14 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 50.83\n",
      "2023-11-06 18:59:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 22\n",
      "2023-11-06 18:59:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:59:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 96103.84\n",
      "2023-11-06 18:59:49 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 22\n",
      "2023-11-06 18:59:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:59:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 73440.88\n",
      "2023-11-06 19:00:20 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 22\n",
      "2023-11-06 19:00:24 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 19:00:24 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 276.70\n",
      "2023-11-06 19:00:41 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 24/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:00:58 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 23\n",
      "2023-11-06 19:01:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:01:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 19067.62\n",
      "2023-11-06 19:01:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 23\n",
      "2023-11-06 19:01:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:01:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5819.25\n",
      "2023-11-06 19:01:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 23\n",
      "2023-11-06 19:01:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:01:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4767.92\n",
      "2023-11-06 19:01:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 23\n",
      "2023-11-06 19:01:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:01:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 63.64\n",
      "2023-11-06 19:02:11 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 23\n",
      "2023-11-06 19:02:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:02:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 105023.81\n",
      "2023-11-06 19:02:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 23\n",
      "2023-11-06 19:02:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:02:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 79324.34\n",
      "2023-11-06 19:03:00 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 23\n",
      "2023-11-06 19:03:05 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 19:03:05 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 294.23\n",
      "2023-11-06 19:03:22 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 25/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:03:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 24\n",
      "2023-11-06 19:03:40 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:03:40 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 21455.66\n",
      "2023-11-06 19:03:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 24\n",
      "2023-11-06 19:03:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:03:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6989.91\n",
      "2023-11-06 19:04:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 24\n",
      "2023-11-06 19:04:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:04:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5740.30\n",
      "2023-11-06 19:04:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 24\n",
      "2023-11-06 19:04:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:04:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 56.76\n",
      "2023-11-06 19:04:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 24\n",
      "2023-11-06 19:04:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 19:04:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 116287.05\n",
      "2023-11-06 19:05:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 24\n",
      "2023-11-06 19:05:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:05:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 85956.28\n",
      "2023-11-06 19:05:41 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 24\n",
      "2023-11-06 19:05:45 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 19:05:45 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 305.55\n",
      "2023-11-06 19:06:02 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 26/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:06:19 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 25\n",
      "2023-11-06 19:06:21 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:06:21 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 22018.10\n",
      "2023-11-06 19:06:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 25\n",
      "2023-11-06 19:06:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:06:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6854.61\n",
      "2023-11-06 19:06:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 25\n",
      "2023-11-06 19:06:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:06:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6303.19\n",
      "2023-11-06 19:07:14 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 25\n",
      "2023-11-06 19:07:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:07:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 57.67\n",
      "2023-11-06 19:07:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 25\n",
      "2023-11-06 19:07:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:07:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 123195.44\n",
      "2023-11-06 19:07:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 25\n",
      "2023-11-06 19:07:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:07:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 92209.66\n",
      "2023-11-06 19:08:21 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 25\n",
      "2023-11-06 19:08:25 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.60\n",
      "2023-11-06 19:08:25 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 324.52\n",
      "2023-11-06 19:08:42 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 27/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:08:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 26\n",
      "2023-11-06 19:09:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:09:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 19295.73\n",
      "2023-11-06 19:09:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 26\n",
      "2023-11-06 19:09:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:09:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5761.50\n",
      "2023-11-06 19:09:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 26\n",
      "2023-11-06 19:09:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.35\n",
      "2023-11-06 19:09:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6137.34\n",
      "2023-11-06 19:09:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 26\n",
      "2023-11-06 19:09:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:09:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 73.48\n",
      "2023-11-06 19:10:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 26\n",
      "2023-11-06 19:10:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:10:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 125579.63\n",
      "2023-11-06 19:10:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 26\n",
      "2023-11-06 19:10:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:10:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 97107.73\n",
      "2023-11-06 19:11:01 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 26\n",
      "2023-11-06 19:11:05 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 19:11:05 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 349.29\n",
      "2023-11-06 19:11:22 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 28/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:11:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 27\n",
      "2023-11-06 19:11:41 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:11:41 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 21405.12\n",
      "2023-11-06 19:11:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 27\n",
      "2023-11-06 19:11:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:11:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6950.20\n",
      "2023-11-06 19:12:16 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 27\n",
      "2023-11-06 19:12:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:12:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6213.89\n",
      "2023-11-06 19:12:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 27\n",
      "2023-11-06 19:12:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:12:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 76.34\n",
      "2023-11-06 19:12:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 27\n",
      "2023-11-06 19:12:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:12:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 130929.91\n",
      "2023-11-06 19:13:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 27\n",
      "2023-11-06 19:13:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:13:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 103017.60\n",
      "2023-11-06 19:13:41 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 27\n",
      "2023-11-06 19:13:46 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 19:13:46 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 379.39\n",
      "2023-11-06 19:14:03 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 29/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:14:20 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 28\n",
      "2023-11-06 19:14:21 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:14:21 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 16715.63\n",
      "2023-11-06 19:14:38 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 28\n",
      "2023-11-06 19:14:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:14:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4556.25\n",
      "2023-11-06 19:14:56 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 28\n",
      "2023-11-06 19:14:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:14:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6831.86\n",
      "2023-11-06 19:15:14 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 28\n",
      "2023-11-06 19:15:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:15:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 137.07\n",
      "2023-11-06 19:15:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 28\n",
      "2023-11-06 19:15:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:15:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 133248.09\n",
      "2023-11-06 19:15:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 28\n",
      "2023-11-06 19:15:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:15:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 109924.70\n",
      "2023-11-06 19:16:22 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 28\n",
      "2023-11-06 19:16:26 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 19:16:26 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 485.72\n",
      "2023-11-06 19:16:43 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 30/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:17:00 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 29\n",
      "2023-11-06 19:17:02 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:17:02 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 18767.50\n",
      "2023-11-06 19:17:19 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 29\n",
      "2023-11-06 19:17:20 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:17:20 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5116.62\n",
      "2023-11-06 19:17:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 29\n",
      "2023-11-06 19:17:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:17:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 10216.00\n",
      "2023-11-06 19:17:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 29\n",
      "2023-11-06 19:17:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:17:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 204.98\n",
      "2023-11-06 19:18:13 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 29\n",
      "2023-11-06 19:18:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:18:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 126934.91\n",
      "2023-11-06 19:18:31 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 29\n",
      "2023-11-06 19:18:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:18:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 109136.23\n",
      "2023-11-06 19:19:02 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 29\n",
      "2023-11-06 19:19:07 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 19:19:07 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 662.88\n",
      "2023-11-06 19:19:24 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 31/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:19:41 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 30\n",
      "2023-11-06 19:19:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:19:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 15666.87\n",
      "2023-11-06 19:19:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 30\n",
      "2023-11-06 19:20:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:20:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4018.58\n",
      "2023-11-06 19:20:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 30\n",
      "2023-11-06 19:20:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:20:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 9264.97\n",
      "2023-11-06 19:20:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 30\n",
      "2023-11-06 19:20:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:20:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 241.48\n",
      "2023-11-06 19:20:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 30\n",
      "2023-11-06 19:20:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:20:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 123882.95\n",
      "2023-11-06 19:21:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 30\n",
      "2023-11-06 19:21:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 19:21:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 109285.16\n",
      "2023-11-06 19:21:43 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 30\n",
      "2023-11-06 19:21:47 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 19:21:47 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 794.38\n",
      "2023-11-06 19:22:04 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 32/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:22:21 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 31\n",
      "2023-11-06 19:22:22 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:22:22 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 14604.77\n",
      "2023-11-06 19:22:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 31\n",
      "2023-11-06 19:22:41 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:22:41 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3725.28\n",
      "2023-11-06 19:22:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 31\n",
      "2023-11-06 19:22:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:22:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 9093.79\n",
      "2023-11-06 19:23:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 31\n",
      "2023-11-06 19:23:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:23:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 295.27\n",
      "2023-11-06 19:23:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 31\n",
      "2023-11-06 19:23:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.35\n",
      "2023-11-06 19:23:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 109485.69\n",
      "2023-11-06 19:23:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 31\n",
      "2023-11-06 19:23:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:23:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 92814.92\n",
      "2023-11-06 19:24:22 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 31\n",
      "2023-11-06 19:24:27 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 19:24:27 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 904.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModifiedState(model=MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "), optimizer=None, loss=None, modifier_data=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparseml.core.session as session_manager\n",
    "from sparseml.core.framework import Framework\n",
    "\n",
    "recipe_file = \"/home/rshaw/zephyr-training/pruning/recipe-50sparse.yaml\"\n",
    "\n",
    "session_manager.create_session()\n",
    "session = session_manager.active_session()\n",
    "session.apply(\n",
    "    framework=Framework.pytorch,\n",
    "    recipe=recipe_file,\n",
    "    model=model,\n",
    "    calib_data=calibration_data,\n",
    "    start=0.0,\n",
    "    device=\"cuda\",\n",
    "    copy_data=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-training\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/one-shot/50sparse-sft-v0/tokenizer_config.json',\n",
       " 'data/one-shot/50sparse-sft-v0/special_tokens_map.json',\n",
       " 'data/one-shot/50sparse-sft-v0/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"data/one-shot/50sparse-sft-v0\"\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sparseml.optim.helpers import load_recipe_yaml_str\n",
    "\n",
    "recipe_output_path = os.path.join(save_path, \"recipe.yaml\")\n",
    "with open(recipe_output_path, \"w\") as fp:\n",
    "    fp.write(load_recipe_yaml_str(recipe_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t\t  model-00006-of-00006.safetensors\n",
      "generation_config.json\t\t  model.safetensors.index.json\n",
      "model-00001-of-00006.safetensors  recipe.yaml\n",
      "model-00002-of-00006.safetensors  special_tokens_map.json\n",
      "model-00003-of-00006.safetensors  tokenizer_config.json\n",
      "model-00004-of-00006.safetensors  tokenizer.json\n",
      "model-00005-of-00006.safetensors\n"
     ]
    }
   ],
   "source": [
    "!ls {save_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparseml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
