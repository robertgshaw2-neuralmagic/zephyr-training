{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sparseml-nightly\n",
      "Version: 1.6.0.20231115\n",
      "Summary: Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models\n",
      "Home-page: https://github.com/neuralmagic/sparseml\n",
      "Author: Neuralmagic, Inc.\n",
      "Author-email: support@neuralmagic.com\n",
      "License: Apache\n",
      "Location: /home/paperspace/zephyr-training/env/lib/python3.9/site-packages\n",
      "Requires: click, GPUtil, ipywidgets, jupyter, matplotlib, merge-args, numpy, onnx, packaging, pandas, progressbar2, protobuf, psutil, pydantic, pyyaml, requests, scikit-image, scikit-learn, scipy, setuptools, sparsezoo-nightly, toposort, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show sparseml-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/paperspace/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.01s/it]\n",
      "18\n",
      "Layer                            | range              | mean               | std_dev            | l1_loss_change     | relative_l1_change | num_outliers      \n",
      "lm_head                          | 146.794            | 0.0117355          | 1.65587            | 0.144357           | 1.59412e-08        | 19919             \n",
      "model.layers.30.mlp.down_proj    | 1213.13            | 0.0078115          | 1.00952            | 0.0577134          | 2.95631e-08        | 8785              \n",
      "model.layers.31.mlp.down_proj    | 78.2416            | -0.00153409        | 0.307508           | 0.0404475          | 1.4833e-08         | 143440            \n",
      "model.layers.10.self_attn.q_proj | 44.4328            | -0.00120299        | 0.348746           | 0.0296465          | 2.25408e-08        | 27139             \n",
      "model.layers.10.self_attn.k_proj | 44.4328            | -0.00120299        | 0.348746           | 0.0296465          | 2.25408e-08        | 27139             \n",
      "model.layers.10.self_attn.v_proj | 44.4328            | -0.00120299        | 0.348746           | 0.0296465          | 2.25408e-08        | 27139             \n",
      "model.layers.31.mlp.gate_proj    | 29.1561            | 8.45928e-05        | 0.317061           | 0.028563           | 1.68968e-08        | 14642             \n",
      "model.layers.31.mlp.up_proj      | 29.1561            | 8.45928e-05        | 0.317061           | 0.028563           | 1.68968e-08        | 14642             \n",
      "model.layers.12.self_attn.q_proj | 40.6623            | -0.00127412        | 0.337032           | 0.0280154          | 1.93256e-08        | 30538             \n",
      "model.layers.12.self_attn.k_proj | 40.6623            | -0.00127412        | 0.337032           | 0.0280154          | 1.93256e-08        | 30538             \n",
      "model.layers.12.self_attn.v_proj | 40.6623            | -0.00127412        | 0.337032           | 0.0280154          | 1.93256e-08        | 30538             \n",
      "model.layers.11.self_attn.q_proj | 39.7199            | -0.00101769        | 0.33716            | 0.0272891          | 1.89642e-08        | 29998             \n",
      "model.layers.11.self_attn.k_proj | 39.7199            | -0.00101769        | 0.33716            | 0.0272891          | 1.89642e-08        | 29998             \n",
      "model.layers.11.self_attn.v_proj | 39.7199            | -0.00101769        | 0.33716            | 0.0272891          | 1.89642e-08        | 29998             \n",
      "model.layers.14.self_attn.q_proj | 38.5683            | -0.000724429       | 0.328767           | 0.027173           | 1.85306e-08        | 31575             \n",
      "model.layers.14.self_attn.k_proj | 38.5683            | -0.000724429       | 0.328767           | 0.027173           | 1.85306e-08        | 31575             \n",
      "model.layers.14.self_attn.v_proj | 38.5683            | -0.000724429       | 0.328767           | 0.027173           | 1.85306e-08        | 31575             \n",
      "model.layers.15.self_attn.q_proj | 36.9763            | -0.000961883       | 0.323321           | 0.0262372          | 1.80405e-08        | 32293             \n",
      "model.layers.15.self_attn.k_proj | 36.9763            | -0.000961883       | 0.323321           | 0.0262372          | 1.80405e-08        | 32293             \n",
      "model.layers.15.self_attn.v_proj | 36.9763            | -0.000961883       | 0.323321           | 0.0262372          | 1.80405e-08        | 32293             \n",
      "model.layers.16.self_attn.q_proj | 35.697             | -0.000283547       | 0.321164           | 0.0258249          | 1.72011e-08        | 33052             \n",
      "model.layers.16.self_attn.k_proj | 35.697             | -0.000283547       | 0.321164           | 0.0258249          | 1.72011e-08        | 33052             \n",
      "model.layers.16.self_attn.v_proj | 35.697             | -0.000283547       | 0.321164           | 0.0258249          | 1.72011e-08        | 33052             \n",
      "model.layers.13.self_attn.q_proj | 36.4035            | -0.00133961        | 0.320024           | 0.0256499          | 1.72829e-08        | 32019             \n",
      "model.layers.13.self_attn.k_proj | 36.4035            | -0.00133961        | 0.320024           | 0.0256499          | 1.72829e-08        | 32019             \n",
      "model.layers.13.self_attn.v_proj | 36.4035            | -0.00133961        | 0.320024           | 0.0256499          | 1.72829e-08        | 32019             \n",
      "model.layers.9.self_attn.q_proj  | 36.7483            | -0.00166685        | 0.30967            | 0.025042           | 1.91496e-08        | 31968             \n",
      "model.layers.9.self_attn.k_proj  | 36.7483            | -0.00166685        | 0.30967            | 0.025042           | 1.91496e-08        | 31968             \n",
      "model.layers.9.self_attn.v_proj  | 36.7483            | -0.00166685        | 0.30967            | 0.025042           | 1.91496e-08        | 31968             \n",
      "model.layers.30.mlp.gate_proj    | 29.0514            | 0.00185997         | 0.339893           | 0.0229987          | 1.45863e-08        | 17925             \n",
      "model.layers.30.mlp.up_proj      | 29.0514            | 0.00185997         | 0.339893           | 0.0229987          | 1.45863e-08        | 17925             \n",
      "model.layers.18.self_attn.q_proj | 31.5365            | 0.000976283        | 0.322945           | 0.0228839          | 1.40408e-08        | 34510             \n",
      "model.layers.18.self_attn.k_proj | 31.5365            | 0.000976283        | 0.322945           | 0.0228839          | 1.40408e-08        | 34510             \n",
      "model.layers.18.self_attn.v_proj | 31.5365            | 0.000976283        | 0.322945           | 0.0228839          | 1.40408e-08        | 34510             \n",
      "model.layers.26.self_attn.q_proj | 27.9711            | 0.00217468         | 0.342359           | 0.021658           | 1.21536e-08        | 43275             \n",
      "model.layers.26.self_attn.k_proj | 27.9711            | 0.00217468         | 0.342359           | 0.021658           | 1.21536e-08        | 43275             \n",
      "model.layers.26.self_attn.v_proj | 27.9711            | 0.00217468         | 0.342359           | 0.021658           | 1.21536e-08        | 43275             \n",
      "model.layers.8.self_attn.q_proj  | 31.2396            | -0.000948696       | 0.291578           | 0.0216443          | 1.56748e-08        | 38617             \n",
      "model.layers.8.self_attn.k_proj  | 31.2396            | -0.000948696       | 0.291578           | 0.0216443          | 1.56748e-08        | 38617             \n",
      "model.layers.8.self_attn.v_proj  | 31.2396            | -0.000948696       | 0.291578           | 0.0216443          | 1.56748e-08        | 38617             \n",
      "model.layers.24.self_attn.q_proj | 27.5658            | 0.00166951         | 0.324677           | 0.0209361          | 1.24563e-08        | 43023             \n",
      "model.layers.24.self_attn.k_proj | 27.5658            | 0.00166951         | 0.324677           | 0.0209361          | 1.24563e-08        | 43023             \n",
      "model.layers.24.self_attn.v_proj | 27.5658            | 0.00166951         | 0.324677           | 0.0209361          | 1.24563e-08        | 43023             \n",
      "model.layers.17.self_attn.q_proj | 28.7452            | 0.00117896         | 0.306078           | 0.020915           | 1.35165e-08        | 36178             \n",
      "model.layers.17.self_attn.k_proj | 28.7452            | 0.00117896         | 0.306078           | 0.020915           | 1.35165e-08        | 36178             \n",
      "model.layers.17.self_attn.v_proj | 28.7452            | 0.00117896         | 0.306078           | 0.020915           | 1.35165e-08        | 36178             \n",
      "model.layers.19.self_attn.q_proj | 27.4184            | 0.0010083          | 0.306058           | 0.0205716          | 1.32641e-08        | 38508             \n",
      "model.layers.19.self_attn.k_proj | 27.4184            | 0.0010083          | 0.306058           | 0.0205716          | 1.32641e-08        | 38508             \n",
      "model.layers.19.self_attn.v_proj | 27.4184            | 0.0010083          | 0.306058           | 0.0205716          | 1.32641e-08        | 38508             \n",
      "model.layers.28.mlp.down_proj    | 59.2073            | 6.88066e-05        | 0.103701           | 0.0200977          | 1.84357e-08        | 337638            \n",
      "model.layers.23.self_attn.q_proj | 25.8954            | 0.0014635          | 0.323409           | 0.0199451          | 1.16795e-08        | 44545             \n",
      "model.layers.23.self_attn.k_proj | 25.8954            | 0.0014635          | 0.323409           | 0.0199451          | 1.16795e-08        | 44545             \n",
      "model.layers.23.self_attn.v_proj | 25.8954            | 0.0014635          | 0.323409           | 0.0199451          | 1.16795e-08        | 44545             \n",
      "model.layers.21.self_attn.q_proj | 26.5909            | 0.00107673         | 0.311997           | 0.0198561          | 1.22607e-08        | 41261             \n",
      "model.layers.21.self_attn.k_proj | 26.5909            | 0.00107673         | 0.311997           | 0.0198561          | 1.22607e-08        | 41261             \n",
      "model.layers.21.self_attn.v_proj | 26.5909            | 0.00107673         | 0.311997           | 0.0198561          | 1.22607e-08        | 41261             \n",
      "model.layers.20.self_attn.q_proj | 26.7317            | 0.000755601        | 0.302814           | 0.019674           | 1.28275e-08        | 38733             \n",
      "model.layers.20.self_attn.k_proj | 26.7317            | 0.000755601        | 0.302814           | 0.019674           | 1.28275e-08        | 38733             \n",
      "model.layers.20.self_attn.v_proj | 26.7317            | 0.000755601        | 0.302814           | 0.019674           | 1.28275e-08        | 38733             \n",
      "model.layers.25.self_attn.q_proj | 24.536             | 0.00198504         | 0.338433           | 0.0196254          | 1.08434e-08        | 48144             \n",
      "model.layers.25.self_attn.k_proj | 24.536             | 0.00198504         | 0.338433           | 0.0196254          | 1.08434e-08        | 48144             \n",
      "model.layers.25.self_attn.v_proj | 24.536             | 0.00198504         | 0.338433           | 0.0196254          | 1.08434e-08        | 48144             \n",
      "model.layers.29.mlp.down_proj    | 51.9877            | 0.000449749        | 0.126494           | 0.0195475          | 1.48804e-08        | 369322            \n",
      "model.layers.22.self_attn.q_proj | 25.4331            | 0.00114384         | 0.316135           | 0.0192853          | 1.1883e-08         | 41178             \n",
      "model.layers.22.self_attn.k_proj | 25.4331            | 0.00114384         | 0.316135           | 0.0192853          | 1.1883e-08         | 41178             \n",
      "model.layers.22.self_attn.v_proj | 25.4331            | 0.00114384         | 0.316135           | 0.0192853          | 1.1883e-08         | 41178             \n",
      "model.layers.31.self_attn.q_proj | 21.1501            | 0.00172204         | 0.279453           | 0.0190776          | 1.28802e-08        | 44527             \n",
      "model.layers.31.self_attn.k_proj | 21.1501            | 0.00172204         | 0.279453           | 0.0190776          | 1.28802e-08        | 44527             \n",
      "model.layers.31.self_attn.v_proj | 21.1501            | 0.00172204         | 0.279453           | 0.0190776          | 1.28802e-08        | 44527             \n",
      "model.layers.28.self_attn.q_proj | 22.4344            | 0.00174621         | 0.336892           | 0.0187918          | 1.00364e-08        | 51575             \n",
      "model.layers.28.self_attn.k_proj | 22.4344            | 0.00174621         | 0.336892           | 0.0187918          | 1.00364e-08        | 51575             \n",
      "model.layers.28.self_attn.v_proj | 22.4344            | 0.00174621         | 0.336892           | 0.0187918          | 1.00364e-08        | 51575             \n",
      "model.layers.29.self_attn.q_proj | 21.954             | 0.00168703         | 0.325584           | 0.0187341          | 1.03371e-08        | 52918             \n",
      "model.layers.29.self_attn.k_proj | 21.954             | 0.00168703         | 0.325584           | 0.0187341          | 1.03371e-08        | 52918             \n",
      "model.layers.29.self_attn.v_proj | 21.954             | 0.00168703         | 0.325584           | 0.0187341          | 1.03371e-08        | 52918             \n",
      "model.layers.27.self_attn.q_proj | 21.3625            | 0.00192813         | 0.332524           | 0.017934           | 9.75818e-09        | 50392             \n",
      "model.layers.27.self_attn.k_proj | 21.3625            | 0.00192813         | 0.332524           | 0.017934           | 9.75818e-09        | 50392             \n",
      "model.layers.27.self_attn.v_proj | 21.3625            | 0.00192813         | 0.332524           | 0.017934           | 9.75818e-09        | 50392             \n",
      "model.layers.29.mlp.gate_proj    | 20.7734            | 0.00126884         | 0.298698           | 0.0176573          | 1.12283e-08        | 32916             \n",
      "model.layers.29.mlp.up_proj      | 20.7734            | 0.00126884         | 0.298698           | 0.0176573          | 1.12283e-08        | 32916             \n",
      "model.layers.30.self_attn.q_proj | 19.8144            | 0.00173367         | 0.330734           | 0.0175639          | 9.3368e-09         | 52602             \n",
      "model.layers.30.self_attn.k_proj | 19.8144            | 0.00173367         | 0.330734           | 0.0175639          | 9.3368e-09         | 52602             \n",
      "model.layers.30.self_attn.v_proj | 19.8144            | 0.00173367         | 0.330734           | 0.0175639          | 9.3368e-09         | 52602             \n",
      "model.layers.7.self_attn.q_proj  | 23.043             | -0.00144561        | 0.279478           | 0.0167914          | 1.14845e-08        | 42672             \n",
      "model.layers.7.self_attn.k_proj  | 23.043             | -0.00144561        | 0.279478           | 0.0167914          | 1.14845e-08        | 42672             \n",
      "model.layers.7.self_attn.v_proj  | 23.043             | -0.00144561        | 0.279478           | 0.0167914          | 1.14845e-08        | 42672             \n",
      "model.layers.28.mlp.gate_proj    | 19.175             | 0.00143256         | 0.289615           | 0.0162723          | 1.05955e-08        | 34470             \n",
      "model.layers.28.mlp.up_proj      | 19.175             | 0.00143256         | 0.289615           | 0.0162723          | 1.05955e-08        | 34470             \n",
      "model.layers.27.mlp.down_proj    | 41.8827            | 1.5007e-06         | 0.0964087          | 0.0159556          | 1.68302e-08        | 245148            \n",
      "model.layers.31.self_attn.o_proj | 15.6033            | 0.00273775         | 0.334705           | 0.0152161          | 1.27499e-08        | 65126             \n",
      "model.layers.27.mlp.gate_proj    | 17.5283            | 0.0013098          | 0.278759           | 0.0149383          | 1.00011e-08        | 36970             \n",
      "model.layers.27.mlp.up_proj      | 17.5283            | 0.0013098          | 0.278759           | 0.0149383          | 1.00011e-08        | 36970             \n",
      "model.layers.6.self_attn.q_proj  | 19.7962            | -0.000875671       | 0.276079           | 0.0147156          | 1.01864e-08        | 42389             \n",
      "model.layers.6.self_attn.k_proj  | 19.7962            | -0.000875671       | 0.276079           | 0.0147156          | 1.01864e-08        | 42389             \n",
      "model.layers.6.self_attn.v_proj  | 19.7962            | -0.000875671       | 0.276079           | 0.0147156          | 1.01864e-08        | 42389             \n",
      "model.layers.16.mlp.gate_proj    | 18.3001            | 0.00029105         | 0.218793           | 0.0144397          | 1.31037e-08        | 22020             \n",
      "model.layers.16.mlp.up_proj      | 18.3001            | 0.00029105         | 0.218793           | 0.0144397          | 1.31037e-08        | 22020             \n",
      "model.layers.19.mlp.gate_proj    | 17.0023            | 0.000700265        | 0.230697           | 0.0136162          | 1.13834e-08        | 25961             \n",
      "model.layers.19.mlp.up_proj      | 17.0023            | 0.000700265        | 0.230697           | 0.0136162          | 1.13834e-08        | 25961             \n",
      "model.layers.26.mlp.down_proj    | 33.7279            | -9.60699e-05       | 0.081409           | 0.0136091          | 1.59889e-08        | 265790            \n",
      "model.layers.26.mlp.gate_proj    | 15.4177            | 0.00115383         | 0.265484           | 0.0133009          | 9.21734e-09        | 40082             \n",
      "model.layers.26.mlp.up_proj      | 15.4177            | 0.00115383         | 0.265484           | 0.0133009          | 9.21734e-09        | 40082             \n",
      "model.layers.1.self_attn.q_proj  | 13.9315            | 0.00164305         | 0.148067           | 0.0132286          | 3.1174e-08         | 44580             \n",
      "model.layers.1.self_attn.k_proj  | 13.9315            | 0.00164305         | 0.148067           | 0.0132286          | 3.1174e-08         | 44580             \n",
      "model.layers.1.self_attn.v_proj  | 13.9315            | 0.00164305         | 0.148067           | 0.0132286          | 3.1174e-08         | 44580             \n",
      "model.layers.5.self_attn.q_proj  | 17.7618            | -0.00100023        | 0.228715           | 0.0128889          | 1.12443e-08        | 41700             \n",
      "model.layers.5.self_attn.k_proj  | 17.7618            | -0.00100023        | 0.228715           | 0.0128889          | 1.12443e-08        | 41700             \n",
      "model.layers.5.self_attn.v_proj  | 17.7618            | -0.00100023        | 0.228715           | 0.0128889          | 1.12443e-08        | 41700             \n",
      "model.layers.25.mlp.down_proj    | 32.1221            | 9.79224e-05        | 0.075622           | 0.0125929          | 1.63852e-08        | 250301            \n",
      "model.layers.23.mlp.down_proj    | 32.9918            | -0.000102411       | 0.0698949          | 0.0122801          | 1.74951e-08        | 235894            \n",
      "model.layers.3.self_attn.q_proj  | 16.9885            | -0.000212121       | 0.234459           | 0.0121302          | 1.063e-08          | 39892             \n",
      "model.layers.3.self_attn.k_proj  | 16.9885            | -0.000212121       | 0.234459           | 0.0121302          | 1.063e-08          | 39892             \n",
      "model.layers.3.self_attn.v_proj  | 16.9885            | -0.000212121       | 0.234459           | 0.0121302          | 1.063e-08          | 39892             \n",
      "model.layers.4.self_attn.q_proj  | 17.1935            | -0.000335392       | 0.231166           | 0.0121136          | 1.12299e-08        | 40499             \n",
      "model.layers.4.self_attn.k_proj  | 17.1935            | -0.000335392       | 0.231166           | 0.0121136          | 1.12299e-08        | 40499             \n",
      "model.layers.4.self_attn.v_proj  | 17.1935            | -0.000335392       | 0.231166           | 0.0121136          | 1.12299e-08        | 40499             \n",
      "model.layers.25.mlp.gate_proj    | 13.7034            | 0.00117784         | 0.252414           | 0.0120294          | 8.72178e-09        | 43070             \n",
      "model.layers.25.mlp.up_proj      | 13.7034            | 0.00117784         | 0.252414           | 0.0120294          | 8.72178e-09        | 43070             \n",
      "model.layers.19.mlp.down_proj    | 30.889             | 0.000528318        | 0.100826           | 0.0119523          | 1.91055e-08        | 60745             \n",
      "model.layers.15.mlp.gate_proj    | 14.4035            | -0.000637054       | 0.195039           | 0.0117755          | 1.14626e-08        | 27442             \n",
      "model.layers.15.mlp.up_proj      | 14.4035            | -0.000637054       | 0.195039           | 0.0117755          | 1.14626e-08        | 27442             \n",
      "model.layers.16.mlp.down_proj    | 27.581             | 0.000622745        | 0.116054           | 0.0117681          | 1.99639e-08        | 28852             \n",
      "model.layers.18.mlp.down_proj    | 28.8859            | -0.000135915       | 0.081537           | 0.0117559          | 1.92841e-08        | 86495             \n",
      "model.layers.9.mlp.gate_proj     | 15.1845            | -0.0011692         | 0.175937           | 0.0113392          | 1.30807e-08        | 22274             \n",
      "model.layers.9.mlp.up_proj       | 15.1845            | -0.0011692         | 0.175937           | 0.0113392          | 1.30807e-08        | 22274             \n",
      "model.layers.24.mlp.down_proj    | 28.3558            | -9.6862e-05        | 0.0707176          | 0.0113238          | 1.56455e-08        | 252111            \n",
      "model.layers.24.mlp.gate_proj    | 12.8024            | 0.000721089        | 0.240605           | 0.0112572          | 8.45207e-09        | 46115             \n",
      "model.layers.24.mlp.up_proj      | 12.8024            | 0.000721089        | 0.240605           | 0.0112572          | 8.45207e-09        | 46115             \n",
      "model.layers.18.mlp.gate_proj    | 13.1846            | 0.000535495        | 0.212884           | 0.0110814          | 9.20552e-09        | 33185             \n",
      "model.layers.18.mlp.up_proj      | 13.1846            | 0.000535495        | 0.212884           | 0.0110814          | 9.20552e-09        | 33185             \n",
      "model.layers.17.mlp.gate_proj    | 13.3728            | 0.000204208        | 0.206305           | 0.0109678          | 9.52734e-09        | 30223             \n",
      "model.layers.17.mlp.up_proj      | 13.3728            | 0.000204208        | 0.206305           | 0.0109678          | 9.52734e-09        | 30223             \n",
      "model.layers.21.mlp.gate_proj    | 12.7654            | 0.000468292        | 0.224112           | 0.0108204          | 8.77216e-09        | 40475             \n",
      "model.layers.21.mlp.up_proj      | 12.7654            | 0.000468292        | 0.224112           | 0.0108204          | 8.77216e-09        | 40475             \n",
      "model.layers.13.mlp.gate_proj    | 13.4208            | -0.000889781       | 0.182414           | 0.0107963          | 1.11954e-08        | 27715             \n",
      "model.layers.13.mlp.up_proj      | 13.4208            | -0.000889781       | 0.182414           | 0.0107963          | 1.11954e-08        | 27715             \n",
      "model.layers.20.mlp.down_proj    | 26.6516            | 0.000368216        | 0.0605701          | 0.0107725          | 1.70181e-08        | 228563            \n",
      "model.layers.22.mlp.gate_proj    | 12.4528            | 0.000484776        | 0.227925           | 0.0107698          | 8.54608e-09        | 44976             \n",
      "model.layers.22.mlp.up_proj      | 12.4528            | 0.000484776        | 0.227925           | 0.0107698          | 8.54608e-09        | 44976             \n",
      "model.layers.23.mlp.gate_proj    | 12.1573            | 0.000830927        | 0.234867           | 0.0106973          | 8.23046e-09        | 44965             \n",
      "model.layers.23.mlp.up_proj      | 12.1573            | 0.000830927        | 0.234867           | 0.0106973          | 8.23046e-09        | 44965             \n",
      "model.layers.20.mlp.gate_proj    | 12.6438            | 0.000268235        | 0.218722           | 0.010611           | 8.78964e-09        | 38846             \n",
      "model.layers.20.mlp.up_proj      | 12.6438            | 0.000268235        | 0.218722           | 0.010611           | 8.78964e-09        | 38846             \n",
      "model.layers.14.mlp.gate_proj    | 12.5646            | -0.000907509       | 0.184135           | 0.010441           | 1.06539e-08        | 28676             \n",
      "model.layers.14.mlp.up_proj      | 12.5646            | -0.000907509       | 0.184135           | 0.010441           | 1.06539e-08        | 28676             \n",
      "model.layers.30.self_attn.o_proj | 10.7023            | -0.000989676       | 0.0932969          | 0.010408           | 2.30594e-08        | 127407            \n",
      "model.layers.17.mlp.down_proj    | 22.529             | -0.000108332       | 0.0514019          | 0.0102675          | 1.78104e-08        | 215291            \n",
      "model.layers.21.mlp.down_proj    | 22.9183            | -3.96545e-06       | 0.0631088          | 0.00997965         | 1.52175e-08        | 235391            \n",
      "model.layers.27.self_attn.o_proj | 10.4412            | -0.000609891       | 0.0860439          | 0.00988925         | 2.63764e-08        | 140807            \n",
      "model.layers.22.mlp.down_proj    | 22.8821            | -1.48662e-05       | 0.0669743          | 0.00980082         | 1.44007e-08        | 241831            \n",
      "model.layers.8.mlp.gate_proj     | 12.5387            | -0.000967902       | 0.168332           | 0.00964731         | 1.05231e-08        | 25576             \n",
      "model.layers.8.mlp.up_proj       | 12.5387            | -0.000967902       | 0.168332           | 0.00964731         | 1.05231e-08        | 25576             \n",
      "model.layers.12.mlp.gate_proj    | 11.7611            | -0.000923855       | 0.173701           | 0.00961184         | 1.02891e-08        | 28714             \n",
      "model.layers.12.mlp.up_proj      | 11.7611            | -0.000923855       | 0.173701           | 0.00961184         | 1.02891e-08        | 28714             \n",
      "model.layers.2.self_attn.q_proj  | 13.7926            | -0.000378477       | 0.171666           | 0.00945662         | 1.33059e-08        | 39779             \n",
      "model.layers.2.self_attn.k_proj  | 13.7926            | -0.000378477       | 0.171666           | 0.00945662         | 1.33059e-08        | 39779             \n",
      "model.layers.2.self_attn.v_proj  | 13.7926            | -0.000378477       | 0.171666           | 0.00945662         | 1.33059e-08        | 39779             \n",
      "model.layers.15.mlp.down_proj    | 20.1521            | 4.93492e-06        | 0.0444796          | 0.00941514         | 1.8218e-08         | 238812            \n",
      "model.layers.4.mlp.gate_proj     | 13.3684            | 0.000685153        | 0.145715           | 0.00930277         | 1.28501e-08        | 17771             \n",
      "model.layers.4.mlp.up_proj       | 13.3684            | 0.000685153        | 0.145715           | 0.00930277         | 1.28501e-08        | 17771             \n",
      "model.layers.11.mlp.gate_proj    | 10.6556            | -0.000641156       | 0.168857           | 0.00881059         | 9.68937e-09        | 28796             \n",
      "model.layers.11.mlp.up_proj      | 10.6556            | -0.000641156       | 0.168857           | 0.00881059         | 9.68937e-09        | 28796             \n",
      "model.layers.28.self_attn.o_proj | 9.02917            | -0.00102676        | 0.0797498          | 0.00879219         | 2.355e-08          | 138030            \n",
      "model.layers.10.mlp.gate_proj    | 10.5257            | -0.000583861       | 0.165047           | 0.00862242         | 9.82313e-09        | 28282             \n",
      "model.layers.10.mlp.up_proj      | 10.5257            | -0.000583861       | 0.165047           | 0.00862242         | 9.82313e-09        | 28282             \n",
      "model.layers.8.mlp.down_proj     | 18.6462            | 0.000206828        | 0.053043           | 0.00830545         | 2.20939e-08        | 51558             \n",
      "model.layers.26.self_attn.o_proj | 8.33137            | 0.000203785        | 0.0745206          | 0.00809911         | 2.34379e-08        | 132748            \n",
      "model.layers.7.mlp.gate_proj     | 9.75879            | -0.00106152        | 0.159868           | 0.0078872          | 8.61244e-09        | 27254             \n",
      "model.layers.7.mlp.up_proj       | 9.75879            | -0.00106152        | 0.159868           | 0.0078872          | 8.61244e-09        | 27254             \n",
      "model.layers.23.self_attn.o_proj | 7.58875            | 9.90827e-05        | 0.0592411          | 0.0073454          | 2.87467e-08        | 122041            \n",
      "model.layers.22.self_attn.o_proj | 7.5989             | 0.00032646         | 0.0567289          | 0.00731916         | 2.58257e-08        | 117588            \n",
      "model.layers.9.mlp.down_proj     | 13.8428            | -0.000109895       | 0.040979           | 0.0071316          | 1.82935e-08        | 110936            \n",
      "model.layers.7.mlp.down_proj     | 14.3512            | -0.000120194       | 0.0317759          | 0.00708926         | 2.04678e-08        | 149863            \n",
      "model.layers.6.mlp.gate_proj     | 8.61503            | -0.000643336       | 0.148778           | 0.00704003         | 7.96356e-09        | 26528             \n",
      "model.layers.6.mlp.up_proj       | 8.61503            | -0.000643336       | 0.148778           | 0.00704003         | 7.96356e-09        | 26528             \n",
      "model.layers.13.mlp.down_proj    | 13.2967            | -0.000332198       | 0.0460551          | 0.00697963         | 1.58788e-08        | 130023            \n",
      "model.layers.5.mlp.gate_proj     | 8.50946            | -0.00053504        | 0.137182           | 0.00680624         | 8.43139e-09        | 25249             \n",
      "model.layers.5.mlp.up_proj       | 8.50946            | -0.00053504        | 0.137182           | 0.00680624         | 8.43139e-09        | 25249             \n",
      "model.layers.2.mlp.gate_proj     | 9.60791            | 0.000876533        | 0.101729           | 0.0066846          | 1.3359e-08         | 6994              \n",
      "model.layers.2.mlp.up_proj       | 9.60791            | 0.000876533        | 0.101729           | 0.0066846          | 1.3359e-08         | 6994              \n",
      "model.layers.14.mlp.down_proj    | 11.5119            | -0.000103241       | 0.0393219          | 0.00652601         | 1.40256e-08        | 242693            \n",
      "model.layers.6.mlp.down_proj     | 13.4397            | 7.85005e-05        | 0.033763           | 0.00645605         | 2.11412e-08        | 85131             \n",
      "model.layers.24.self_attn.o_proj | 6.5118             | -0.000102395       | 0.0610052          | 0.00633882         | 2.21604e-08        | 126617            \n",
      "model.layers.18.self_attn.o_proj | 6.30452            | 9.05853e-05        | 0.0513335          | 0.00618536         | 2.35346e-08        | 125685            \n",
      "model.layers.4.mlp.down_proj     | 18.4739            | 3.38147e-05        | 0.0635575          | 0.00610485         | 2.63688e-08        | 15967             \n",
      "model.layers.0.self_attn.q_proj  | 9.33044            | -0.00116691        | 0.0864893          | 0.00583035         | 3.3836e-08         | 90044             \n",
      "model.layers.0.self_attn.k_proj  | 9.33044            | -0.00116691        | 0.0864893          | 0.00583035         | 3.3836e-08         | 90044             \n",
      "model.layers.0.self_attn.v_proj  | 9.33044            | -0.00116691        | 0.0864893          | 0.00583035         | 3.3836e-08         | 90044             \n",
      "model.layers.25.self_attn.o_proj | 5.93258            | 7.78202e-05        | 0.0704917          | 0.00580499         | 1.91567e-08        | 125000            \n",
      "model.layers.29.self_attn.o_proj | 5.78921            | 0.000637379        | 0.0945126          | 0.00570586         | 1.21071e-08        | 138826            \n",
      "model.layers.11.mlp.down_proj    | 8.48571            | -8.88127e-05       | 0.0334318          | 0.00538866         | 1.31152e-08        | 237219            \n",
      "model.layers.12.mlp.down_proj    | 8.59543            | 3.85386e-05        | 0.0328681          | 0.00529185         | 1.28727e-08        | 274259            \n",
      "model.layers.10.mlp.down_proj    | 7.76863            | -0.00013978        | 0.0314474          | 0.00513041         | 1.26651e-08        | 254498            \n",
      "model.layers.3.mlp.gate_proj     | 6.33826            | 0.000363451        | 0.102564           | 0.00501667         | 8.16841e-09        | 22224             \n",
      "model.layers.3.mlp.up_proj       | 6.33826            | 0.000363451        | 0.102564           | 0.00501667         | 8.16841e-09        | 22224             \n",
      "model.layers.21.self_attn.o_proj | 5.06359            | 0.000151506        | 0.0529388          | 0.0049643          | 2.00412e-08        | 123205            \n",
      "model.layers.15.self_attn.o_proj | 5.02888            | -0.000471172       | 0.0677366          | 0.00492479         | 1.21804e-08        | 112604            \n",
      "model.layers.17.self_attn.o_proj | 4.9493             | 0.00021448         | 0.0617091          | 0.00485387         | 1.46229e-08        | 131218            \n",
      "model.layers.1.mlp.down_proj     | 1536.59            | 0.00756554         | 1.91712            | 0.00482577         | 8.53742e-09        | 4282              \n",
      "model.layers.19.self_attn.o_proj | 4.51999            | 0.000210113        | 0.0504312          | 0.00443068         | 1.7962e-08         | 124198            \n",
      "model.layers.20.self_attn.o_proj | 4.26358            | -0.000180379       | 0.0556027          | 0.00418917         | 1.55537e-08        | 130051            \n",
      "model.layers.16.self_attn.o_proj | 3.91589            | 0.000867334        | 0.0688603          | 0.00382775         | 9.71037e-09        | 126234            \n",
      "model.layers.8.self_attn.o_proj  | 3.78742            | -0.000101272       | 0.045739           | 0.00370826         | 1.49413e-08        | 125972            \n",
      "model.layers.0.mlp.gate_proj     | 3.77306            | 0.000673296        | 0.0581396          | 0.00369459         | 1.05185e-08        | 49507             \n",
      "model.layers.0.mlp.up_proj       | 3.77306            | 0.000673296        | 0.0581396          | 0.00369459         | 1.05185e-08        | 49507             \n",
      "model.layers.6.self_attn.o_proj  | 3.71364            | -0.000320347       | 0.03104            | 0.00357599         | 2.23994e-08        | 115000            \n",
      "model.layers.13.self_attn.o_proj | 3.61594            | 0.000487129        | 0.0616144          | 0.0035267          | 9.83267e-09        | 125133            \n",
      "model.layers.10.self_attn.o_proj | 3.53599            | 0.000605802        | 0.0559616          | 0.00346103         | 1.01525e-08        | 118813            \n",
      "model.layers.7.self_attn.o_proj  | 3.49553            | -4.86302e-06       | 0.0345356          | 0.00340232         | 1.8752e-08         | 135706            \n",
      "model.layers.9.self_attn.o_proj  | 3.46564            | -6.16364e-05       | 0.0498666          | 0.00339792         | 1.21396e-08        | 129222            \n",
      "model.layers.5.mlp.down_proj     | 4.65305            | -0.000101085       | 0.0191075          | 0.00327309         | 1.28209e-08        | 232359            \n",
      "model.layers.14.self_attn.o_proj | 3.2348             | 0.00027651         | 0.0662458          | 0.00315765         | 7.85838e-09        | 116501            \n",
      "model.layers.12.self_attn.o_proj | 3.228              | -0.00050915        | 0.0571488          | 0.00314938         | 9.24442e-09        | 114950            \n",
      "model.layers.1.mlp.gate_proj     | 3.20983            | 0.000832595        | 0.0803985          | 0.00314649         | 5.20745e-09        | 37261             \n",
      "model.layers.1.mlp.up_proj       | 3.20983            | 0.000832595        | 0.0803985          | 0.00314649         | 5.20745e-09        | 37261             \n",
      "model.layers.11.self_attn.o_proj | 3.02782            | 0.000374366        | 0.0573009          | 0.00297898         | 8.61347e-09        | 114338            \n",
      "model.layers.1.self_attn.o_proj  | 2.77913            | 0.000140832        | 0.0373086          | 0.00267847         | 1.81711e-08        | 68801             \n",
      "model.layers.5.self_attn.o_proj  | 2.54927            | 0.00011598         | 0.0270819          | 0.00247671         | 1.86485e-08        | 118048            \n",
      "model.layers.2.mlp.down_proj     | 6.58327            | 0.000185986        | 0.0284482          | 0.00226588         | 2.35684e-08        | 16491             \n",
      "model.layers.4.self_attn.o_proj  | 2.23658            | 8.97876e-05        | 0.0185841          | 0.00216679         | 2.24349e-08        | 118106            \n",
      "model.layers.3.mlp.down_proj     | 3.42802            | 9.7032e-06         | 0.0113677          | 0.00206605         | 1.52515e-08        | 214513            \n",
      "model.layers.0.self_attn.o_proj  | 2.09053            | 0.000329955        | 0.0400683          | 0.00178707         | 1.51867e-08        | 106590            \n",
      "model.layers.2.self_attn.o_proj  | 1.86184            | -6.91831e-05       | 0.0111575          | 0.00176514         | 3.14074e-08        | 112614            \n",
      "model.layers.3.self_attn.o_proj  | 1.75015            | 4.67434e-05        | 0.0138884          | 0.00171683         | 2.3437e-08         | 106749            \n",
      "model.layers.0.mlp.down_proj     | 5.6795             | -0.000118556       | 0.0226925          | 0.00157896         | 2.33331e-08        | 40867             \n"
     ]
    }
   ],
   "source": [
    "!python3 analyze_activations.py --model meta-llama/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat recipe-quant.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Construct Calib Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from sparseml.transformers.data.base_llm import TransformersDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "system_prompt = {\n",
    "    \"content\": \"You are a friendly chatbot\",\n",
    "    \"role\": \"system\"\n",
    "}\n",
    "\n",
    "class ChatDataset(TransformersDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        seqlen,\n",
    "        nsamples,\n",
    "        path,\n",
    "        seed: int = 0,\n",
    "        split: str = \"train_sft\",\n",
    "        split_percent_to_use: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            seqlen=seqlen,\n",
    "            nsamples=nsamples,\n",
    "            path=path,\n",
    "            name=None,\n",
    "            seed=seed,\n",
    "            split=split,\n",
    "            use_max_tokens=False,\n",
    "            split_percent_to_use=split_percent_to_use,\n",
    "        )\n",
    "\n",
    "        tok = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "        processed_data = []\n",
    "        for sample in tqdm.tqdm(self._data):\n",
    "            assert \"messages\" in sample\n",
    "            messages_with_sys_prompt = [system_prompt] + sample[\"messages\"]\n",
    "            processed_data.append(\n",
    "                tok.apply_chat_template(\n",
    "                    messages_with_sys_prompt, \n",
    "                    tokenize=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # print(processed_data[-1])\n",
    "            \n",
    "        self.create_dataloader(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "dataset_id = \"HuggingFaceH4/ultrachat_200k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChatDataset(\n",
    "    model=model_id,\n",
    "    seqlen=512,\n",
    "    nsamples=512,\n",
    "    path=dataset_id\n",
    ")\n",
    "\n",
    "calibration_data = dataset.loader\n",
    "tokenizer = dataset.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fd91d841134e239f3324993ca7cca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "print(model.dtype)\n",
    "print(model.device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "SKIPPING MODEL.to(device)\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 17:59:29 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 1/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 17:59:44 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 0\n",
      "2023-11-06 17:59:46 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.31\n",
      "2023-11-06 17:59:46 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.05\n",
      "2023-11-06 18:00:00 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 0\n",
      "2023-11-06 18:00:01 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.24\n",
      "2023-11-06 18:00:01 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.12\n",
      "2023-11-06 18:00:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 0\n",
      "2023-11-06 18:00:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.26\n",
      "2023-11-06 18:00:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.30\n",
      "2023-11-06 18:00:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 0\n",
      "2023-11-06 18:00:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.28\n",
      "2023-11-06 18:00:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.07\n",
      "2023-11-06 18:00:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 0\n",
      "2023-11-06 18:00:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.31\n",
      "2023-11-06 18:00:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 533.56\n",
      "2023-11-06 18:01:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 0\n",
      "2023-11-06 18:01:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.31\n",
      "2023-11-06 18:01:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 482.15\n",
      "2023-11-06 18:01:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 0\n",
      "2023-11-06 18:01:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.67\n",
      "2023-11-06 18:01:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.16\n",
      "2023-11-06 18:02:00 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 2/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457424GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.798828GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:02:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 1\n",
      "2023-11-06 18:02:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.31\n",
      "2023-11-06 18:02:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 30.10\n",
      "2023-11-06 18:02:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 1\n",
      "2023-11-06 18:02:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.31\n",
      "2023-11-06 18:02:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 9.00\n",
      "2023-11-06 18:02:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 1\n",
      "2023-11-06 18:02:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:02:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5.26\n",
      "2023-11-06 18:03:11 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 1\n",
      "2023-11-06 18:03:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:03:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.28\n",
      "2023-11-06 18:03:29 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 1\n",
      "2023-11-06 18:03:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:03:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1690.74\n",
      "2023-11-06 18:03:47 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 1\n",
      "2023-11-06 18:03:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:03:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1497.88\n",
      "2023-11-06 18:04:18 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 1\n",
      "2023-11-06 18:04:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:04:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 57.27\n",
      "2023-11-06 18:04:40 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 3/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:04:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 2\n",
      "2023-11-06 18:04:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:04:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2555.46\n",
      "2023-11-06 18:05:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 2\n",
      "2023-11-06 18:05:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:05:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 901.78\n",
      "2023-11-06 18:05:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 2\n",
      "2023-11-06 18:05:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:05:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 210.92\n",
      "2023-11-06 18:05:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 2\n",
      "2023-11-06 18:05:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:05:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.35\n",
      "2023-11-06 18:06:09 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 2\n",
      "2023-11-06 18:06:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 18:06:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3687.54\n",
      "2023-11-06 18:06:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 2\n",
      "2023-11-06 18:06:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 18:06:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3264.32\n",
      "2023-11-06 18:06:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 2\n",
      "2023-11-06 18:07:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.65\n",
      "2023-11-06 18:07:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1.08\n",
      "2023-11-06 18:07:20 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 4/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:07:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 3\n",
      "2023-11-06 18:07:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:07:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2140.45\n",
      "2023-11-06 18:07:56 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 3\n",
      "2023-11-06 18:07:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:07:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 771.81\n",
      "2023-11-06 18:08:14 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 3\n",
      "2023-11-06 18:08:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:08:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 234.46\n",
      "2023-11-06 18:08:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 3\n",
      "2023-11-06 18:08:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:08:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 0.78\n",
      "2023-11-06 18:08:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 3\n",
      "2023-11-06 18:08:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:08:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5771.44\n",
      "2023-11-06 18:09:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 3\n",
      "2023-11-06 18:09:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:09:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5023.25\n",
      "2023-11-06 18:09:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 3\n",
      "2023-11-06 18:09:44 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 18:09:44 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2.06\n",
      "2023-11-06 18:10:01 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 5/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:10:18 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 4\n",
      "2023-11-06 18:10:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:10:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3627.70\n",
      "2023-11-06 18:10:36 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 4\n",
      "2023-11-06 18:10:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:10:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1286.52\n",
      "2023-11-06 18:10:54 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 4\n",
      "2023-11-06 18:10:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:10:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 440.39\n",
      "2023-11-06 18:11:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 4\n",
      "2023-11-06 18:11:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:11:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1.01\n",
      "2023-11-06 18:11:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 4\n",
      "2023-11-06 18:11:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:11:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 7956.79\n",
      "2023-11-06 18:11:49 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 4\n",
      "2023-11-06 18:11:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:11:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6552.94\n",
      "2023-11-06 18:12:19 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 4\n",
      "2023-11-06 18:12:24 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 18:12:24 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3.82\n",
      "2023-11-06 18:12:41 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 6/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:12:58 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 5\n",
      "2023-11-06 18:12:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:12:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5885.69\n",
      "2023-11-06 18:13:16 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 5\n",
      "2023-11-06 18:13:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:13:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2182.52\n",
      "2023-11-06 18:13:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 5\n",
      "2023-11-06 18:13:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:13:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 586.41\n",
      "2023-11-06 18:13:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 5\n",
      "2023-11-06 18:13:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:13:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2.66\n",
      "2023-11-06 18:14:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 5\n",
      "2023-11-06 18:14:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:14:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 11329.21\n",
      "2023-11-06 18:14:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 5\n",
      "2023-11-06 18:14:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:14:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 8771.77\n",
      "2023-11-06 18:14:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 5\n",
      "2023-11-06 18:15:04 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:15:04 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6.88\n",
      "2023-11-06 18:15:21 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 7/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:15:38 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 6\n",
      "2023-11-06 18:15:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:15:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4999.64\n",
      "2023-11-06 18:15:56 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 6\n",
      "2023-11-06 18:15:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:15:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1878.91\n",
      "2023-11-06 18:16:14 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 6\n",
      "2023-11-06 18:16:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:16:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 557.31\n",
      "2023-11-06 18:16:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 6\n",
      "2023-11-06 18:16:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:16:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2.85\n",
      "2023-11-06 18:16:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 6\n",
      "2023-11-06 18:16:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:16:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 13286.23\n",
      "2023-11-06 18:17:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 6\n",
      "2023-11-06 18:17:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:17:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 10427.98\n",
      "2023-11-06 18:17:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 6\n",
      "2023-11-06 18:17:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 18:17:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 10.13\n",
      "2023-11-06 18:18:00 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 8/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:18:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 7\n",
      "2023-11-06 18:18:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:18:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5964.08\n",
      "2023-11-06 18:18:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 7\n",
      "2023-11-06 18:18:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:18:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2250.06\n",
      "2023-11-06 18:18:54 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 7\n",
      "2023-11-06 18:18:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:18:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 707.32\n",
      "2023-11-06 18:19:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 7\n",
      "2023-11-06 18:19:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:19:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5.04\n",
      "2023-11-06 18:19:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 7\n",
      "2023-11-06 18:19:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:19:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 16235.30\n",
      "2023-11-06 18:19:48 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 7\n",
      "2023-11-06 18:19:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:19:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 12512.98\n",
      "2023-11-06 18:20:19 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 7\n",
      "2023-11-06 18:20:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:20:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 13.59\n",
      "2023-11-06 18:20:40 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 9/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:20:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 8\n",
      "2023-11-06 18:20:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:20:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6425.66\n",
      "2023-11-06 18:21:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 8\n",
      "2023-11-06 18:21:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:21:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2361.46\n",
      "2023-11-06 18:21:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 8\n",
      "2023-11-06 18:21:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:21:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 751.06\n",
      "2023-11-06 18:21:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 8\n",
      "2023-11-06 18:21:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:21:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5.81\n",
      "2023-11-06 18:22:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 8\n",
      "2023-11-06 18:22:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:22:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17331.82\n",
      "2023-11-06 18:22:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 8\n",
      "2023-11-06 18:22:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:22:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 13765.79\n",
      "2023-11-06 18:22:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 8\n",
      "2023-11-06 18:23:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:23:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 16.21\n",
      "2023-11-06 18:23:20 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 10/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:23:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 9\n",
      "2023-11-06 18:23:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:23:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 8542.04\n",
      "2023-11-06 18:23:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 9\n",
      "2023-11-06 18:23:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:23:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3270.70\n",
      "2023-11-06 18:24:13 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 9\n",
      "2023-11-06 18:24:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:24:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 995.81\n",
      "2023-11-06 18:24:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 9\n",
      "2023-11-06 18:24:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:24:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 7.31\n",
      "2023-11-06 18:24:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 9\n",
      "2023-11-06 18:24:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:24:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17979.39\n",
      "2023-11-06 18:25:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 9\n",
      "2023-11-06 18:25:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:25:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 14885.97\n",
      "2023-11-06 18:25:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 9\n",
      "2023-11-06 18:25:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.60\n",
      "2023-11-06 18:25:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 19.36\n",
      "2023-11-06 18:26:00 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 11/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:26:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 10\n",
      "2023-11-06 18:26:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:26:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 7801.41\n",
      "2023-11-06 18:26:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 10\n",
      "2023-11-06 18:26:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:26:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3253.01\n",
      "2023-11-06 18:26:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 10\n",
      "2023-11-06 18:26:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:26:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 826.11\n",
      "2023-11-06 18:27:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 10\n",
      "2023-11-06 18:27:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:27:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 9.96\n",
      "2023-11-06 18:27:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 10\n",
      "2023-11-06 18:27:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:27:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17523.68\n",
      "2023-11-06 18:27:48 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 10\n",
      "2023-11-06 18:27:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.39\n",
      "2023-11-06 18:27:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 14970.51\n",
      "2023-11-06 18:28:19 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 10\n",
      "2023-11-06 18:28:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 18:28:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 21.18\n",
      "2023-11-06 18:28:40 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 12/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:28:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 11\n",
      "2023-11-06 18:28:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:28:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 8717.14\n",
      "2023-11-06 18:29:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 11\n",
      "2023-11-06 18:29:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:29:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3448.88\n",
      "2023-11-06 18:29:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 11\n",
      "2023-11-06 18:29:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:29:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1139.76\n",
      "2023-11-06 18:29:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 11\n",
      "2023-11-06 18:29:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:29:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 14.23\n",
      "2023-11-06 18:30:09 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 11\n",
      "2023-11-06 18:30:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:30:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 18250.99\n",
      "2023-11-06 18:30:27 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 11\n",
      "2023-11-06 18:30:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:30:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 16092.48\n",
      "2023-11-06 18:30:58 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 11\n",
      "2023-11-06 18:31:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:31:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 24.49\n",
      "2023-11-06 18:31:19 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 13/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:31:36 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 12\n",
      "2023-11-06 18:31:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:31:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 9960.01\n",
      "2023-11-06 18:31:54 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 12\n",
      "2023-11-06 18:31:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:31:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3558.50\n",
      "2023-11-06 18:32:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 12\n",
      "2023-11-06 18:32:14 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:32:14 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1244.84\n",
      "2023-11-06 18:32:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 12\n",
      "2023-11-06 18:32:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:32:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 16.67\n",
      "2023-11-06 18:32:48 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 12\n",
      "2023-11-06 18:32:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:32:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 20568.92\n",
      "2023-11-06 18:33:07 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 12\n",
      "2023-11-06 18:33:08 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:33:08 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 18550.61\n",
      "2023-11-06 18:33:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 12\n",
      "2023-11-06 18:33:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:33:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 29.92\n",
      "2023-11-06 18:33:59 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 14/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:34:16 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 13\n",
      "2023-11-06 18:34:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:34:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 11112.79\n",
      "2023-11-06 18:34:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 13\n",
      "2023-11-06 18:34:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:34:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4720.46\n",
      "2023-11-06 18:34:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 13\n",
      "2023-11-06 18:34:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:34:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 1435.06\n",
      "2023-11-06 18:35:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 13\n",
      "2023-11-06 18:35:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:35:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 20.89\n",
      "2023-11-06 18:35:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 13\n",
      "2023-11-06 18:35:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:35:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 22420.46\n",
      "2023-11-06 18:35:46 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 13\n",
      "2023-11-06 18:35:47 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:35:47 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 20954.68\n",
      "2023-11-06 18:36:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 13\n",
      "2023-11-06 18:36:22 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.66\n",
      "2023-11-06 18:36:22 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 36.82\n",
      "2023-11-06 18:36:38 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 15/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:36:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 14\n",
      "2023-11-06 18:36:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:36:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 10507.15\n",
      "2023-11-06 18:37:13 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 14\n",
      "2023-11-06 18:37:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:37:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3792.59\n",
      "2023-11-06 18:37:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 14\n",
      "2023-11-06 18:37:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:37:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2013.50\n",
      "2023-11-06 18:37:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 14\n",
      "2023-11-06 18:37:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:37:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 28.07\n",
      "2023-11-06 18:38:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 14\n",
      "2023-11-06 18:38:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.35\n",
      "2023-11-06 18:38:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 24259.43\n",
      "2023-11-06 18:38:26 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 14\n",
      "2023-11-06 18:38:27 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:38:27 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 23219.50\n",
      "2023-11-06 18:38:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 14\n",
      "2023-11-06 18:39:02 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 18:39:02 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 49.46\n",
      "2023-11-06 18:39:18 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 16/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:39:36 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 15\n",
      "2023-11-06 18:39:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:39:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 13886.56\n",
      "2023-11-06 18:39:54 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 15\n",
      "2023-11-06 18:39:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:39:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4490.59\n",
      "2023-11-06 18:40:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 15\n",
      "2023-11-06 18:40:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:40:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2519.37\n",
      "2023-11-06 18:40:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 15\n",
      "2023-11-06 18:40:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:40:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 38.07\n",
      "2023-11-06 18:40:48 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 15\n",
      "2023-11-06 18:40:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.37\n",
      "2023-11-06 18:40:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 29798.51\n",
      "2023-11-06 18:41:06 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 15\n",
      "2023-11-06 18:41:08 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 18:41:08 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 27489.35\n",
      "2023-11-06 18:41:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 15\n",
      "2023-11-06 18:41:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 18:41:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 68.47\n",
      "2023-11-06 18:41:59 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 17/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:42:16 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 16\n",
      "2023-11-06 18:42:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:42:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 15176.93\n",
      "2023-11-06 18:42:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 16\n",
      "2023-11-06 18:42:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:42:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5555.56\n",
      "2023-11-06 18:42:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 16\n",
      "2023-11-06 18:42:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:42:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2694.75\n",
      "2023-11-06 18:43:11 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 16\n",
      "2023-11-06 18:43:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:43:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 41.64\n",
      "2023-11-06 18:43:29 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 16\n",
      "2023-11-06 18:43:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:43:30 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 36975.43\n",
      "2023-11-06 18:43:47 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 16\n",
      "2023-11-06 18:43:48 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 18:43:48 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 32969.61\n",
      "2023-11-06 18:44:18 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 16\n",
      "2023-11-06 18:44:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.65\n",
      "2023-11-06 18:44:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 97.75\n",
      "2023-11-06 18:44:39 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 18/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:44:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 17\n",
      "2023-11-06 18:44:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:44:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 13043.64\n",
      "2023-11-06 18:45:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 17\n",
      "2023-11-06 18:45:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:45:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4639.60\n",
      "2023-11-06 18:45:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 17\n",
      "2023-11-06 18:45:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:45:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2200.51\n",
      "2023-11-06 18:45:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 17\n",
      "2023-11-06 18:45:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:45:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 44.39\n",
      "2023-11-06 18:46:09 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 17\n",
      "2023-11-06 18:46:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:46:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 43761.37\n",
      "2023-11-06 18:46:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 17\n",
      "2023-11-06 18:46:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:46:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 39205.86\n",
      "2023-11-06 18:46:58 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 17\n",
      "2023-11-06 18:47:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 18:47:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 129.72\n",
      "2023-11-06 18:47:20 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 19/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:47:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 18\n",
      "2023-11-06 18:47:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:47:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 15093.23\n",
      "2023-11-06 18:47:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 18\n",
      "2023-11-06 18:47:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:47:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4884.28\n",
      "2023-11-06 18:48:13 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 18\n",
      "2023-11-06 18:48:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:48:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 2700.34\n",
      "2023-11-06 18:48:31 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 18\n",
      "2023-11-06 18:48:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:48:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 50.46\n",
      "2023-11-06 18:48:49 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 18\n",
      "2023-11-06 18:48:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:48:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 50902.71\n",
      "2023-11-06 18:49:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 18\n",
      "2023-11-06 18:49:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.39\n",
      "2023-11-06 18:49:09 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 45465.19\n",
      "2023-11-06 18:49:38 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 18\n",
      "2023-11-06 18:49:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.67\n",
      "2023-11-06 18:49:43 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 185.88\n",
      "2023-11-06 18:50:00 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 20/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:50:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 19\n",
      "2023-11-06 18:50:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:50:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17659.00\n",
      "2023-11-06 18:50:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 19\n",
      "2023-11-06 18:50:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:50:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5346.69\n",
      "2023-11-06 18:50:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 19\n",
      "2023-11-06 18:50:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:50:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3621.70\n",
      "2023-11-06 18:51:11 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 19\n",
      "2023-11-06 18:51:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:51:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 61.98\n",
      "2023-11-06 18:51:29 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 19\n",
      "2023-11-06 18:51:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:51:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 60886.28\n",
      "2023-11-06 18:51:48 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 19\n",
      "2023-11-06 18:51:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:51:49 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 52607.35\n",
      "2023-11-06 18:52:18 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 19\n",
      "2023-11-06 18:52:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.60\n",
      "2023-11-06 18:52:23 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 241.54\n",
      "2023-11-06 18:52:40 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 21/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:52:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 20\n",
      "2023-11-06 18:52:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:52:58 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17734.67\n",
      "2023-11-06 18:53:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 20\n",
      "2023-11-06 18:53:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:53:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5696.65\n",
      "2023-11-06 18:53:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 20\n",
      "2023-11-06 18:53:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:53:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3875.21\n",
      "2023-11-06 18:53:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 20\n",
      "2023-11-06 18:53:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:53:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 70.42\n",
      "2023-11-06 18:54:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 20\n",
      "2023-11-06 18:54:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.35\n",
      "2023-11-06 18:54:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 73768.74\n",
      "2023-11-06 18:54:28 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 20\n",
      "2023-11-06 18:54:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:54:29 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 60970.01\n",
      "2023-11-06 18:54:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 20\n",
      "2023-11-06 18:55:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:55:03 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 267.35\n",
      "2023-11-06 18:55:20 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 22/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:55:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 21\n",
      "2023-11-06 18:55:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:55:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 18332.68\n",
      "2023-11-06 18:55:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 21\n",
      "2023-11-06 18:55:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:55:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5851.79\n",
      "2023-11-06 18:56:14 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 21\n",
      "2023-11-06 18:56:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:56:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4031.51\n",
      "2023-11-06 18:56:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 21\n",
      "2023-11-06 18:56:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:56:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 64.39\n",
      "2023-11-06 18:56:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 21\n",
      "2023-11-06 18:56:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:56:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 90413.82\n",
      "2023-11-06 18:57:08 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 21\n",
      "2023-11-06 18:57:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:57:10 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 68921.45\n",
      "2023-11-06 18:57:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 21\n",
      "2023-11-06 18:57:44 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 18:57:44 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 270.28\n",
      "2023-11-06 18:58:01 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 23/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:58:18 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 22\n",
      "2023-11-06 18:58:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:58:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 17970.44\n",
      "2023-11-06 18:58:36 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 22\n",
      "2023-11-06 18:58:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 18:58:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5665.70\n",
      "2023-11-06 18:58:54 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 22\n",
      "2023-11-06 18:58:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:58:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4354.91\n",
      "2023-11-06 18:59:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 22\n",
      "2023-11-06 18:59:14 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:59:14 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 50.83\n",
      "2023-11-06 18:59:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 22\n",
      "2023-11-06 18:59:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 18:59:32 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 96103.84\n",
      "2023-11-06 18:59:49 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 22\n",
      "2023-11-06 18:59:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 18:59:50 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 73440.88\n",
      "2023-11-06 19:00:20 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 22\n",
      "2023-11-06 19:00:24 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 19:00:24 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 276.70\n",
      "2023-11-06 19:00:41 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 24/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:00:58 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 23\n",
      "2023-11-06 19:01:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:01:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 19067.62\n",
      "2023-11-06 19:01:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 23\n",
      "2023-11-06 19:01:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:01:18 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5819.25\n",
      "2023-11-06 19:01:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 23\n",
      "2023-11-06 19:01:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:01:36 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4767.92\n",
      "2023-11-06 19:01:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 23\n",
      "2023-11-06 19:01:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:01:54 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 63.64\n",
      "2023-11-06 19:02:11 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 23\n",
      "2023-11-06 19:02:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:02:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 105023.81\n",
      "2023-11-06 19:02:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 23\n",
      "2023-11-06 19:02:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:02:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 79324.34\n",
      "2023-11-06 19:03:00 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 23\n",
      "2023-11-06 19:03:05 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 19:03:05 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 294.23\n",
      "2023-11-06 19:03:22 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 25/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:03:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 24\n",
      "2023-11-06 19:03:40 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:03:40 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 21455.66\n",
      "2023-11-06 19:03:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 24\n",
      "2023-11-06 19:03:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:03:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6989.91\n",
      "2023-11-06 19:04:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 24\n",
      "2023-11-06 19:04:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:04:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5740.30\n",
      "2023-11-06 19:04:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 24\n",
      "2023-11-06 19:04:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:04:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 56.76\n",
      "2023-11-06 19:04:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 24\n",
      "2023-11-06 19:04:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 19:04:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 116287.05\n",
      "2023-11-06 19:05:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 24\n",
      "2023-11-06 19:05:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:05:11 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 85956.28\n",
      "2023-11-06 19:05:41 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 24\n",
      "2023-11-06 19:05:45 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 19:05:45 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 305.55\n",
      "2023-11-06 19:06:02 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 26/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:06:19 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 25\n",
      "2023-11-06 19:06:21 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:06:21 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 22018.10\n",
      "2023-11-06 19:06:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 25\n",
      "2023-11-06 19:06:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:06:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6854.61\n",
      "2023-11-06 19:06:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 25\n",
      "2023-11-06 19:06:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:06:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6303.19\n",
      "2023-11-06 19:07:14 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 25\n",
      "2023-11-06 19:07:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:07:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 57.67\n",
      "2023-11-06 19:07:32 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 25\n",
      "2023-11-06 19:07:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:07:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 123195.44\n",
      "2023-11-06 19:07:50 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 25\n",
      "2023-11-06 19:07:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:07:51 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 92209.66\n",
      "2023-11-06 19:08:21 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 25\n",
      "2023-11-06 19:08:25 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.60\n",
      "2023-11-06 19:08:25 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 324.52\n",
      "2023-11-06 19:08:42 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 27/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:08:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 26\n",
      "2023-11-06 19:09:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:09:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 19295.73\n",
      "2023-11-06 19:09:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 26\n",
      "2023-11-06 19:09:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:09:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5761.50\n",
      "2023-11-06 19:09:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 26\n",
      "2023-11-06 19:09:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.35\n",
      "2023-11-06 19:09:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6137.34\n",
      "2023-11-06 19:09:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 26\n",
      "2023-11-06 19:09:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:09:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 73.48\n",
      "2023-11-06 19:10:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 26\n",
      "2023-11-06 19:10:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:10:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 125579.63\n",
      "2023-11-06 19:10:30 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 26\n",
      "2023-11-06 19:10:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:10:31 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 97107.73\n",
      "2023-11-06 19:11:01 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 26\n",
      "2023-11-06 19:11:05 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 19:11:05 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 349.29\n",
      "2023-11-06 19:11:22 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 28/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:11:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 27\n",
      "2023-11-06 19:11:41 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:11:41 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 21405.12\n",
      "2023-11-06 19:11:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 27\n",
      "2023-11-06 19:11:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:11:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6950.20\n",
      "2023-11-06 19:12:16 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 27\n",
      "2023-11-06 19:12:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:12:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6213.89\n",
      "2023-11-06 19:12:34 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 27\n",
      "2023-11-06 19:12:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:12:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 76.34\n",
      "2023-11-06 19:12:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 27\n",
      "2023-11-06 19:12:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:12:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 130929.91\n",
      "2023-11-06 19:13:10 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 27\n",
      "2023-11-06 19:13:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:13:12 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 103017.60\n",
      "2023-11-06 19:13:41 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 27\n",
      "2023-11-06 19:13:46 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.61\n",
      "2023-11-06 19:13:46 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 379.39\n",
      "2023-11-06 19:14:03 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 29/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:14:20 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 28\n",
      "2023-11-06 19:14:21 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:14:21 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 16715.63\n",
      "2023-11-06 19:14:38 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 28\n",
      "2023-11-06 19:14:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:14:39 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4556.25\n",
      "2023-11-06 19:14:56 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 28\n",
      "2023-11-06 19:14:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:14:57 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 6831.86\n",
      "2023-11-06 19:15:14 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 28\n",
      "2023-11-06 19:15:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:15:16 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 137.07\n",
      "2023-11-06 19:15:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 28\n",
      "2023-11-06 19:15:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:15:34 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 133248.09\n",
      "2023-11-06 19:15:51 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 28\n",
      "2023-11-06 19:15:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:15:52 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 109924.70\n",
      "2023-11-06 19:16:22 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 28\n",
      "2023-11-06 19:16:26 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 19:16:26 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 485.72\n",
      "2023-11-06 19:16:43 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 30/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:17:00 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 29\n",
      "2023-11-06 19:17:02 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:17:02 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 18767.50\n",
      "2023-11-06 19:17:19 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 29\n",
      "2023-11-06 19:17:20 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:17:20 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 5116.62\n",
      "2023-11-06 19:17:37 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 29\n",
      "2023-11-06 19:17:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:17:38 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 10216.00\n",
      "2023-11-06 19:17:55 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 29\n",
      "2023-11-06 19:17:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:17:56 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 204.98\n",
      "2023-11-06 19:18:13 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 29\n",
      "2023-11-06 19:18:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:18:15 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 126934.91\n",
      "2023-11-06 19:18:31 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 29\n",
      "2023-11-06 19:18:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:18:33 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 109136.23\n",
      "2023-11-06 19:19:02 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 29\n",
      "2023-11-06 19:19:07 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 19:19:07 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 662.88\n",
      "2023-11-06 19:19:24 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 31/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:19:41 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 30\n",
      "2023-11-06 19:19:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:19:42 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 15666.87\n",
      "2023-11-06 19:19:59 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 30\n",
      "2023-11-06 19:20:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:20:00 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 4018.58\n",
      "2023-11-06 19:20:17 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 30\n",
      "2023-11-06 19:20:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:20:19 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 9264.97\n",
      "2023-11-06 19:20:35 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 30\n",
      "2023-11-06 19:20:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:20:37 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 241.48\n",
      "2023-11-06 19:20:53 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 30\n",
      "2023-11-06 19:20:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:20:55 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 123882.95\n",
      "2023-11-06 19:21:12 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 30\n",
      "2023-11-06 19:21:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.36\n",
      "2023-11-06 19:21:13 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 109285.16\n",
      "2023-11-06 19:21:43 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 30\n",
      "2023-11-06 19:21:47 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.63\n",
      "2023-11-06 19:21:47 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 794.38\n",
      "2023-11-06 19:22:04 sparseml.modifiers.obcq.pytorch INFO     \n",
      "===== Compressing layer 32/32 to sparsity 0.5 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------\n",
      "moving inputs to gpu...\n",
      "----------------------------------------------------------------------------------------------\n",
      "done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 19:22:21 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.q_proj of layer 31\n",
      "2023-11-06 19:22:22 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.32\n",
      "2023-11-06 19:22:22 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 14604.77\n",
      "2023-11-06 19:22:39 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.k_proj of layer 31\n",
      "2023-11-06 19:22:41 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:22:41 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 3725.28\n",
      "2023-11-06 19:22:57 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.v_proj of layer 31\n",
      "2023-11-06 19:22:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:22:59 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 9093.79\n",
      "2023-11-06 19:23:15 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module self_attn.o_proj of layer 31\n",
      "2023-11-06 19:23:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.33\n",
      "2023-11-06 19:23:17 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 295.27\n",
      "2023-11-06 19:23:33 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.gate_proj of layer 31\n",
      "2023-11-06 19:23:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.35\n",
      "2023-11-06 19:23:35 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 109485.69\n",
      "2023-11-06 19:23:52 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.up_proj of layer 31\n",
      "2023-11-06 19:23:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 1.34\n",
      "2023-11-06 19:23:53 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 92814.92\n",
      "2023-11-06 19:24:22 sparseml.modifiers.obcq.utils.layer_compressor INFO     Compressing module mlp.down_proj of layer 31\n",
      "2023-11-06 19:24:27 sparseml.modifiers.obcq.utils.sparsegpt INFO     time 4.62\n",
      "2023-11-06 19:24:27 sparseml.modifiers.obcq.utils.sparsegpt INFO     error 904.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 4.457302GB\n",
      "torch.cuda.max_memory_allocated: 7.816967GB\n",
      "torch.cuda.max_memory_reserved: 8.806641GB\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModifiedState(model=MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "), optimizer=None, loss=None, modifier_data=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparseml.core.session as session_manager\n",
    "from sparseml.core.framework import Framework\n",
    "\n",
    "recipe_file = \"/home/rshaw/zephyr-training/pruning/recipe-50sparse.yaml\"\n",
    "\n",
    "session_manager.create_session()\n",
    "session = session_manager.active_session()\n",
    "session.apply(\n",
    "    framework=Framework.pytorch,\n",
    "    recipe=recipe_file,\n",
    "    model=model,\n",
    "    calib_data=calibration_data,\n",
    "    start=0.0,\n",
    "    device=\"cuda\",\n",
    "    copy_data=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-training\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/one-shot/50sparse-sft-v0/tokenizer_config.json',\n",
       " 'data/one-shot/50sparse-sft-v0/special_tokens_map.json',\n",
       " 'data/one-shot/50sparse-sft-v0/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"data/one-shot/50sparse-sft-v0\"\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sparseml.optim.helpers import load_recipe_yaml_str\n",
    "\n",
    "recipe_output_path = os.path.join(save_path, \"recipe.yaml\")\n",
    "with open(recipe_output_path, \"w\") as fp:\n",
    "    fp.write(load_recipe_yaml_str(recipe_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t\t  model-00006-of-00006.safetensors\n",
      "generation_config.json\t\t  model.safetensors.index.json\n",
      "model-00001-of-00006.safetensors  recipe.yaml\n",
      "model-00002-of-00006.safetensors  special_tokens_map.json\n",
      "model-00003-of-00006.safetensors  tokenizer_config.json\n",
      "model-00004-of-00006.safetensors  tokenizer.json\n",
      "model-00005-of-00006.safetensors\n"
     ]
    }
   ],
   "source": [
    "!ls {save_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparseml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
