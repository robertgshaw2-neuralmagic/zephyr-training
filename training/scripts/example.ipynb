{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Packed Chat Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-training/training/scripts\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 7.98MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 33.7MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 624/624 [00:00<00:00, 4.00MB/s]\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf as om\n",
    "from llmfoundry.utils.config_utils import pop_config\n",
    "from llmfoundry.utils.builders import build_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "yaml_path = \"/home/rshaw/zephyr-training/training/data/yamls/mistral-sft-50sparse-fp16-v0-run1.yaml\"\n",
    "with open(yaml_path) as f:\n",
    "    cfg = om.load(f)\n",
    "om.resolve(cfg)\n",
    "                                         \n",
    "train_loader_config = pop_config(cfg, 'train_loader', must_exist=True)\n",
    "\n",
    "tokenizer_config = pop_config(cfg, 'tokenizer', must_exist=True)\n",
    "tokenizer_name = tokenizer_config['name']\n",
    "tokenizer_kwargs = tokenizer_config.get('kwargs', {})\n",
    "tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)\n",
    "\n",
    "chat_tokenizer_name = tokenizer_config.get('chat_template_tokenizer', None)\n",
    "if chat_tokenizer_name is not None:\n",
    "    chat_tokenizer = AutoTokenizer.from_pretrained(chat_tokenizer_name)\n",
    "    tokenizer.chat_template = chat_tokenizer.chat_template\n",
    "\n",
    "device_train_batch_size = pop_config(cfg, 'device_train_batch_size', must_exist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-training/.venv/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 20000/20000 [00:04<00:00, 4994.60 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 4518 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Could you paraphrase the compliments given towards the model and her outfit? Answer according to: JEAN GREIGE by MADELINE PENDLETON: 9 - Biker Beach Gypsy?\n",
      "9 - Biker Beach Gypsy?\n",
      "There weren't a lot of photos to choose from today, as the camera battery became worn out as the memory card filled with pictures of my dog in a sad little cone.\n",
      "Doesn't she look pathetic? It's nothing major (she just has an ointment she isn't supposed to lick), but it sure does make her look adorably miserable.\n",
      "All of these photos were taken from the Photogenics website here. Isn't she amazing? I love that she is both tough and beautiful.\n",
      "fantastic outfit, I am lovvvving the skirt with the leather jacket.\n",
      "Adorable dog, awesome outfit, and that model kind of rules.\n",
      "Glad to have discovered your blog!\n",
      "wow, I love your outfit. That skirt is just amazing.\n",
      "just had an enjoyable browse through your blog, and I'm really liking this leather jacket, long skirt-sweater combo! your style is very eclectic, but to a whole new level! such a breath fresh air! a new follower indeed!\n",
      "come stop by for a visit and or follow too?!\n",
      "just came across your blog. I love it! I'll definitely be following.\n",
      "definitely love your headband, gosh ur dog is adoooorable !</s>\n",
      "<|assistant|>\n",
      "The compliments towards the model and her outfit in JEAN GREIGE by MADELINE PENDLETON's post titled \"9 - Biker Beach Gypsy\" are that the model is amazing for being tough and beautiful, and that her outfit consisting of a leather jacket and a long skirt is fantastic and amazing. The author also loves the model's headband and the dog in the pictures is adorable. The author is glad to have discovered the blog and will be a new follower. The author describes the style as eclectic and refreshing.</s>\n",
      "<|user|>\n",
      "Can you provide more outfit ideas from JEAN GREIGE by MADELINE PENDLETON that have a similar style as the one in \"9 - Biker Beach Gypsy\"?</s>\n",
      "<|assistant|>\n",
      "Certainly! Here are some other outfit ideas from JEAN GREIGE by MADELINE PENDLETON that have a similar style to the one in \"9 - Biker Beach Gypsy\":\n",
      "\n",
      "1. \"The Simple Things\" - This post features a simple white t-shirt paired with a denim shirt, a long black skirt, and black ankle boots. It has a similar bohemian feel to the outfit in \"9 - Biker Beach Gypsy.\"\n",
      "\n",
      "2. \"Tie Dye Dreams\" - This post features a tie-dye t-shirt paired with denim cut-offs, a black leather jacket, and black ankle boots. The leather jacket gives it a bit of edge, like the outfit in \"9 - Biker Beach Gypsy.\"\n",
      "\n",
      "3. \"70s Dreams\" - This post features a brown suede skirt paired with a white tank top, a denim shirt, and brown ankle boots. It has a similar vintage vibe to the outfit in \"9 - Biker Beach Gypsy.\"\n",
      "\n",
      "4. \"All White Everything\" - This post features a white tank top paired with white jeans, a denim shirt, and white ankle boots. The denim shirt and ankle boots add a bit of edge to an otherwise simple look, similar to the leather jacket in \"9 - Biker Beach Gypsy.\" \n",
      "\n",
      "I hope this gives you some inspiration!</s>\n",
      "<|user|>\n",
      "Can you tell me more about the style of JEAN GREIGE by MADELINE PENDLETON? Like, are there any specific pieces or colors that are common in their outfits?</s>\n",
      "<|assistant|>\n",
      "Sure! JEAN GREIGE by MADELINE PENDLETON has a unique and eclectic style that combines elements of bohemian, grunge, and vintage fashion. The outfits often feature oversized silhouettes, layered clothing, and textural contrast. Some common pieces in JEAN GREIGE's outfits include leather jackets, denim shirts, oversized sweaters or cardigans, maxi skirts, and ankle boots. Neutral colors such as black, white, brown, and grey are frequently used, along with earthy tones and muted pastels. JEAN GREIGE also incorporates statement accessories like wide-brimmed hats, necklaces, and belts to add interest to the outfits. Overall, JEAN GREIGE's style is a mix of edgy, feminine, and comfortable pieces that create a unique and eye-catching look.</s>\n",
      "\n",
      "Sample 3009 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Write a horror story in third person omniscient point of view about a group of five childhood friends, all now in their late twenties, who decide to reunite for a long weekend at a haunted bed and breakfast in the New England countryside. The story should begin with a chapter detailing their arrival at the isolated and decrepit Victorian home, from the ominous creak of the iron gate to the musty scent of the lobby. Each chapter should switch focus between the different friends, revealing bits of their individual past experiences with the supernatural and their current fears and anxieties. As the story progresses, the friends should encounter increasingly unsettling and inexplicable phenomena, from apparitions in the hallways to sudden temperature drops and disembodied whispers. The climax should reveal the true nature of the haunting, which is tied to a dark secret shared by all the friends. The story should end with a final twist and leave the reader questioning the true identity of the ghostly presence at the bed and breakfast.</s>\n",
      "<|assistant|>\n",
      "Chapter One: The Arrival\n",
      "\n",
      "The iron gate to the bed and breakfast creaked open as the five childhood friends drove through it. The tires of their rental car crunched against the gravel driveway as they approached the decrepit Victorian home. The once-grand house was now worn and weathered, the paint faded and peeling. The windows were boarded up and the porch sagged under their weight.\n",
      "\n",
      "As they approached the front door, they could smell the musty scent of the lobby. The air was thick with dust and the sounds of old wooden floors creaking underfoot. There were no other cars parked outside and no sign of any other guests.\n",
      "\n",
      "Despite the ominous feeling that hung in the air, the group of friends felt excited to be reunited after years apart. They had planned this long weekend as a way to catch up on old times and revisit their childhood memories.\n",
      "\n",
      "The owner of the bed and breakfast, a wiry old man with a toothy grin, greeted them at the door. His eyes seemed to dart around, taking in every detail of the group. He showed them to their rooms, each decorated in an old-fashioned style with antique furniture and lace curtains.\n",
      "\n",
      "As they settled in, the group couldn't help but feel a sense of unease. They couldn't quite put their finger on it, but there was something off about the place. It was as if the house itself was watching them.\n",
      "\n",
      "Chapter Two: Cam's Experience\n",
      "\n",
      "Cam lay in his bed, staring up at the ceiling. He couldn't shake the feeling that something was amiss. He had always been sensitive to the paranormal ever since he was a child. He had seen things that he couldn't explain, and had always been able to sense when something was wrong.\n",
      "\n",
      "As he lay there, he heard a faint whispering coming from the hallway. He sat up, straining to hear what was being said, but the voices were too faint to make out. He got out of bed and opened the door, but there was no one there.\n",
      "\n",
      "He shrugged it off and went back to bed. But as he lay there in the darkness, he felt a cold breeze waft over him. He pulled the blankets up to his chin and closed his eyes, hoping that sleep would come soon.\n",
      "\n",
      "Chapter Three: Alana's Experience\n",
      "\n",
      "Alana had always been fascinated by the supernatural. She had read every ghost story and watched every horror movie she could get her hands on. But now, as she lay in her bed, she regretted her fascination.\n",
      "\n",
      "She felt a hand brush against her cheek and a faint whisper. She sat up, heart pounding, but there was no one there. She reached for the lamp on her bedside table, but it wouldn't turn on. She tried several times, but the light bulb remained dark.\n",
      "\n",
      "She felt a cold breeze sweep through the room and goosebumps rose on her arms. She grabbed her phone and turned on the flashlight, hoping to find some explanation for what had just happened. But as she shone the light around the room, she saw nothing out of the ordinary.\n",
      "\n",
      "Chapter Four: Tyler's Experience\n",
      "\n",
      "Tyler had always been skeptical of the supernatural. He had a hard time believing in things that he couldn't see with his own eyes. But as he lay in bed, he couldn't deny the feeling of unease that gnawed at his stomach.\n",
      "\n",
      "He heard a faint scratching sound coming from under the bed. He peered over the edge, but saw nothing there. He lay back down, thinking that it was just his imagination playing tricks on him.\n",
      "\n",
      "But then he felt something grab his foot. He kicked out, but felt nothing there. He lay there, paralyzed with fear, as the sensation of something cold and clammy slithered over his leg.\n",
      "\n",
      "Chapter Five: Maya's Experience\n",
      "\n",
      "Maya had always been fascinated by the supernatural world, but she had never experienced anything firsthand. As she lay in bed, she felt a sense of dread creep over her. She heard a faint humming sound coming from the wall next to her bed.\n",
      "\n",
      "She got up and walked over to the wall, placing her ear against it. She could hear someone singing softly. The voice was strange, almost otherworldly. She couldn't make out the words, but the melody was hauntingly beautiful.\n",
      "\n",
      "She reached for the doorknob, but it wouldn't turn. She pushed harder, but the door remained stuck. She felt a cold breeze wash over her, and the singing grew louder.\n",
      "\n",
      "Chapter Six: Nick's Experience\n",
      "\n",
      "Nick had always been the joker of the group, but now he felt a sense of unease that he couldn't shake. He heard a faint tapping sound coming from the window, but when he drew back the curtains, there was nothing there.\n",
      "\n",
      "He sat back down on the bed, feeling a sense of weight pressing down on his chest. He heard the creaking of the bed and the sound of something breathing softly. He looked around the room, but there was no one there.\n",
      "\n",
      "He felt a hand touch his shoulder,</s>\n",
      "<|user|>\n",
      "The horror story you generated is great so far! can you add some more details about the dark history of the bed and breakfast? I feel like there's something sinister lurking in the past of this place.</s>\n",
      "<|assistant|>\n",
      "Chapter Seven: The Dark History\n",
      "\n",
      "As the weekend wore on, the group of friends couldn't shake the feeling that something was off about the bed and breakfast. They began to talk amongst themselves, sharing their strange experiences and trying to make sense of what was happening.\n",
      "\n",
      "Finally, they decided to ask the owner about the history of the place. He hesitated at first, but eventually agreed to share what he knew.\n",
      "\n",
      "According to him, the bed and breakfast had been built in the early 1900s by a wealthy family. The family had been beset by tragedy, with several members dying under mysterious circumstances. It was said that they had dabbled in the occult, and that their experiments had gone horribly wrong.\n",
      "\n",
      "The owner had bought the property in the 1960s and had converted it into a bed and breakfast. Over the years, he had heard stories of strange occurrences and had even experienced some himself. But he had always dismissed them as the ramblings of superstitious guests.\n",
      "\n",
      "As the friends delved deeper into the history of the place, they began to uncover dark secrets. They found old newspaper clippings detailing disappearances and deaths in the area, and rumors of satanic rituals that had taken place on the property.\n",
      "\n",
      "They soon realized that the haunting was not just a random occurrence, but was tied to the dark history of the bed and breakfast. And as they dug deeper, they began to uncover a truth that was even more terrifying than they could have imagined.\n",
      "\n",
      "Chapter Eight: The Truth Revealed\n",
      "\n",
      "As the friends continued to investigate the history of the bed and breakfast, they made a shocking discovery. They realized that all of them had a connection to the property, and to the dark history that lay hidden within its walls.\n",
      "\n",
      "Years ago, when they were all still children, they had played a game in the woods near the bed and breakfast. The game had involved a makeshift ritual, one that they had copied from a book they had found in the library.\n",
      "\n",
      "It had seemed harmless at the time, but now they realized that they had unwittingly summoned something dark and dangerous. The thing had attached itself to them, following them through their lives and bringing with it a sense of dread and unease.\n",
      "\n",
      "As the truth was revealed, the friends realized that they were in grave danger. The thing they had summoned all those years ago was still with them, and it was more powerful than ever before.\n",
      "\n",
      "Chapter Nine: The Final Encounter\n",
      "\n",
      "As the night wore on, the friends huddled together in fear, waiting for whatever was coming. They could hear footsteps in the hallway, and the sound of something whispering just beyond the door.\n",
      "\n",
      "Finally, they heard a scratching at the window. As they approached, they saw a figure standing outside, bathed in the eerie glow of the moonlight.\n",
      "\n",
      "It was the figure of a woman, with long hair the color of midnight and eyes that glowed with a malevolent light. The woman beckoned to them, and they felt themselves drawn towards her, powerless to resist.\n",
      "\n",
      "As they approached the window, the woman spoke, her voice soft and hypnotic. \"You have summoned me,\" she said. \"And now, you must pay the price.\"\n",
      "\n",
      "The friends realized too late that the thing they had summoned all those years ago was not a ghost or a demon, but something far more terrifying. It was a manifestation of their own darkest fears and desires, a creature born from their collective imagination and fueled by their own guilt.\n",
      "\n",
      "As the creature closed in for the final kill, the friends realized that they had become the architects of their own terror. The creature was not something that could be defeated, but something that they had created and nurtured, a shadow of their own psyche that would haunt them forever.</s>\n",
      "<|user|>\n",
      "Wow, that was intense! can you add some more details about the ritual the friends played as children? I'm curious to know what exactly they did to summon the creature.</s>\n",
      "<|assistant|>\n",
      "Chapter Ten: The Ritual\n",
      "\n",
      "After their final confrontation with the creature, the friends were left shaken and traumatized. They remembered the ritual that they had played as children and how it had led to the manifestation of their darkest fears and desires.\n",
      "\n",
      "They decided to delve deeper into the ritual, hoping to uncover more about what they had done and how to rid themselves of the creature for good.\n",
      "\n",
      "They searched through old journals and books, trying to piece together the fragments of memory that remained. Finally, they found an old journal belonging to one of Cam's relatives.\n",
      "\n",
      "The journal contained a detailed description of the ritual that the friends had played as children. It involved the use of an ancient talisman and a series of incantations designed to summon a powerful creature from the depths of the abyss.\n",
      "\n",
      "The journal described how the talisman had been imbued with the power of the creature, and how each of the friends had imbued it with their own desires and fears.\n",
      "\n",
      "As they read through the journal, the friends realized that they had all played a part in summoning the creature. Each of them had contributed their own fears and desires to its creation, and each of them had unwittingly nurtured its power over the years.\n",
      "\n",
      "They knew now that they could not simply banish the creature or destroy it. It was a part of them, a reflection of their own psychological makeup. They would have to come to terms with their own demons if they ever hoped to be free of the creature's influence.\n",
      "\n",
      "The friends left the bed and breakfast, carrying with them the knowledge of what they had done and the heavy burden of their own guilt. They knew that they would never be the same again, but they also knew that, in facing their fears and confronting their own demons, they had taken the first step towards redemption.</s>\n",
      "\n",
      "Sample 11631 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Analyze the impact of digital transformation on business models across various industries.</s>\n",
      "<|assistant|>\n",
      "The impact of digital transformation on business models across various industries has been significant in recent times. In this era of technology revolution, companies of all sizes have been forced to adapt and embrace digital transformation to remain competitive and relevant in their respective industries. Some ways in which digital transformation has impacted business models across various sectors include:\n",
      "\n",
      "1. Reduced Costs: Digital transformation has made it possible for companies to automate their processes, thereby reducing operational costs. This impact is particularly significant in sectors like manufacturing, retail, and logistics, where automation has revolutionized the supply chain.\n",
      "\n",
      "2. Improved Efficiency: Digital technologies have improved the way businesses interact with their customers and manage their operations. This impact is particularly evident in sectors like healthcare, finance, and insurance, where digital technologies have made it possible to provide better customer service with reduced turnaround times.\n",
      "\n",
      "3. New Revenue Streams: Digital transformation has led to the emergence of new revenue streams for businesses. Sectors like media, entertainment, and e-commerce have witnessed tremendous growth in revenue due to the proliferation of digital technologies.\n",
      "\n",
      "4. Increased Competition: Digital transformation has increased the level of competition in various industries. New players are coming up with disruptive business models that are powered by digital technologies. Established players who are slow to adapt risk losing market share to these new entrants.\n",
      "\n",
      "5. Enhanced Customer Experience: Digital transformation has enabled businesses to provide more personalized and customized experiences to their customers. From retail to hospitality, customers now expect businesses to tailor their experiences to meet their individual needs, preferences, and tastes.\n",
      "\n",
      "In conclusion, digital transformation has impacted business models across various industries in different ways. While some industries have embraced digital technologies faster than others, it is evident that digital transformation is now an essential aspect of any successful business strategy. Businesses that ignore the impact of digital transformation risk being left behind in a rapidly changing world.</s>\n",
      "<|user|>\n",
      "Could you elaborate more on how digital transformation has impacted the tourism sector?</s>\n",
      "<|assistant|>\n",
      "Certainly! Digital transformation has greatly impacted the tourism sector, and has transformed the way people plan and book their trips. Here are some of the ways digital transformation has impacted the tourism industry:\n",
      "\n",
      "1. Online booking platforms: The rise of online booking platforms like Expedia, Booking.com, and Airbnb has greatly impacted the tourism sector. These platforms have made it easier for travelers to research and book their trips online, which has led to the decline of traditional travel agencies. 2. Mobile apps: The use of mobile apps has made it easier for travelers to access information about their destination, book tours, and find local services. Apps like Tripadvisor and Airbnb have revolutionized the way travelers plan their trips, making it more convenient and accessible. 3. Virtual reality: Virtual reality technology has made it possible for tourists to experience destinations before they go, which has had a significant impact on the industry. This technology has made it easier for tourism operators to attract customers and provide a more immersive experience. 4. Social media: Social media has become an important tool for tourism operators to market their services and destinations. Platforms like Instagram, Facebook, and Twitter have made it easier for travelers to share their experiences and recommendations with others, which has helped tourism operators to reach new audiences. 5. Big data: The use of big data has enabled tourism operators to understand their customers better and provide more personalized experiences. By using data analytics, tourism operators can gain insights into customer preferences and behavior, which can be used to develop tailored marketing campaigns and targeted services. In summary, digital transformation has had a significant impact on the tourism sector, by making it easier for travelers to plan and book their trips online, providing more immersive experiences through virtual reality technology, and enabling tourism operators to gain insights into customer behavior through big data analytics.</s>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from train import build_dataloader\n",
    "\n",
    "dataloader = build_dataloader(\n",
    "    train_loader_config,\n",
    "    tokenizer,\n",
    "    device_train_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dataloader.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2429 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1950, 13305,   745,  ...,  1816,   304,   668],\n",
       "        [28725,   708,  3209,  ..., 28723,    13,    13],\n",
       "        [ 9968,  4779,   298,  ...,  3316,   302,  4289],\n",
       "        ...,\n",
       "        [28725, 18946,   304,  ..., 17271,   298, 18887],\n",
       "        [22141, 28723,   415,  ...,   264,   808,   302],\n",
       "        [19639,   297,  1008,  ...,  2553, 28723,  2909]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1950, 13305,   745,  ...,  1816,   304,   668],\n",
       "        [28725,   708,  3209,  ..., 28723,    13,    13],\n",
       "        [ 9968,  4779,   298,  ...,  3316,   302,  4289],\n",
       "        ...,\n",
       "        [28725, 18946,   304,  ..., 17271,   298, 18887],\n",
       "        [22141, 28723,   415,  ...,   264,   808,   302],\n",
       "        [19639,   297,  1008,  ...,  2553, 28723,  2909]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for search engines as a mortgage broker:\n",
      "\n",
      "1. Research keywords: Use keyword research tools such as Google Keyword Planner or SEMrush to identify relevant keywords that potential clients might use to search for mortgage brokers in your area. Use these keywords in your website's content, headings, and metadata.\n",
      "\n",
      "2. Meta tags: Ensure that your website's title tags and meta descriptions are optimized for search engines. These tags should be relevant, factual, and attention-grabbing.\n",
      "\n",
      "3. Valuable content: Create valuable content that is informative and engaging for your readers. Your website should have plenty of informative, well-written content that is optimized for search engines.\n",
      "\n",
      "4. Local Search: As a mortgage broker, focus on local searches in your area by including your location on your website's metadata and content. \n",
      "\n",
      "5. Mobile Friendly: Make sure your website is mobile-friendly by having a responsive design to accommodate mobile and tablet users.\n",
      "\n",
      "6. Site speed: Optimize your website's loading speeds by reducing the file size, compression, and enabling caching.\n",
      "\n",
      "7. Utilize Images and Videos: Include relevant images of your business, and informative videos to attract and engage your audience.\n",
      "\n",
      "By implementing these tips, you can improve your website's search engine optimization and increase your visibility online as a mortgage broker.</s> \n",
      "<|user|>\n",
      "Could you please suggest some topics related to the mortgage industry that would be interesting to potential clients for my blog?</s> \n",
      "<|assistant|>\n",
      "Yes! Here are some possible blog topics related to the mortgage industry that could be of interest to potential clients:\n",
      "\n",
      "1. How to improve your credit score to qualify for a better mortgage rate.\n",
      "2. The difference between fixed and variable rate mortgages.\n",
      "3. How to save for a down payment on a home.\n",
      "4. What first-time homebuyers need to know about the mortgage application process.\n",
      "5. The benefits of working with a mortgage broker versus a bank.\n",
      "6. The top myths about mortgages and why they're not true.\n",
      "7. How to refinance your mortgage and save money.\n",
      "8. The pros and cons of paying off your mortgage early.\n",
      "9. How to deal with unexpected mortgage rate increases.\n",
      "10. The role of mortgage insurance in the buying process.\n",
      "\n",
      "Remember, the key to a successful blog is providing valuable and informative content to your readers. By showcasing your knowledge and expertise on topics that are relevant to the mortgage industry, you can establish yourself as a thought leader and attract potential clients to your business.</s> \n",
      "</s><s> <|system|>\n",
      "</s> \n",
      "<|user|>\n",
      "What are some common side dishes or accompaniments that are served with this traditional food?</s> \n",
      "<|assistant|>\n",
      "I am not sure what traditional food you are referring to. Please provide more context, and I will be happy to assist you.</s> \n",
      "<|user|>\n",
      "Well, I was actually hoping to get some advice on traditional sides to serve with a classic roast beef dish. Can you help me out with that?</s> \n",
      "<|assistant|>\n",
      "Certainly! Some traditional side dishes that pair well with roast beef are:\n",
      "\n",
      "1. Roasted vegetables: Roast carrots, potatoes, onions, and other vegetables alongside the beef for a hearty and flavorful side dish.\n",
      "\n",
      "2. Yorkshire Pudding: A British classic often served with roast beef. It is made from flour, eggs, and milk, and is cooked in the oven with the beef.\n",
      "\n",
      "3. Creamed Spinach: Tender spinach leaves in a creamy sauce with garlic, onion and parmesan cheese.\n",
      "\n",
      "4. Horseradish Sauce: Horseradish sauce is a classic condiment that pairs particularly well with beef. It adds a spicy kick and brings out the flavor of the meat.\n",
      "\n",
      "5. Mashed Potatoes: Simple, but always a crowd-pleaser! Creamy, buttery mashed potatoes are the perfect complement to roast beef.</s> \n",
      "<|user|>\n",
      "Wow, those all sound like fantastic options! But can you recommend any side dishes that are a bit more unique or unusual to really impress my dinner guests?</s> \n",
      "<|assistant|>\n",
      "Sure! Here are some unique and unusual side dishes that would complement roast beef well:\n",
      "\n",
      "1. Pierogi: Pierogi are a traditional Polish dish that consists of a filling (usually potato, cheese or sauerkraut) that is wrapped in a thin dough and boiled or fried. They're a great way to add some flavor to your meal.\n",
      "\n",
      "2. Grilled Romaine Hearts: Halve the romaine hearts and grill them for a smoky, charred flavor. Top them with a vinaigrette or crumbled blue cheese.\n",
      "\n",
      "3. Roasted Fennel with Balsamic: Slice a fennel bulb and roast it with olive oil, salt and pepper, then drizzle it with a balsamic reduction for a sweet and tangy side dish.\n",
      "\n",
      "4. Herbed Quinoa Pilaf: Cook quinoa in vegetable broth with fresh herbs like thyme, rosemary and parsley, and add diced vegetables such as carrot, celery and onion.\n",
      "\n",
      "5. Roasted Brussels Sprouts with Bacon and Maple Syrup: Toss Brussels sprouts with bacon and maple syrup, then roast them in the oven for a sweet and savory side dish.</s> \n",
      "<|user|>\n",
      "Hmm, those unique side dish suggestions sound intriguing! I'm particularly interested in trying the grilled romaine hearts and the roasted fennel with balsamic. Do you have any tips on how to make those dishes stand out even more?</s> \n",
      "<|assistant|>\n",
      "Yes, of course! Here are some tips for making those side dishes stand out:\n",
      "\n",
      "1. Grilled Romaine Hearts: Cut the romaine hearts in half lengthways and brush them with olive oil. Grill them for a few minutes over high heat until they become slightly charred and wilted. Serve them topped with a balsamic glaze, crumbled blue cheese and toasted chopped nuts.\n",
      "\n",
      "2. Roasted Fennel with Balsamic: Cut the fennel into thin wedges and toss with olive oil, salt and pepper. Roast in the oven until tender and golden brown, then transfer to a serving dish. Drizzle with a balsamic reduction and sprinkle with chopped fresh herbs like parsley or dill.\n",
      "\n",
      "By adding some extra elements to the dishes like the toppings or special sauces or seasonings, you can turn simple side dishes into gourmet, restaurant-worthy dishes that will impress your dinner guests.</s> \n",
      "</s><s> <|system|>\n",
      "</s> \n",
      "<|user|>\n",
      "Create a guide on how to create your own homemade vegetable broth.</s> \n",
      "<|assistant|>\n",
      "Vegetable broth is a versatile ingredient that can be used as a base for soups, stews, or in place of water in various dishes, adding flavor and nutrition. The best part is that it can be easily made at home using simple ingredients. Here is a step-by-step guide on how to make your own homemade vegetable broth.\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "- 2-3 carrots\n",
      "- 2-3 celery stalks\n",
      "- 1 medium onion\n",
      "- 3-4 garlic cloves\n",
      "- 1-2 bay leaves\n",
      "- 1 teaspoon whole black pepper\n",
      "- 1 teaspoon salt\n",
      "- 8 cups of water\n",
      "- Optional: additional vegetables such as mushrooms, leek, parsley, thyme, or rosemary.\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Begin by preparing your vegetables. Wash the carrots and celery and chop them roughly into chunks. Peel the onion and cut into quarters. Peel the garlic cloves and crush them slightly.\n",
      "\n",
      "2. Place all your vegetables into a large pot. You can add any additional vegetables like mushrooms, leek, or herbs if you like, they will add more flavor.\n",
      "\n",
      "3. Add the bay leaves, whole black pepper and salt to the pot.\n",
      "\n",
      "4. Pour in 8 cups of water, enough to cover all the vegetables.\n",
      "\n",
      "5. Turn the heat to medium-high and let the broth come to a boil.\n",
      "\n",
      "6. Once boiling, lower the heat to simmer and let the broth cook for at least an hour, or until the vegetables have softened and the broth is flavorful.\n",
      "\n",
      "7. Remove the pot from the heat and let it cool slightly.\n",
      "\n",
      "8. Using a strainer, strain the broth into a large bowl or container, pressing down on the vegetables to extract as much liquid as possible.\n",
      "\n",
      "9. Discard the solid vegetables and other ingredients.\n",
      "\n",
      "10. If the broth is too concentrated, you can add more water to dilute it. Taste the broth and add more seasoning if necessary.\n",
      "\n",
      "11. Once the broth has cooled completely, you can store it in\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(batch[\"input_ids\"])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-training/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/rshaw/zephyr-training/.venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/mistral-7b-sft-beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(example, tokenizer):\n",
    "    messages = example[\"messages\"]\n",
    "    # We add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    return example\n",
    "\n",
    "train_dataset = dataset.map(apply_chat_template, fn_kwargs={\"tokenizer\": tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 174456 of the processed training set:\n",
      "\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Write a biting and humorous satire that lampoons the excessive snobbery and self-importance of the contemporary restaurant scene, poking fun at everything from complicated menus and pretentious food descriptions to the haughty attitudes of chefs and servers. Use exaggerated characters, tongue-in-cheek language, and clever irony to skewer the ridiculousness of an industry that takes itself too seriously and alienates customers with its elitist airs. Focus on witty observations and clever parodies of common restaurant tropes, such as foodie culture, farm-to-table trends, and molecular gastronomy. Above all, aim to entertain and amuse with your clever skewering of an all-too familiar target.</s>\n",
      "<|assistant|>\n",
      "As the maitre d' led me to my seat at the latest trendy restaurant, he gave me a disdainful glance that made me feel like a peasant showing up to a royal banquet uninvited. With a menu that looked like a Shakespearean soliloquy, I struggled to decipher their pretentious dishes. Was I really expected to know what \"seared monkfish with paprika-infused quinoa and a reduction of foraged elderberries\" tasted like?\n",
      "\n",
      "But never fear, the waiter was there to translate the menu for me, describing each dish with a flourish of arrogance that made me question my own intelligence. \"The sea urchin foam with nitrogen-infused sea grapes and a hint of smoked rosemary is a delicate piece of artistry,\" he pompously proclaimed, ignoring my bewildered expression.\n",
      "\n",
      "I couldn't help but think: why does eating out have to be such a highbrow experience? Why can't I just order a good old-fashioned burger without feeling like a culinary philistine? This obsession with obscure, locally-sourced ingredients and molecular gastronomy techniques is alienating and ridiculous.\n",
      "\n",
      "And don't even get me started on the chefs themselves. They parade around the dining room like medieval kings, doling out praise and disdain in equal measure. As if their culinary creations were more important than world peace or ending poverty.\n",
      "\n",
      "\"Oh, you don't like truffle oil? Clearly, your unsophisticated palate can't handle refined flavors,\" snorted the chef, his nose in the air.\n",
      "\n",
      "But it's not just the food that's become a spectacle in itself. The restaurant scene has also developed its own cult of personality. Chef fetishism and foodie culture have created a class of worshippers, who will spend hours waiting in line for a table and pay exorbitant prices just for the bragging rights.\n",
      "\n",
      "And let's not forget about the farm-to-table trend. Suddenly, every restaurant worth its salt (or fleur de sel) has to have its own organic garden, and the waitstaff all have to don plaid shirts and suspenders like they've just come back from a day's manual labor on the farm. But are we really supposed to believe that those heirloom tomatoes were grown just for us, and not to fulfill the restaurant's Instagram aesthetic?\n",
      "\n",
      "It's time for the restaurant industry to take a cue from the rest of society and stop taking itself so seriously. Let's put the focus back on good food and good company, and leave the haughtiness and exclusivity at the door. Who knows, maybe we'll be able to enjoy a meal without feeling like we've just sat through a lecture at a high-end culinary school.</s>\n",
      "<|user|>\n",
      "This is hilarious! I can totally relate to the snobbery and self-importance of some restaurants. Can you please add more examples of ridiculous food descriptions and parodies of foodie culture? the more outrageous, the better!</s>\n",
      "<|assistant|>\n",
      "Absolutely, I'm glad you're enjoying it! Here are a few more examples of over-the-top food descriptions and parodies of foodie culture:\n",
      "\n",
      "- \"Our deconstructed beet salad features a minimalist presentation of microgreens, microbeets, and microfeta, all drizzled with a balsamic reduction that will make your taste buds sing a symphony of flavor.\" Translation: We took apart a salad and served it in tiny portions for no reason.\n",
      "\n",
      "- \"Our grass-fed, hand-massaged Wagyu beef is sourced from a farm so exclusive, it only serves its cows organic champagne.\" Translation: We're charging you $100 for something you could get at the grocery store.\n",
      "\n",
      "- \"Our chef's special molecular gastronomy creation is a feast for the senses - edible flowers burst with flavor in your mouth, while the liquid nitrogen-infused foam washes over your palate. It's like experiencing the Big Bang in your mouth.\" Translation: We're using science to make food taste weird because we're bored.\n",
      "\n",
      "- \"We source our artisanal honey from a beekeeper who personally sings to each hive to ensure optimum honey production. You can taste the love and care in every spoonful.\" Translation: We're trying really hard to make something as basic as honey sound special.\n",
      "\n",
      "- \"Our farm-to-table experience means that every dish is infused with the essence of the land. From the misty mornings to the gentle breezes, every flavor profile tells a story of our region's terroir.\" Translation: We're just trying to impress you with fancy words, but really we just ordered our produce from a local farmer.\n",
      "\n",
      "I hope these examples give you a good chuckle! Remember, sometimes the best way to enjoy food is to not take it too seriously.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for index in random.sample(range(len(train_dataset)), 1):\n",
    "    print(f\"Sample {index} of the processed training set:\\n\\n{train_dataset[index]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from trl.trainer.utils import ConstantLengthDataset\n",
    "\n",
    "packed_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    formatting_func=None,\n",
    "    seq_length=2048,\n",
    "    infinite=False,\n",
    "    num_of_sequences=1024,\n",
    "    chars_per_token=3.6,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": 16,\n",
    "    \"collate_fn\": collator,\n",
    "}\n",
    "\n",
    "print(isinstance(packed_dataset, torch.utils.data.IterableDataset))\n",
    "\n",
    "dl = DataLoader(packed_dataset, **dataloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2048])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understanding. Utilize sensory exploration, through water play, sandboxes, and manipulatives to keep student's minds strong and engaged.\n",
      "\n",
      "5. Encourage Questions and Discussion: Encourage children to ask questions and discuss lessons. By asking questions, children can solidify their understanding of the topic and the discussion promotes communication and analysis of new ideas—which is vital to their education.\n",
      "\n",
      "6. Have a Schedule: Have a basic schedule for each day covering major topics and activities to allow for maximum use of available time. Be flexible, but always have a lesson plan at hand, to maximize time and ensure that your students are mindful.\n",
      "\n",
      "Remember, lesson plans can change for multiple reasons, but always keep it engaging, interactive, and fun as the intellectual and emotional growth of preschoolers are reliant on them having a truly engaging educational experience.</s> \n",
      "<|user|>\n",
      "Thanks for the tips on how to plan creative lessons and build relationships with parents and colleagues. Can you suggest some resources or websites where I can find more activities and materials to incorporate into my lessons?</s> \n",
      "<|assistant|>\n",
      "Yes, there are numerous resources and websites available to help preschool teachers find activities and materials to incorporate into their lessons. Here are some popular websites for you to explore further:\n",
      "\n",
      "1. Education.com: This website offers a wide range of resources, including lesson plans, worksheets, games, and activities for preschool teachers.\n",
      "\n",
      "2. Scholastic: Scholastic offers free lesson plans, hands-on activities, and printable resources to help preschool teachers support their students’ literacy development.\n",
      "\n",
      "3. PBS Kids for Educators: PBS Kids provides teachers with access to educational games, videos, and lesson plans to support preschool learning.\n",
      "\n",
      "4. Teachers Pay Teachers: This website is a marketplace for teachers to browse, buy, and sell educational resources, including lesson plans, activities, and games.\n",
      "\n",
      "5. Pre-K Pages: This website provides resources specifically for preschool teachers, including lesson plans, games, and printables.\n",
      "\n",
      "6. Teach Preschool: Teach Preschool offers ideas and inspiration for preschool teachers, including lesson plans, art activities, science experiments, and more.\n",
      "\n",
      "These resources and websites will provide you with access to new and fresh ideas and materials that you can use to enhance your classroom experience.</s> \n",
      "</s><s> <|system|>\n",
      "</s> \n",
      "<|user|>\n",
      "Please create a seven-day meal plan with breakfast, lunch, dinner, and snacks that offers a balanced and healthy diet. The meal plan should contain meat and vegetarian options, incorporate whole grains, vegetables, fruits, and healthy fats. Please provide a detailed description of each meal, including portion sizes and recommended cooking methods. Additionally, please consider dietary restrictions and allergies when selecting ingredients.</s> \n",
      "<|assistant|>\n",
      "Monday:\n",
      "\n",
      "Breakfast: Avocado Toast with a Poached Egg and a Mixed Berry Smoothie.\n",
      "For the avocado toast, toast one slice of whole-grain bread and top with smashed avocado, a poached egg, salt, pepper, and a squeeze of lime juice. Serve with a mixed berry smoothie made with mixed berries, almond milk, and a scoop of protein powder.\n",
      "Snack: Sliced apples with almond butter.\n",
      "Lunch: Greek Chicken Pita.\n",
      "Fill a whole-grain pita with grilled chicken, tomatoes, cucumber, red onion, feta cheese, and tzatziki sauce. Serve with a side of baby carrots.\n",
      "Snack: Hummus with veggie dippers.\n",
      "Dinner: One-Pan Baked Cod and Vegetables. \n",
      "Bake a fillet of cod and mixed vegetables (such as bell peppers, cherry tomatoes, and onions) with olive oil, salt, and pepper. Serve with a side of quinoa.\n",
      "\n",
      "Tuesday:\n",
      "\n",
      "Breakfast: Banana Pancakes with Berries and Greek Yogurt.\n",
      "Mix together mashed bananas, oats, egg, and cinnamon to make pancakes. Top with fresh berries and Greek yogurt.\n",
      "Snack: Edamame with sea salt.\n",
      "Lunch: Roasted Vegetable Salad with Grilled Chicken.\n",
      "Roast mixed vegetables (such as zucchini, eggplant, and bell peppers) and serve over a bed of greens with grilled chicken and balsamic vinaigrette.\n",
      "Snack: Chopped veggies with ranch dipping sauce.\n",
      "Dinner: Vegetarian Stir-Fry.\n",
      "Stir-fry mixed vegetables (such as broccoli, bell peppers, carrots, and snow peas) with tofu in soy sauce, garlic, and ginger. Serve with brown rice.\n",
      "\n",
      "Wednesday:\n",
      "\n",
      "Breakfast: Green Smoothie and Hard-Boiled Egg.\n",
      "Blend together spinach, frozen banana, almond milk, and protein powder to make a smoothie. Enjoy with a hard-boiled egg.\n",
      "Snack: Yogurt with granola and blueberries.\n",
      "Lunch: Turkey and Cheese Wrap.\n",
      "Fill a whole-grain wrap with turkey, cheese, spinach, and honey mustard. Serve with a side of apple slices.\n",
      "Snack: Air-popped popcorn with a sprinkle of salt.\n",
      "Dinner: Baked Salmon with Roasted Broccoli.\n",
      "Bake a salmon fillet with lemon and herbs. Serve with roasted broccoli and sweet potato wedges.\n",
      "\n",
      "Thursday:\n",
      "\n",
      "Breakfast: Greek Yogurt Parfait with Berries and Almonds.\n",
      "Layer Greek yogurt with fresh berries, almonds, and a drizzle of honey.\n",
      "Snack: Carrots with hummus.\n",
      "Lunch: Grilled Vegetable Panini.\n",
      "Grill mixed vegetables (such as eggplant, zucchini, and peppers) and serve on a whole-grain bread with pesto and mozzarella. Serve with a side of watermelon.\n",
      "Snack: Rice cakes with almond butter.\n",
      "Dinner: Beef Tacos with Black Beans and Corn.\n",
      "Cook ground beef with taco seasoning and serve in a whole-grain taco shell with black beans, corn, and avocado.\n",
      "\n",
      "Friday:\n",
      "\n",
      "Breakfast: Veggie Omelet with Whole Grain Toast and a Side of Berries.\n",
      "Whisk together eggs with vegetables (such as mushrooms, spinach, tomato, onion, and bell peppers) and serve with whole-grain toast and a side of mixed berries.\n",
      "Snack: Cottage cheese with fruit.\n",
      "Lunch: Goat Cheese Salad with Chicken and Pears.\n",
      "Top mixed greens with grilled chicken, goat cheese, sliced pears, and balsamic vinaigrette.\n",
      "Snack: Roasted almonds with sea salt.\n",
      "Dinner: Lentil Soup and Whole Grain Bread.\n",
      "Cook lentils with vegetables (such as carrots, onion, and celery) in vegetable broth. Serve with a slice of whole-grain bread.\n",
      "\n",
      "Saturday:\n",
      "\n",
      "Breakfast: Avocado and Egg Toast with Fresh Squeezed Orange Juice.\n",
      "Toast whole-grain bread and top with mashed avocado, a fried egg, salt, and pepper. Enjoy with fresh squeezed orange juice.\n",
      "Snack: Greek yogurt with mixed nuts.\n",
      "Lunch: Grilled Chicken Caesar Salad.\n",
      "Grill chicken and top a bed of romaine lettuce with croutons, parmesan cheese, and Caesar dressing.\n",
      "Snack: Apple slices with peanut butter.\n",
      "Dinner: Vegetarian Spaghetti Squash with Meat Sauce.\n",
      "Cook spaghetti squash in the oven and top with marinara sauce and ground turkey meatballs.\n",
      "\n",
      "Sunday:\n",
      "\n",
      "Breakfast: Scrambled Eggs with Cinnamon Sweet Potato Hash and Fresh Fruit.\n",
      "Scramble eggs with spinach and cheese, and cook sliced sweet potatoes with cinnamon and olive oil. Serve with fresh fruit.\n",
      "Snack: In-season fruit.\n",
      "Lunch: Turkey Club Sandwich.\n",
      "Layer turkey, bacon, lettuce, tomatoes, avocado, and mayo on whole-grain bread. Serve with a side of carrots and celery sticks.\n",
      "Snack: Apple slices with cheese slices.\n",
      "Dinner: Pork Fried Rice with Mixed Vegetables.\n",
      "Stir-fry cooked pork and mixed vegetables (such as carrots, peas, and bell peppers) in sesame oil</s> \n",
      "<|user|>\n",
      "This meal plan looks great! However, I notice that there are more meat options compared to vegetarian options. Can you add more vegetarian meal options for me, please?</s> \n",
      "<|assistant\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(batch[\"input_ids\"])[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Chat Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from scripts.chat_dataloader import build_chat_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf as om\n",
    "from llmfoundry.utils.config_utils import pop_config\n",
    "from llmfoundry.utils.builders import build_tokenizer\n",
    "\n",
    "yaml_path = \"/data/yamls/run2.yaml\"\n",
    "with open(yaml_path) as f:\n",
    "    cfg = om.load(f)\n",
    "om.resolve(cfg)\n",
    "                                         \n",
    "train_loader_config = pop_config(cfg, 'train_loader', must_exist=True)\n",
    "\n",
    "tokenizer_config = pop_config(cfg, 'tokenizer', must_exist=True)\n",
    "tokenizer_name = tokenizer_config['name']\n",
    "tokenizer_kwargs = tokenizer_config.get('kwargs', {})\n",
    "tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_train_batch_size = pop_config(cfg, 'device_train_batch_size', must_exist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scripts\n"
     ]
    }
   ],
   "source": [
    "%cd scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fecc9ebfcf64fef83105fc3d9cbc26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scripts.train import build_dataloader\n",
    "\n",
    "dataloader = build_dataloader(\n",
    "    train_loader_config,\n",
    "    tokenizer,\n",
    "    device_train_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21733,   798,  4893,   297, 22303, 12638, 28723,     2, 28705,    13])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[idx,-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21733,   798,  4893,   297, 22303, 12638, 28723,     2, 28705,    13])\n"
     ]
    }
   ],
   "source": [
    "print(labels[idx,-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,     1,     1,  ...,     2, 28705,    13])\n",
      "<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s> <|system|>\n",
      "You are a friendly chatbot</s> \n",
      "<|user|>\n",
      "Examine the various ways in which political patronage has affected the quality and effectiveness of public services, including but not limited to issues related to accountability, efficiency, and democratic governance. Consider the historical and contemporary contexts in which patronage has operated, and assess the potential outcomes of different policy interventions aimed at minimizing its negative effects. Back up your analysis with empirical evidence, case studies, and relevant research literature.</s> \n",
      "<|assistant|>\n",
      "Political patronage is the practice of awarding individuals with government jobs or contracts solely based on their political affiliation or support for politicians in power. This practice has permeated various aspects of public service delivery and had significant consequences on the quality and effectiveness of these services, undermining accountability, efficiency, and democratic governance.\n",
      "\n",
      "Historically, political patronage has been pervasive in many countries, including the United States. For instance, in the 19th century, political bosses in major cities used patronage to build political machines that controlled local politics and used public resources to reward their supporters. In contemporary times, political patronage continues to be prevalent in many developing and developed countries, with politicians frequently using public resources to reward their supporters.\n",
      "\n",
      "One of the main ways in which political patronage affects public services is by undermining accountability. When political considerations and loyalty are the basis for hiring or awarding contracts, the best-qualified candidates or companies may be overlooked in favor of less competent but politically connected actors. This undermines meritocracy and creates a culture of impunity among politicians and public officials, which erodes public trust in the government.\n",
      "\n",
      "Additionally, political patronage can also reduce efficiency by impeding the implementation of policies that could benefit the general public. Politicians often prioritize projects and policies that appease their supporters over those that may have more significant benefits for the public at large, which can lead to wasteful spending and a lack of progress on critical issues.\n",
      "\n",
      "Furthermore, political patronage can have adverse consequences for democratic governance. When politicians use public resources to reward their supporters, they create a cycle of corruption and a culture of dependency that undermines the legitimacy of democratic institutions. This can lead to widespread disillusionment and a lack of faith in the ability of democratic institutions to deliver for the greater good.\n",
      "\n",
      "Several policy interventions can help to minimize the negative effects of political patronage. The implementation of robust anti-corruption mechanisms, such as creating an independent audit office, strengthening whistleblower protections, and enhancing transparency in the procurement process, can help to mitigate corruption risks. Improved civil service reform, such as merit-based hiring, tenure, and performance-based evaluations, could ensure that public service delivery is not influenced by political considerations. Finally, institutional changes, such as creating stronger checks and balances, can also help to reduce the influence of political patronage on public service delivery.\n",
      "\n",
      "Empirical evidence and research literature demonstrate the negative effects of political patronage on public service delivery. For instance, a study of public procurement in India found that political interference and patronage led to low-quality infrastructure and higher procurement costs. Similarly, a World Bank study found that political patronage was a significant barrier to effective public service delivery in Kenya.\n",
      "\n",
      "In conclusion, political patronage has significant consequences for the quality and effectiveness of public services, including but not limited to issues related to accountability, efficiency, and democratic governance. The adoption of robust anti-corruption mechanisms, civil service reform, and institutional changes can mitigate the adverse effects of political patronage, promote public trust in democratic institutions, and help to deliver better public services.</s> \n",
      "<|user|>\n",
      "Thank you for providing a comprehensive analysis of political patronage and its effect on public service delivery. I would like to know more about the role of civil society in fighting against political patronage. Can you please provide more information and examples of how civil society can help minimize the negative effects of political patronage on public services?</s> \n",
      "<|assistant|>\n",
      "Certainly! Civil society plays a crucial role in fighting against political patronage and promoting good governance. Here are some examples of how civil society can help minimize the negative effects of political patronage on public services:\n",
      "\n",
      "1. Advocacy and Awareness-raising: Civil society organizations can play a vital role in raising awareness about the negative effects of political patronage on public services. They can generate public pressure on the authorities to carry out reforms aimed at reducing political interference in the hiring process and procurement. Civil society groups can also educate citizens on their role in monitoring public service delivery and exposing corrupt practices.\n",
      "\n",
      "2. Monitoring and Oversight: Civil society organizations can undertake independent monitoring and evaluation of public services, identify areas where patronage is being practiced, and provide evidence of such practices to the authorities. This leads to accountability and transparency in decision-making.\n",
      "\n",
      "3. Building Capacities of communities to Monitor Public Services: Civil society organizations can help communities develop and implement mechanisms for monitoring public service delivery, including tracking the distribution of public resources and oversight of infrastructure projects. This makes it difficult for politicians and their proxies to engage in patronage practices without being detected.\n",
      "\n",
      "4. Mobilization of Citizens: Civil society organizations can mobilize citizens to demand accountability and transparency in the allocation of public resources. They can also engage stakeholders across the political spectrum in advocating for reforms aimed at reducing political patronage in public service delivery.\n",
      "\n",
      "Some examples of successful civil society interventions against political patronage include the following:\n",
      "\n",
      "- The Zero Tolerance Campaign for Public Service Corruption in Malawi, which involved civil society organizations and faith-based groups campaigning for transparency, accountability, and meritocracy in the recruitment of public officials in the country.\n",
      "\n",
      "- The Citizens' Monitoring of Public Procurement campaign in the Philippines, which involved civil society organizations and the media working together to monitor public procurement processes and scrutinizing projects funded by the government.\n",
      "\n",
      "- The Provision of Social Services by the Ugandan Government study, which included civil society organizations and local communities in monitoring the delivery of public services. \n",
      "\n",
      "In conclusion, civil society can play a crucial role in reducing the negative effects of political patronage on public services. By advocating for good governance, monitoring and oversight, building community capacities to monitor public services, and mobilizing citizens, civil society can contribute to making public service delivery more efficient, fair, and accountable.</s> \n",
      "<|user|>\n",
      "Thank you for your response on the role of civil society in the fight against political patronage. Your insights have been very helpful. However, I would like you to provide more examples of successful civil society interventions against political patronage in Africa. Can you please share more on this?</s> \n",
      "<|assistant|>\n",
      "Certainly! Here are some additional examples of successful civil society interventions against political patronage in Africa:\n",
      "\n",
      "1. The Citizens' Budget Initiative (CBI) in Kenya: This project involves civil society organizations and citizen groups working to improve transparency and accountability in the budget process. The CBI has developed tools and training materials to help citizens understand government budgets and monitor the allocation of resources. Through the CBI, citizen groups have been able to engage with policymakers and scrutinize public expenditure to ensure that it is in line with public needs and priorities.\n",
      "\n",
      "2. The FixMyCommunity Platform in Nigeria: This platform, developed by the Civil Society Legislative Advocacy Centre (CISLAC), allows citizens to report instances of corruption and poor service delivery in their communities. The platform has been able to track the delivery of public services such as road repairs, sanitation, and healthcare in several Nigerian cities, leading to improved service delivery and reduced corruption.\n",
      "\n",
      "3. The Mahali Center for Good Governance and Civic Engagement in Tanzania: This center works to promote good governance, accountability, and civic participation in Tanzania. The Mahali Center has been instrumental in promoting the use of social accountability tools such as participatory budgeting and community scorecards to monitor public services and hold officials accountable.\n",
      "\n",
      "4. The Social Accountability Monitoring in Mozambique Project: This project, implemented by the Centre for Public Integrity (CIP), is a network of civil society organizations, journalists, and community members working to promote transparency and accountability in public procurement in Mozambique. The project has been able to expose instances of corruption and influence policy reforms aimed at reducing patronage in public procurement.\n",
      "\n",
      "In all of these cases, civil society organizations have been able to successfully challenge political patronage by promoting transparency, accountability, and citizens' active participation in the governance process. These interventions have led to improved service delivery, reduced corruption, and enhanced public trust in democratic institutions.</s> \n",
      "\n",
      "1: 13 // 13\n",
      "2: 28705 // 28705\n",
      "3: 2 // 2\n",
      "4: 28723 // 28723\n",
      "5: 12638 // 12638\n",
      "6: 22303 // 22303\n",
      "7: 297 // 297\n",
      "8: 4893 // 4893\n",
      "9: 798 // 798\n",
      "10: 21733 // 21733\n",
      "11: 304 // 304\n",
      "12: 28725 // 28725\n",
      "13: 21363 // 21363\n",
      "14: 9397 // 9397\n",
      "15: 28725 // 28725\n",
      "16: 9562 // 9562\n",
      "17: 2372 // 2372\n",
      "18: 11957 // 11957\n",
      "19: 298 // 298\n",
      "20: 4163 // 4163\n",
      "21: 506 // 506\n",
      "22: 27319 // 27319\n",
      "23: 791 // 791\n",
      "24: 2957 // 2957\n",
      "25: 28723 // 28723\n",
      "26: 1759 // 1759\n",
      "27: 617 // 617\n",
      "28: 2411 // 2411\n",
      "29: 272 // 272\n",
      "30: 297 // 297\n",
      "31: 17002 // 17002\n",
      "32: 5038 // 5038\n",
      "33: 28742 // 28742\n",
      "34: 9893 // 9893\n",
      "35: 304 // 304\n",
      "36: 28725 // 28725\n",
      "37: 2437 // 2437\n",
      "38: 2708 // 2708\n",
      "39: 28725 // 28725\n",
      "40: 23767 // 23767\n",
      "41: 1203 // 1203\n",
      "42: 21325 // 21325\n",
      "43: 486 // 486\n",
      "44: 465 // 465\n",
      "45: 20367 // 20367\n",
      "46: 4355 // 4355\n",
      "47: 8035 // 8035\n",
      "48: 10130 // 10130\n",
      "49: 298 // 298\n",
      "50: 2358 // 2358\n",
      "51: 750 // 750\n",
      "52: 506 // 506\n",
      "53: 9909 // 9909\n",
      "54: 6190 // 6190\n",
      "55: 6259 // 6259\n",
      "56: 28725 // 28725\n",
      "57: 4469 // 4469\n",
      "58: 1167 // 1167\n",
      "59: 302 // 302\n",
      "60: 544 // 544\n",
      "61: 657 // 657\n",
      "62: 13 // 13\n",
      "63: 13 // 13\n",
      "64: 28723 // 28723\n",
      "65: 466 // 466\n",
      "66: 482 // 482\n",
      "67: 13053 // 13053\n",
      "68: 798 // 798\n",
      "69: 297 // 297\n",
      "70: 465 // 465\n",
      "71: 20367 // 20367\n",
      "72: 17032 // 17032\n",
      "73: 438 // 438\n",
      "74: 17939 // 17939\n",
      "75: 28713 // 28713\n",
      "76: 9861 // 9861\n",
      "77: 4920 // 4920\n",
      "78: 8015 // 8015\n",
      "79: 304 // 304\n",
      "80: 21363 // 21363\n",
      "81: 302 // 302\n",
      "82: 13290 // 13290\n",
      "83: 26411 // 26411\n",
      "84: 298 // 298\n",
      "85: 2358 // 2358\n",
      "86: 750 // 750\n",
      "87: 659 // 659\n",
      "88: 2488 // 2488\n",
      "89: 415 // 415\n",
      "90: 28723 // 28723\n",
      "91: 1651 // 1651\n",
      "92: 1379 // 1379\n",
      "93: 20915 // 20915\n",
      "94: 297 // 297\n",
      "95: 466 // 466\n",
      "96: 482 // 482\n",
      "97: 13053 // 13053\n",
      "98: 798 // 798\n",
      "99: 297 // 297\n",
      "100: 2437 // 2437\n",
      "101: 2708 // 2708\n",
      "102: 304 // 304\n",
      "103: 23767 // 23767\n",
      "104: 1203 // 1203\n",
      "105: 11711 // 11711\n",
      "106: 298 // 298\n",
      "107: 2739 // 2739\n",
      "108: 3338 // 3338\n",
      "109: 3618 // 3618\n",
      "110: 304 // 304\n",
      "111: 28725 // 28725\n",
      "112: 26197 // 26197\n",
      "113: 28725 // 28725\n",
      "114: 9909 // 9909\n",
      "115: 6190 // 6190\n",
      "116: 6259 // 6259\n",
      "117: 302 // 302\n",
      "118: 3681 // 3681\n",
      "119: 264 // 264\n",
      "120: 349 // 349\n",
      "121: 557 // 557\n",
      "122: 2665 // 2665\n",
      "123: 28743 // 28743\n",
      "124: 325 // 325\n",
      "125: 472 // 472\n",
      "126: 18208 // 18208\n",
      "127: 4271 // 4271\n",
      "128: 354 // 354\n",
      "129: 11304 // 11304\n",
      "130: 272 // 272\n",
      "131: 486 // 486\n",
      "132: 13492 // 13492\n",
      "133: 28725 // 28725\n",
      "134: 2488 // 2488\n",
      "135: 851 // 851\n",
      "136: 28747 // 28747\n",
      "137: 7759 // 7759\n",
      "138: 1651 // 1651\n",
      "139: 1379 // 1379\n",
      "140: 20915 // 20915\n",
      "141: 297 // 297\n",
      "142: 288 // 288\n",
      "143: 27838 // 27838\n",
      "144: 2437 // 2437\n",
      "145: 11919 // 11919\n",
      "146: 7575 // 7575\n",
      "147: 415 // 415\n",
      "148: 28723 // 28723\n",
      "149: 28781 // 28781\n",
      "150: 13 // 13\n",
      "151: 13 // 13\n",
      "152: 28723 // 28723\n",
      "153: 522 // 522\n",
      "154: 2708 // 2708\n",
      "155: 10755 // 10755\n",
      "156: 2400 // 2400\n",
      "157: 304 // 304\n",
      "158: 3345 // 3345\n",
      "159: 798 // 798\n",
      "160: 9009 // 9009\n",
      "161: 298 // 298\n",
      "162: 24073 // 24073\n",
      "163: 7420 // 7420\n",
      "164: 3618 // 3618\n",
      "165: 304 // 304\n",
      "166: 288 // 288\n",
      "167: 8326 // 8326\n",
      "168: 5377 // 5377\n",
      "169: 4105 // 4105\n",
      "170: 390 // 390\n",
      "171: 1259 // 1259\n",
      "172: 7040 // 7040\n",
      "173: 2437 // 2437\n",
      "174: 2708 // 2708\n",
      "175: 2809 // 2809\n",
      "176: 302 // 302\n",
      "177: 938 // 938\n",
      "178: 272 // 272\n",
      "179: 21325 // 21325\n",
      "180: 297 // 297\n",
      "181: 28166 // 28166\n",
      "182: 750 // 750\n",
      "183: 659 // 659\n",
      "184: 6064 // 6064\n",
      "185: 4827 // 4827\n",
      "186: 10947 // 10947\n",
      "187: 415 // 415\n",
      "188: 28723 // 28723\n",
      "189: 4877 // 4877\n",
      "190: 4879 // 4879\n",
      "191: 320 // 320\n",
      "192: 297 // 297\n",
      "193: 17002 // 17002\n",
      "194: 294 // 294\n",
      "195: 13269 // 13269\n",
      "196: 304 // 304\n",
      "197: 28725 // 28725\n",
      "198: 2437 // 2437\n",
      "199: 2708 // 2708\n",
      "200: 28725 // 28725\n",
      "201: 617 // 617\n",
      "202: 2411 // 2411\n",
      "203: 1179 // 1179\n",
      "204: 11711 // 11711\n",
      "205: 298 // 298\n",
      "206: 3791 // 3791\n",
      "207: 4982 // 4982\n",
      "208: 851 // 851\n",
      "209: 28747 // 28747\n",
      "210: 4877 // 4877\n",
      "211: 4879 // 4879\n",
      "212: 320 // 320\n",
      "213: 297 // 297\n",
      "214: 3012 // 3012\n",
      "215: 2317 // 2317\n",
      "216: 294 // 294\n",
      "217: 449 // 449\n",
      "218: 334 // 334\n",
      "219: 304 // 304\n",
      "220: 617 // 617\n",
      "221: 6696 // 6696\n",
      "222: 5801 // 5801\n",
      "223: 354 // 354\n",
      "224: 6064 // 6064\n",
      "225: 4827 // 4827\n",
      "226: 10947 // 10947\n",
      "227: 415 // 415\n",
      "228: 28723 // 28723\n",
      "229: 28770 // 28770\n",
      "230: 13 // 13\n",
      "231: 13 // 13\n",
      "232: 28723 // 28723\n",
      "233: 21363 // 21363\n",
      "234: 9397 // 9397\n",
      "235: 304 // 304\n",
      "236: 9562 // 9562\n",
      "237: 2372 // 2372\n",
      "238: 11957 // 11957\n",
      "239: 298 // 298\n",
      "240: 5374 // 5374\n",
      "241: 28725 // 28725\n",
      "242: 9245 // 9245\n",
      "243: 753 // 753\n",
      "244: 15501 // 15501\n",
      "245: 2856 // 2856\n",
      "246: 297 // 297\n",
      "247: 15240 // 15240\n",
      "248: 304 // 304\n",
      "249: 28725 // 28725\n",
      "250: 5174 // 5174\n",
      "251: 8417 // 8417\n",
      "252: 28725 // 28725\n",
      "253: 24767 // 24767\n",
      "254: 3878 // 3878\n",
      "255: 390 // 390\n",
      "256: 1259 // 1259\n",
      "257: 3345 // 3345\n",
      "258: 798 // 798\n",
      "259: 302 // 302\n",
      "260: 9562 // 9562\n",
      "261: 272 // 272\n",
      "262: 3508 // 3508\n",
      "263: 298 // 298\n",
      "264: 2358 // 2358\n",
      "265: 750 // 750\n",
      "266: 659 // 659\n",
      "267: 5181 // 5181\n",
      "268: 415 // 415\n",
      "269: 28723 // 28723\n",
      "270: 9750 // 9750\n",
      "271: 652 // 652\n",
      "272: 297 // 297\n",
      "273: 9562 // 9562\n",
      "274: 2372 // 2372\n",
      "275: 5366 // 5366\n",
      "276: 304 // 304\n",
      "277: 21363 // 21363\n",
      "278: 302 // 302\n",
      "279: 13290 // 13290\n",
      "280: 2264 // 2264\n",
      "281: 298 // 298\n",
      "282: 9893 // 9893\n",
      "283: 5976 // 5976\n",
      "284: 557 // 557\n",
      "285: 1645 // 1645\n",
      "286: 28758 // 28758\n",
      "287: 1851 // 1851\n",
      "288: 28743 // 28743\n",
      "289: 325 // 325\n",
      "290: 11304 // 11304\n",
      "291: 2426 // 2426\n",
      "292: 402 // 402\n",
      "293: 10822 // 10822\n",
      "294: 1197 // 1197\n",
      "295: 26216 // 26216\n",
      "296: 8522 // 8522\n",
      "297: 12045 // 12045\n",
      "298: 272 // 272\n",
      "299: 486 // 486\n",
      "300: 6202 // 6202\n",
      "301: 28725 // 28725\n",
      "302: 5181 // 5181\n",
      "303: 851 // 851\n",
      "304: 28747 // 28747\n",
      "305: 22072 // 22072\n",
      "306: 297 // 297\n",
      "307: 21018 // 21018\n",
      "308: 22512 // 22512\n",
      "309: 3912 // 3912\n",
      "310: 5183 // 5183\n",
      "311: 23047 // 23047\n",
      "312: 415 // 415\n",
      "313: 28723 // 28723\n",
      "314: 28750 // 28750\n",
      "315: 13 // 13\n",
      "316: 13 // 13\n",
      "317: 28723 // 28723\n",
      "318: 28231 // 28231\n",
      "319: 304 // 304\n",
      "320: 3208 // 3208\n",
      "321: 798 // 798\n",
      "322: 395 // 395\n",
      "323: 1407 // 1407\n",
      "324: 297 // 297\n",
      "325: 349 // 349\n",
      "326: 378 // 378\n",
      "327: 369 // 369\n",
      "328: 5407 // 5407\n",
      "329: 298 // 298\n",
      "330: 8861 // 8861\n",
      "331: 27040 // 27040\n",
      "332: 798 // 798\n",
      "333: 653 // 653\n",
      "334: 262 // 262\n",
      "335: 27352 // 27352\n",
      "336: 304 // 304\n",
      "337: 8774 // 8774\n",
      "338: 1082 // 1082\n",
      "339: 7498 // 7498\n",
      "340: 395 // 395\n",
      "341: 14200 // 14200\n",
      "342: 298 // 298\n",
      "343: 2358 // 2358\n",
      "344: 750 // 750\n",
      "345: 506 // 506\n",
      "346: 4938 // 4938\n",
      "347: 20642 // 20642\n",
      "348: 28725 // 28725\n",
      "349: 8758 // 8758\n",
      "350: 334 // 334\n",
      "351: 272 // 272\n",
      "352: 10091 // 10091\n",
      "353: 28723 // 28723\n",
      "354: 5823 // 5823\n",
      "355: 302 // 302\n",
      "356: 21590 // 21590\n",
      "357: 272 // 272\n",
      "358: 9009 // 9009\n",
      "359: 304 // 304\n",
      "360: 28713 // 28713\n",
      "361: 8326 // 8326\n",
      "362: 3058 // 3058\n",
      "363: 2380 // 2380\n",
      "364: 9893 // 9893\n",
      "365: 1316 // 1316\n",
      "366: 298 // 298\n",
      "367: 7069 // 7069\n",
      "368: 4154 // 4154\n",
      "369: 304 // 304\n",
      "370: 7040 // 7040\n",
      "371: 6202 // 6202\n",
      "372: 659 // 659\n",
      "373: 8758 // 8758\n",
      "374: 334 // 334\n",
      "375: 415 // 415\n",
      "376: 28723 // 28723\n",
      "377: 1759 // 1759\n",
      "378: 8326 // 8326\n",
      "379: 272 // 272\n",
      "380: 297 // 297\n",
      "381: 2437 // 2437\n",
      "382: 2708 // 2708\n",
      "383: 304 // 304\n",
      "384: 23767 // 23767\n",
      "385: 1203 // 1203\n",
      "386: 4916 // 4916\n",
      "387: 298 // 298\n",
      "388: 2739 // 2739\n",
      "389: 4938 // 4938\n",
      "390: 20642 // 20642\n",
      "391: 304 // 304\n",
      "392: 9909 // 9909\n",
      "393: 6190 // 6190\n",
      "394: 6259 // 6259\n",
      "395: 14657 // 14657\n",
      "396: 2488 // 2488\n",
      "397: 851 // 851\n",
      "398: 28747 // 28747\n",
      "399: 28455 // 28455\n",
      "400: 297 // 297\n",
      "401: 28731 // 28731\n",
      "402: 8758 // 8758\n",
      "403: 28743 // 28743\n",
      "404: 325 // 325\n",
      "405: 26938 // 26938\n",
      "406: 6829 // 6829\n",
      "407: 527 // 527\n",
      "408: 7095 // 7095\n",
      "409: 28742 // 28742\n",
      "410: 596 // 596\n",
      "411: 463 // 463\n",
      "412: 16334 // 16334\n",
      "413: 415 // 415\n",
      "414: 28723 // 28723\n",
      "415: 28740 // 28740\n",
      "416: 13 // 13\n",
      "417: 13 // 13\n",
      "418: 28747 // 28747\n",
      "419: 7710 // 7710\n",
      "420: 297 // 297\n",
      "421: 465 // 465\n",
      "422: 20367 // 20367\n",
      "423: 4355 // 4355\n",
      "424: 1835 // 1835\n",
      "425: 27319 // 27319\n",
      "426: 791 // 791\n",
      "427: 6190 // 6190\n",
      "428: 6259 // 6259\n",
      "429: 6053 // 6053\n",
      "430: 302 // 302\n",
      "431: 9254 // 9254\n",
      "432: 4870 // 4870\n",
      "433: 741 // 741\n",
      "434: 460 // 460\n",
      "435: 4003 // 4003\n",
      "436: 28808 // 28808\n",
      "437: 346 // 346\n",
      "438: 2029 // 2029\n",
      "439: 28743 // 28743\n",
      "440: 13 // 13\n",
      "441: 28767 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "442: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "443: 11143 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "444: 489 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "445: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "446: 28789 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "447: 13 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "448: 28705 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "449: 2 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "450: 28804 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "451: 456 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "452: 356 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "453: 680 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "454: 4098 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "455: 4665 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "456: 368 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "457: 2418 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "458: 28723 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "459: 7710 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "460: 297 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "461: 465 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "462: 20367 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "463: 4355 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "464: 1835 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "465: 27319 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "466: 791 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "467: 6190 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "468: 6259 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "469: 6053 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "470: 302 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "471: 9254 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "472: 680 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "473: 3084 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "474: 298 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "475: 368 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "476: 737 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "477: 682 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "478: 315 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "479: 28725 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "480: 2993 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "481: 28723 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "482: 10865 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "483: 1215 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "484: 750 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "485: 506 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "486: 20715 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "487: 3604 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "488: 28723 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "489: 465 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "490: 20367 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "491: 4355 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "492: 1835 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "493: 3992 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "494: 272 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "495: 297 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "496: 6190 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "497: 6259 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "498: 302 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "499: 3905 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "500: 272 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "501: 356 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "502: 2899 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "503: 574 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "504: 354 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "505: 368 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "506: 15896 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "507: 13 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "508: 28767 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "509: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "510: 1838 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "511: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "512: 28789 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "513: 13 // 13\n",
      "514: 28705 // 28705\n",
      "515: 2 // 2\n",
      "516: 28723 // 28723\n",
      "517: 522 // 522\n",
      "518: 2708 // 2708\n",
      "519: 304 // 304\n",
      "520: 28725 // 28725\n",
      "521: 4968 // 4968\n",
      "522: 28725 // 28725\n",
      "523: 9096 // 9096\n",
      "524: 680 // 680\n",
      "525: 9562 // 9562\n",
      "526: 2372 // 2372\n",
      "527: 798 // 798\n",
      "528: 2492 // 2492\n",
      "529: 298 // 298\n",
      "530: 14951 // 14951\n",
      "531: 541 // 541\n",
      "532: 6190 // 6190\n",
      "533: 6259 // 6259\n",
      "534: 28725 // 28725\n",
      "535: 9893 // 9893\n",
      "536: 3864 // 3864\n",
      "537: 24622 // 24622\n",
      "538: 304 // 304\n",
      "539: 28725 // 28725\n",
      "540: 3345 // 3345\n",
      "541: 798 // 798\n",
      "542: 9009 // 9009\n",
      "543: 298 // 298\n",
      "544: 1218 // 1218\n",
      "545: 17025 // 17025\n",
      "546: 3618 // 3618\n",
      "547: 3667 // 3667\n",
      "548: 28725 // 28725\n",
      "549: 454 // 454\n",
      "550: 13775 // 13775\n",
      "551: 304 // 304\n",
      "552: 16882 // 16882\n",
      "553: 28725 // 28725\n",
      "554: 617 // 617\n",
      "555: 2411 // 2411\n",
      "556: 1179 // 1179\n",
      "557: 354 // 354\n",
      "558: 1077 // 1077\n",
      "559: 11881 // 11881\n",
      "560: 2463 // 2463\n",
      "561: 28723 // 28723\n",
      "562: 3345 // 3345\n",
      "563: 798 // 798\n",
      "564: 356 // 356\n",
      "565: 465 // 465\n",
      "566: 20367 // 20367\n",
      "567: 4355 // 4355\n",
      "568: 302 // 302\n",
      "569: 6092 // 6092\n",
      "570: 7087 // 7087\n",
      "571: 272 // 272\n",
      "572: 17032 // 17032\n",
      "573: 297 // 297\n",
      "574: 3905 // 3905\n",
      "575: 13040 // 13040\n",
      "576: 264 // 264\n",
      "577: 1156 // 1156\n",
      "578: 541 // 541\n",
      "579: 6190 // 6190\n",
      "580: 6259 // 6259\n",
      "581: 28725 // 28725\n",
      "582: 12886 // 12886\n",
      "583: 657 // 657\n",
      "584: 13 // 13\n",
      "585: 13 // 13\n",
      "586: 28705 // 28705\n",
      "587: 28723 // 28723\n",
      "588: 3345 // 3345\n",
      "589: 798 // 798\n",
      "590: 302 // 302\n",
      "591: 9562 // 9562\n",
      "592: 272 // 272\n",
      "593: 16882 // 16882\n",
      "594: 297 // 297\n",
      "595: 9750 // 9750\n",
      "596: 1862 // 1862\n",
      "597: 304 // 304\n",
      "598: 9909 // 9909\n",
      "599: 6190 // 6190\n",
      "600: 6259 // 6259\n",
      "601: 4658 // 4658\n",
      "602: 690 // 690\n",
      "603: 28725 // 28725\n",
      "604: 3881 // 3881\n",
      "605: 9714 // 9714\n",
      "606: 276 // 276\n",
      "607: 391 // 391\n",
      "608: 28721 // 28721\n",
      "609: 500 // 500\n",
      "610: 272 // 272\n",
      "611: 486 // 486\n",
      "612: 8074 // 8074\n",
      "613: 7575 // 7575\n",
      "614: 302 // 302\n",
      "615: 1522 // 1522\n",
      "616: 7133 // 7133\n",
      "617: 415 // 415\n",
      "618: 28733 // 28733\n",
      "619: 13 // 13\n",
      "620: 13 // 13\n",
      "621: 28723 // 28723\n",
      "622: 3058 // 3058\n",
      "623: 272 // 272\n",
      "624: 486 // 486\n",
      "625: 25178 // 25178\n",
      "626: 7028 // 7028\n",
      "627: 3864 // 3864\n",
      "628: 262 // 262\n",
      "629: 27352 // 27352\n",
      "630: 304 // 304\n",
      "631: 9537 // 9537\n",
      "632: 466 // 466\n",
      "633: 482 // 482\n",
      "634: 13053 // 13053\n",
      "635: 798 // 798\n",
      "636: 9009 // 9009\n",
      "637: 298 // 298\n",
      "638: 2553 // 2553\n",
      "639: 2739 // 2739\n",
      "640: 4077 // 4077\n",
      "641: 272 // 272\n",
      "642: 304 // 304\n",
      "643: 9909 // 9909\n",
      "644: 6190 // 6190\n",
      "645: 6259 // 6259\n",
      "646: 5290 // 5290\n",
      "647: 690 // 690\n",
      "648: 28725 // 28725\n",
      "649: 21937 // 21937\n",
      "650: 272 // 272\n",
      "651: 297 // 297\n",
      "652: 5774 // 5774\n",
      "653: 466 // 466\n",
      "654: 482 // 482\n",
      "655: 28717 // 28717\n",
      "656: 1133 // 1133\n",
      "657: 4271 // 4271\n",
      "658: 302 // 302\n",
      "659: 288 // 288\n",
      "660: 27838 // 27838\n",
      "661: 28742 // 28742\n",
      "662: 596 // 596\n",
      "663: 463 // 463\n",
      "664: 16334 // 16334\n",
      "665: 415 // 415\n",
      "666: 28733 // 28733\n",
      "667: 13 // 13\n",
      "668: 13 // 13\n",
      "669: 28723 // 28723\n",
      "670: 2939 // 2939\n",
      "671: 272 // 272\n",
      "672: 297 // 297\n",
      "673: 10755 // 10755\n",
      "674: 798 // 798\n",
      "675: 302 // 302\n",
      "676: 466 // 466\n",
      "677: 20396 // 20396\n",
      "678: 272 // 272\n",
      "679: 297 // 297\n",
      "680: 2426 // 2426\n",
      "681: 3804 // 3804\n",
      "682: 279 // 279\n",
      "683: 3051 // 3051\n",
      "684: 304 // 304\n",
      "685: 28725 // 28725\n",
      "686: 2437 // 2437\n",
      "687: 2708 // 2708\n",
      "688: 28725 // 28725\n",
      "689: 23767 // 23767\n",
      "690: 1203 // 1203\n",
      "691: 354 // 354\n",
      "692: 288 // 288\n",
      "693: 5774 // 5774\n",
      "694: 4938 // 4938\n",
      "695: 5527 // 5527\n",
      "696: 28733 // 28733\n",
      "697: 7152 // 7152\n",
      "698: 304 // 304\n",
      "699: 9909 // 9909\n",
      "700: 6190 // 6190\n",
      "701: 6259 // 6259\n",
      "702: 5290 // 5290\n",
      "703: 690 // 690\n",
      "704: 28725 // 28725\n",
      "705: 28498 // 28498\n",
      "706: 4380 // 4380\n",
      "707: 297 // 297\n",
      "708: 12992 // 12992\n",
      "709: 3198 // 3198\n",
      "710: 5836 // 5836\n",
      "711: 4271 // 4271\n",
      "712: 354 // 354\n",
      "713: 27003 // 27003\n",
      "714: 617 // 617\n",
      "715: 263 // 263\n",
      "716: 15893 // 15893\n",
      "717: 23697 // 23697\n",
      "718: 415 // 415\n",
      "719: 28733 // 28733\n",
      "720: 13 // 13\n",
      "721: 13 // 13\n",
      "722: 28747 // 28747\n",
      "723: 2296 // 2296\n",
      "724: 272 // 272\n",
      "725: 3024 // 3024\n",
      "726: 465 // 465\n",
      "727: 20367 // 20367\n",
      "728: 4355 // 4355\n",
      "729: 1835 // 1835\n",
      "730: 27319 // 27319\n",
      "731: 791 // 791\n",
      "732: 6190 // 6190\n",
      "733: 6259 // 6259\n",
      "734: 6053 // 6053\n",
      "735: 302 // 302\n",
      "736: 9254 // 9254\n",
      "737: 11600 // 11600\n",
      "738: 13 // 13\n",
      "739: 13 // 13\n",
      "740: 28723 // 28723\n",
      "741: 9562 // 9562\n",
      "742: 2372 // 2372\n",
      "743: 798 // 798\n",
      "744: 297 // 297\n",
      "745: 465 // 465\n",
      "746: 20367 // 20367\n",
      "747: 4355 // 4355\n",
      "748: 17032 // 17032\n",
      "749: 438 // 438\n",
      "750: 17939 // 17939\n",
      "751: 28713 // 28713\n",
      "752: 9861 // 9861\n",
      "753: 354 // 354\n",
      "754: 1077 // 1077\n",
      "755: 11881 // 11881\n",
      "756: 297 // 297\n",
      "757: 12066 // 12066\n",
      "758: 4355 // 4355\n",
      "759: 272 // 272\n",
      "760: 2673 // 2673\n",
      "761: 15523 // 15523\n",
      "762: 15790 // 15790\n",
      "763: 14200 // 14200\n",
      "764: 835 // 835\n",
      "765: 541 // 541\n",
      "766: 1306 // 1306\n",
      "767: 28723 // 28723\n",
      "768: 5823 // 5823\n",
      "769: 798 // 798\n",
      "770: 302 // 302\n",
      "771: 21590 // 21590\n",
      "772: 272 // 272\n",
      "773: 297 // 297\n",
      "774: 23767 // 23767\n",
      "775: 1203 // 1203\n",
      "776: 304 // 304\n",
      "777: 2437 // 2437\n",
      "778: 2708 // 2708\n",
      "779: 5637 // 5637\n",
      "780: 298 // 298\n",
      "781: 9893 // 9893\n",
      "782: 653 // 653\n",
      "783: 24622 // 24622\n",
      "784: 541 // 541\n",
      "785: 9909 // 9909\n",
      "786: 6190 // 6190\n",
      "787: 12045 // 12045\n",
      "788: 28747 // 28747\n",
      "789: 596 // 596\n",
      "790: 463 // 463\n",
      "791: 16334 // 16334\n",
      "792: 302 // 302\n",
      "793: 1837 // 1837\n",
      "794: 20087 // 20087\n",
      "795: 351 // 351\n",
      "796: 28723 // 28723\n",
      "797: 28781 // 28781\n",
      "798: 13 // 13\n",
      "799: 13 // 13\n",
      "800: 28723 // 28723\n",
      "801: 15538 // 15538\n",
      "802: 1250 // 1250\n",
      "803: 1671 // 1671\n",
      "804: 10879 // 10879\n",
      "805: 465 // 465\n",
      "806: 20367 // 20367\n",
      "807: 297 // 297\n",
      "808: 14200 // 14200\n",
      "809: 298 // 298\n",
      "810: 497 // 497\n",
      "811: 28744 // 28744\n",
      "812: 430 // 430\n",
      "813: 652 // 652\n",
      "814: 304 // 304\n",
      "815: 18594 // 18594\n",
      "816: 354 // 354\n",
      "817: 3796 // 3796\n",
      "818: 378 // 378\n",
      "819: 2870 // 2870\n",
      "820: 851 // 851\n",
      "821: 28723 // 28723\n",
      "822: 7028 // 7028\n",
      "823: 14461 // 14461\n",
      "824: 302 // 302\n",
      "825: 454 // 454\n",
      "826: 13775 // 13775\n",
      "827: 304 // 304\n",
      "828: 5823 // 5823\n",
      "829: 798 // 798\n",
      "830: 302 // 302\n",
      "831: 6023 // 6023\n",
      "832: 272 // 272\n",
      "833: 15271 // 15271\n",
      "834: 2490 // 2490\n",
      "835: 28725 // 28725\n",
      "836: 9562 // 9562\n",
      "837: 2372 // 2372\n",
      "838: 798 // 798\n",
      "839: 16882 // 16882\n",
      "840: 354 // 354\n",
      "841: 23400 // 23400\n",
      "842: 4808 // 4808\n",
      "843: 304 // 304\n",
      "844: 1950 // 1950\n",
      "845: 9750 // 9750\n",
      "846: 1316 // 1316\n",
      "847: 541 // 541\n",
      "848: 9909 // 9909\n",
      "849: 6190 // 6190\n",
      "850: 12045 // 12045\n",
      "851: 28747 // 28747\n",
      "852: 8074 // 8074\n",
      "853: 4271 // 4271\n",
      "854: 27838 // 27838\n",
      "855: 298 // 298\n",
      "856: 9750 // 9750\n",
      "857: 302 // 302\n",
      "858: 1218 // 1218\n",
      "859: 323 // 323\n",
      "860: 6275 // 6275\n",
      "861: 14578 // 14578\n",
      "862: 28723 // 28723\n",
      "863: 28770 // 28770\n",
      "864: 13 // 13\n",
      "865: 13 // 13\n",
      "866: 28723 // 28723\n",
      "867: 14727 // 14727\n",
      "868: 28733 // 28733\n",
      "869: 5161 // 5161\n",
      "870: 297 // 297\n",
      "871: 23767 // 23767\n",
      "872: 1203 // 1203\n",
      "873: 304 // 304\n",
      "874: 2437 // 2437\n",
      "875: 2708 // 2708\n",
      "876: 298 // 298\n",
      "877: 8681 // 8681\n",
      "878: 851 // 851\n",
      "879: 28723 // 28723\n",
      "880: 12503 // 12503\n",
      "881: 272 // 272\n",
      "882: 298 // 298\n",
      "883: 10879 // 10879\n",
      "884: 1259 // 1259\n",
      "885: 302 // 302\n",
      "886: 5566 // 5566\n",
      "887: 3084 // 3084\n",
      "888: 304 // 304\n",
      "889: 28725 // 28725\n",
      "890: 5200 // 5200\n",
      "891: 2844 // 2844\n",
      "892: 1250 // 1250\n",
      "893: 349 // 349\n",
      "894: 465 // 465\n",
      "895: 20367 // 20367\n",
      "896: 970 // 970\n",
      "897: 5020 // 5020\n",
      "898: 9051 // 9051\n",
      "899: 28725 // 28725\n",
      "900: 3345 // 3345\n",
      "901: 798 // 798\n",
      "902: 302 // 302\n",
      "903: 15197 // 15197\n",
      "904: 304 // 304\n",
      "905: 16882 // 16882\n",
      "906: 7126 // 7126\n",
      "907: 621 // 621\n",
      "908: 16353 // 16353\n",
      "909: 541 // 541\n",
      "910: 9909 // 9909\n",
      "911: 6190 // 6190\n",
      "912: 12045 // 12045\n",
      "913: 28747 // 28747\n",
      "914: 454 // 454\n",
      "915: 740 // 740\n",
      "916: 451 // 451\n",
      "917: 304 // 304\n",
      "918: 288 // 288\n",
      "919: 27838 // 27838\n",
      "920: 28723 // 28723\n",
      "921: 28750 // 28750\n",
      "922: 13 // 13\n",
      "923: 13 // 13\n",
      "924: 28723 // 28723\n",
      "925: 10879 // 10879\n",
      "926: 24106 // 24106\n",
      "927: 288 // 288\n",
      "928: 10594 // 10594\n",
      "929: 304 // 304\n",
      "930: 9562 // 9562\n",
      "931: 2372 // 2372\n",
      "932: 798 // 798\n",
      "933: 16882 // 16882\n",
      "934: 297 // 297\n",
      "935: 3905 // 3905\n",
      "936: 652 // 652\n",
      "937: 356 // 356\n",
      "938: 9893 // 9893\n",
      "939: 380 // 380\n",
      "940: 3600 // 3600\n",
      "941: 835 // 835\n",
      "942: 541 // 541\n",
      "943: 4938 // 4938\n",
      "944: 6190 // 6190\n",
      "945: 12045 // 12045\n",
      "946: 28723 // 28723\n",
      "947: 466 // 466\n",
      "948: 482 // 482\n",
      "949: 13053 // 13053\n",
      "950: 304 // 304\n",
      "951: 1759 // 1759\n",
      "952: 21349 // 21349\n",
      "953: 272 // 272\n",
      "954: 297 // 297\n",
      "955: 26994 // 26994\n",
      "956: 4355 // 4355\n",
      "957: 17032 // 17032\n",
      "958: 438 // 438\n",
      "959: 17939 // 17939\n",
      "960: 28713 // 28713\n",
      "961: 9861 // 9861\n",
      "962: 575 // 575\n",
      "963: 7096 // 7096\n",
      "964: 298 // 298\n",
      "965: 12503 // 12503\n",
      "966: 272 // 272\n",
      "967: 356 // 356\n",
      "968: 6403 // 6403\n",
      "969: 798 // 798\n",
      "970: 8270 // 8270\n",
      "971: 541 // 541\n",
      "972: 1306 // 1306\n",
      "973: 28723 // 28723\n",
      "974: 3345 // 3345\n",
      "975: 798 // 798\n",
      "976: 356 // 356\n",
      "977: 465 // 465\n",
      "978: 20367 // 20367\n",
      "979: 4355 // 4355\n",
      "980: 302 // 302\n",
      "981: 6092 // 6092\n",
      "982: 7087 // 7087\n",
      "983: 272 // 272\n",
      "984: 684 // 684\n",
      "985: 13619 // 13619\n",
      "986: 15851 // 15851\n",
      "987: 297 // 297\n",
      "988: 3905 // 3905\n",
      "989: 12588 // 12588\n",
      "990: 264 // 264\n",
      "991: 1156 // 1156\n",
      "992: 541 // 541\n",
      "993: 9909 // 9909\n",
      "994: 6190 // 6190\n",
      "995: 12045 // 12045\n",
      "996: 28747 // 28747\n",
      "997: 27941 // 27941\n",
      "998: 28733 // 28733\n",
      "999: 1467 // 1467\n",
      "1000: 1706 // 1706\n",
      "1001: 330 // 330\n",
      "1002: 304 // 304\n",
      "1003: 2426 // 2426\n",
      "1004: 402 // 402\n",
      "1005: 10822 // 10822\n",
      "1006: 28723 // 28723\n",
      "1007: 28740 // 28740\n",
      "1008: 13 // 13\n",
      "1009: 13 // 13\n",
      "1010: 28747 // 28747\n",
      "1011: 3345 // 3345\n",
      "1012: 798 // 798\n",
      "1013: 356 // 356\n",
      "1014: 465 // 465\n",
      "1015: 20367 // 20367\n",
      "1016: 4355 // 4355\n",
      "1017: 302 // 302\n",
      "1018: 6092 // 6092\n",
      "1019: 7087 // 7087\n",
      "1020: 272 // 272\n",
      "1021: 26518 // 26518\n",
      "1022: 1316 // 1316\n",
      "1023: 541 // 541\n",
      "1024: 6190 // 6190\n",
      "1025: 6259 // 6259\n",
      "1026: 910 // 910\n",
      "1027: 302 // 302\n",
      "1028: 9254 // 9254\n",
      "1029: 741 // 741\n",
      "1030: 460 // 460\n",
      "1031: 4003 // 4003\n",
      "1032: 28723 // 28723\n",
      "1033: 617 // 617\n",
      "1034: 2411 // 2411\n",
      "1035: 1179 // 1179\n",
      "1036: 21325 // 21325\n",
      "1037: 304 // 304\n",
      "1038: 465 // 465\n",
      "1039: 20367 // 20367\n",
      "1040: 4355 // 4355\n",
      "1041: 1835 // 1835\n",
      "1042: 8552 // 8552\n",
      "1043: 297 // 297\n",
      "1044: 3905 // 3905\n",
      "1045: 13040 // 13040\n",
      "1046: 264 // 264\n",
      "1047: 8928 // 8928\n",
      "1048: 6190 // 6190\n",
      "1049: 12045 // 12045\n",
      "1050: 28808 // 28808\n",
      "1051: 346 // 346\n",
      "1052: 2029 // 2029\n",
      "1053: 28743 // 28743\n",
      "1054: 13 // 13\n",
      "1055: 28767 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1056: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1057: 11143 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1058: 489 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1059: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1060: 28789 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1061: 13 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1062: 28705 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1063: 2 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1064: 28804 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1065: 3345 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1066: 798 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1067: 356 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1068: 465 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1069: 20367 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1070: 4355 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1071: 302 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1072: 6092 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1073: 7087 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1074: 272 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1075: 26518 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1076: 1316 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1077: 541 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1078: 6190 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1079: 6259 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1080: 910 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1081: 302 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1082: 9254 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1083: 304 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1084: 1871 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1085: 680 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1086: 3084 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1087: 4665 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1088: 368 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1089: 2418 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1090: 28723 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1091: 465 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1092: 20367 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1093: 4355 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1094: 1835 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1095: 8552 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1096: 297 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1097: 6190 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1098: 6259 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1099: 302 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1100: 3905 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1101: 272 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1102: 684 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1103: 680 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1104: 873 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1105: 298 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1106: 737 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1107: 682 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1108: 315 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1109: 28723 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1110: 9562 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1111: 2372 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1112: 798 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1113: 356 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1114: 2030 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1115: 871 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1116: 304 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1117: 465 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1118: 20367 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1119: 4355 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1120: 302 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1121: 5643 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1122: 15313 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1123: 264 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1124: 7501 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1125: 354 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1126: 368 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1127: 15896 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1128: 13 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1129: 28767 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1130: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1131: 1838 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1132: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1133: 28789 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1134: 13 // 13\n",
      "1135: 28705 // 28705\n",
      "1136: 2 // 2\n",
      "1137: 28723 // 28723\n",
      "1138: 3345 // 3345\n",
      "1139: 798 // 798\n",
      "1140: 1873 // 1873\n",
      "1141: 5663 // 5663\n",
      "1142: 298 // 298\n",
      "1143: 1316 // 1316\n",
      "1144: 304 // 304\n",
      "1145: 28725 // 28725\n",
      "1146: 12638 // 12638\n",
      "1147: 22303 // 22303\n",
      "1148: 297 // 297\n",
      "1149: 4893 // 4893\n",
      "1150: 798 // 798\n",
      "1151: 11711 // 11711\n",
      "1152: 28725 // 28725\n",
      "1153: 465 // 465\n",
      "1154: 20367 // 20367\n",
      "1155: 4355 // 4355\n",
      "1156: 302 // 302\n",
      "1157: 6092 // 6092\n",
      "1158: 4177 // 4177\n",
      "1159: 616 // 616\n",
      "1160: 272 // 272\n",
      "1161: 11590 // 11590\n",
      "1162: 2367 // 2367\n",
      "1163: 541 // 541\n",
      "1164: 4435 // 4435\n",
      "1165: 28211 // 28211\n",
      "1166: 304 // 304\n",
      "1167: 28725 // 28725\n",
      "1168: 9861 // 9861\n",
      "1169: 2372 // 2372\n",
      "1170: 6259 // 6259\n",
      "1171: 28725 // 28725\n",
      "1172: 23400 // 23400\n",
      "1173: 12992 // 12992\n",
      "1174: 4754 // 4754\n",
      "1175: 28733 // 28733\n",
      "1176: 6891 // 6891\n",
      "1177: 16458 // 16458\n",
      "1178: 302 // 302\n",
      "1179: 24056 // 24056\n",
      "1180: 415 // 415\n",
      "1181: 28723 // 28723\n",
      "1182: 617 // 617\n",
      "1183: 2411 // 2411\n",
      "1184: 22303 // 22303\n",
      "1185: 304 // 304\n",
      "1186: 28725 // 28725\n",
      "1187: 12832 // 12832\n",
      "1188: 28725 // 28725\n",
      "1189: 2437 // 2437\n",
      "1190: 2708 // 2708\n",
      "1191: 298 // 298\n",
      "1192: 5202 // 5202\n",
      "1193: 4382 // 4382\n",
      "1194: 298 // 298\n",
      "1195: 6516 // 6516\n",
      "1196: 459 // 459\n",
      "1197: 562 // 562\n",
      "1198: 2490 // 2490\n",
      "1199: 28725 // 28725\n",
      "1200: 3345 // 3345\n",
      "1201: 798 // 798\n",
      "1202: 302 // 302\n",
      "1203: 23798 // 23798\n",
      "1204: 304 // 304\n",
      "1205: 4045 // 4045\n",
      "1206: 272 // 272\n",
      "1207: 354 // 354\n",
      "1208: 13416 // 13416\n",
      "1209: 5864 // 5864\n",
      "1210: 659 // 659\n",
      "1211: 465 // 465\n",
      "1212: 20367 // 20367\n",
      "1213: 4355 // 4355\n",
      "1214: 28725 // 28725\n",
      "1215: 12886 // 12886\n",
      "1216: 657 // 657\n",
      "1217: 13 // 13\n",
      "1218: 13 // 13\n",
      "1219: 28723 // 28723\n",
      "1220: 28455 // 28455\n",
      "1221: 297 // 297\n",
      "1222: 9562 // 9562\n",
      "1223: 2372 // 2372\n",
      "1224: 798 // 798\n",
      "1225: 5645 // 5645\n",
      "1226: 298 // 298\n",
      "1227: 19644 // 19644\n",
      "1228: 5864 // 5864\n",
      "1229: 264 // 264\n",
      "1230: 403 // 403\n",
      "1231: 465 // 465\n",
      "1232: 20367 // 20367\n",
      "1233: 4355 // 4355\n",
      "1234: 369 // 369\n",
      "1235: 1419 // 1419\n",
      "1236: 3881 // 3881\n",
      "1237: 7838 // 7838\n",
      "1238: 3304 // 3304\n",
      "1239: 264 // 264\n",
      "1240: 28725 // 28725\n",
      "1241: 19190 // 19190\n",
      "1242: 28723 // 28723\n",
      "1243: 6966 // 6966\n",
      "1244: 466 // 466\n",
      "1245: 482 // 482\n",
      "1246: 13053 // 13053\n",
      "1247: 4337 // 4337\n",
      "1248: 304 // 304\n",
      "1249: 14461 // 14461\n",
      "1250: 14817 // 14817\n",
      "1251: 28733 // 28733\n",
      "1252: 2859 // 2859\n",
      "1253: 298 // 298\n",
      "1254: 4163 // 4163\n",
      "1255: 465 // 465\n",
      "1256: 20367 // 20367\n",
      "1257: 304 // 304\n",
      "1258: 26994 // 26994\n",
      "1259: 4355 // 4355\n",
      "1260: 369 // 369\n",
      "1261: 1419 // 1419\n",
      "1262: 5558 // 5558\n",
      "1263: 297 // 297\n",
      "1264: 466 // 466\n",
      "1265: 482 // 482\n",
      "1266: 13053 // 13053\n",
      "1267: 798 // 798\n",
      "1268: 302 // 302\n",
      "1269: 3881 // 3881\n",
      "1270: 264 // 264\n",
      "1271: 28725 // 28725\n",
      "1272: 3335 // 3335\n",
      "1273: 1263 // 1263\n",
      "1274: 28723 // 28723\n",
      "1275: 9562 // 9562\n",
      "1276: 2372 // 2372\n",
      "1277: 798 // 798\n",
      "1278: 356 // 356\n",
      "1279: 465 // 465\n",
      "1280: 20367 // 20367\n",
      "1281: 4355 // 4355\n",
      "1282: 302 // 302\n",
      "1283: 6092 // 6092\n",
      "1284: 7087 // 7087\n",
      "1285: 272 // 272\n",
      "1286: 16499 // 16499\n",
      "1287: 11354 // 11354\n",
      "1288: 3332 // 3332\n",
      "1289: 304 // 304\n",
      "1290: 5566 // 5566\n",
      "1291: 25537 // 25537\n",
      "1292: 25225 // 25225\n",
      "1293: 13 // 13\n",
      "1294: 13 // 13\n",
      "1295: 28723 // 28723\n",
      "1296: 9562 // 9562\n",
      "1297: 2372 // 2372\n",
      "1298: 798 // 798\n",
      "1299: 356 // 356\n",
      "1300: 465 // 465\n",
      "1301: 20367 // 20367\n",
      "1302: 4355 // 4355\n",
      "1303: 302 // 302\n",
      "1304: 8015 // 8015\n",
      "1305: 272 // 272\n",
      "1306: 7643 // 7643\n",
      "1307: 298 // 298\n",
      "1308: 1316 // 1316\n",
      "1309: 835 // 835\n",
      "1310: 541 // 541\n",
      "1311: 28725 // 28725\n",
      "1312: 2233 // 2233\n",
      "1313: 4549 // 4549\n",
      "1314: 304 // 304\n",
      "1315: 13804 // 13804\n",
      "1316: 12774 // 12774\n",
      "1317: 6818 // 6818\n",
      "1318: 390 // 390\n",
      "1319: 1259 // 1259\n",
      "1320: 28725 // 28725\n",
      "1321: 4435 // 4435\n",
      "1322: 28211 // 28211\n",
      "1323: 28725 // 28725\n",
      "1324: 8126 // 8126\n",
      "1325: 28723 // 28723\n",
      "1326: 697 // 697\n",
      "1327: 1917 // 1917\n",
      "1328: 4355 // 4355\n",
      "1329: 486 // 486\n",
      "1330: 19927 // 19927\n",
      "1331: 459 // 459\n",
      "1332: 349 // 349\n",
      "1333: 9562 // 9562\n",
      "1334: 2372 // 2372\n",
      "1335: 798 // 798\n",
      "1336: 369 // 369\n",
      "1337: 5407 // 5407\n",
      "1338: 829 // 829\n",
      "1339: 28725 // 28725\n",
      "1340: 697 // 697\n",
      "1341: 6548 // 6548\n",
      "1342: 5527 // 5527\n",
      "1343: 28733 // 28733\n",
      "1344: 4397 // 4397\n",
      "1345: 304 // 304\n",
      "1346: 28725 // 28725\n",
      "1347: 482 // 482\n",
      "1348: 3095 // 3095\n",
      "1349: 28725 // 28725\n",
      "1350: 21349 // 21349\n",
      "1351: 5527 // 5527\n",
      "1352: 28733 // 28733\n",
      "1353: 279 // 279\n",
      "1354: 3051 // 3051\n",
      "1355: 390 // 390\n",
      "1356: 1259 // 1259\n",
      "1357: 28725 // 28725\n",
      "1358: 9861 // 9861\n",
      "1359: 2372 // 2372\n",
      "1360: 6259 // 6259\n",
      "1361: 1800 // 1800\n",
      "1362: 21831 // 21831\n",
      "1363: 28723 // 28723\n",
      "1364: 14679 // 14679\n",
      "1365: 21363 // 21363\n",
      "1366: 11590 // 11590\n",
      "1367: 2367 // 2367\n",
      "1368: 298 // 298\n",
      "1369: 1316 // 1316\n",
      "1370: 541 // 541\n",
      "1371: 28725 // 28725\n",
      "1372: 1759 // 1759\n",
      "1373: 466 // 466\n",
      "1374: 482 // 482\n",
      "1375: 13053 // 13053\n",
      "1376: 272 // 272\n",
      "1377: 297 // 297\n",
      "1378: 23767 // 23767\n",
      "1379: 1203 // 1203\n",
      "1380: 7161 // 7161\n",
      "1381: 8050 // 8050\n",
      "1382: 304 // 304\n",
      "1383: 28725 // 28725\n",
      "1384: 1308 // 1308\n",
      "1385: 2096 // 2096\n",
      "1386: 1072 // 1072\n",
      "1387: 1231 // 1231\n",
      "1388: 291 // 291\n",
      "1389: 20978 // 20978\n",
      "1390: 288 // 288\n",
      "1391: 15974 // 15974\n",
      "1392: 28725 // 28725\n",
      "1393: 4007 // 4007\n",
      "1394: 24790 // 24790\n",
      "1395: 7126 // 7126\n",
      "1396: 396 // 396\n",
      "1397: 6818 // 6818\n",
      "1398: 390 // 390\n",
      "1399: 1259 // 1259\n",
      "1400: 28725 // 28725\n",
      "1401: 23400 // 23400\n",
      "1402: 12992 // 12992\n",
      "1403: 4754 // 4754\n",
      "1404: 28733 // 28733\n",
      "1405: 6891 // 6891\n",
      "1406: 16458 // 16458\n",
      "1407: 302 // 302\n",
      "1408: 9786 // 9786\n",
      "1409: 415 // 415\n",
      "1410: 28723 // 28723\n",
      "1411: 465 // 465\n",
      "1412: 20367 // 20367\n",
      "1413: 4355 // 4355\n",
      "1414: 302 // 302\n",
      "1415: 6092 // 6092\n",
      "1416: 7087 // 7087\n",
      "1417: 272 // 272\n",
      "1418: 26518 // 26518\n",
      "1419: 298 // 298\n",
      "1420: 1316 // 1316\n",
      "1421: 541 // 541\n",
      "1422: 27319 // 27319\n",
      "1423: 791 // 791\n",
      "1424: 4920 // 4920\n",
      "1425: 282 // 282\n",
      "1426: 1548 // 1548\n",
      "1427: 28735 // 28735\n",
      "1428: 13 // 13\n",
      "1429: 13 // 13\n",
      "1430: 28723 // 28723\n",
      "1431: 1179 // 1179\n",
      "1432: 6517 // 6517\n",
      "1433: 272 // 272\n",
      "1434: 354 // 354\n",
      "1435: 5663 // 5663\n",
      "1436: 298 // 298\n",
      "1437: 12638 // 12638\n",
      "1438: 22303 // 22303\n",
      "1439: 302 // 302\n",
      "1440: 5537 // 5537\n",
      "1441: 272 // 272\n",
      "1442: 297 // 297\n",
      "1443: 7152 // 7152\n",
      "1444: 302 // 302\n",
      "1445: 5502 // 5502\n",
      "1446: 264 // 264\n",
      "1447: 304 // 304\n",
      "1448: 466 // 466\n",
      "1449: 3623 // 3623\n",
      "1450: 425 // 425\n",
      "1451: 704 // 704\n",
      "1452: 23723 // 23723\n",
      "1453: 298 // 298\n",
      "1454: 1736 // 1736\n",
      "1455: 541 // 541\n",
      "1456: 851 // 851\n",
      "1457: 28723 // 28723\n",
      "1458: 12638 // 12638\n",
      "1459: 22303 // 22303\n",
      "1460: 302 // 302\n",
      "1461: 2426 // 2426\n",
      "1462: 321 // 321\n",
      "1463: 12700 // 12700\n",
      "1464: 272 // 272\n",
      "1465: 1303 // 1303\n",
      "1466: 858 // 858\n",
      "1467: 640 // 640\n",
      "1468: 369 // 369\n",
      "1469: 26473 // 26473\n",
      "1470: 302 // 302\n",
      "1471: 5679 // 5679\n",
      "1472: 264 // 264\n",
      "1473: 304 // 304\n",
      "1474: 21363 // 21363\n",
      "1475: 302 // 302\n",
      "1476: 10061 // 10061\n",
      "1477: 264 // 264\n",
      "1478: 2231 // 2231\n",
      "1479: 590 // 590\n",
      "1480: 28725 // 28725\n",
      "1481: 18909 // 18909\n",
      "1482: 652 // 652\n",
      "1483: 11499 // 11499\n",
      "1484: 298 // 298\n",
      "1485: 5823 // 5823\n",
      "1486: 798 // 798\n",
      "1487: 938 // 938\n",
      "1488: 18594 // 18594\n",
      "1489: 1684 // 1684\n",
      "1490: 28723 // 28723\n",
      "1491: 617 // 617\n",
      "1492: 2411 // 2411\n",
      "1493: 22303 // 22303\n",
      "1494: 354 // 354\n",
      "1495: 13416 // 13416\n",
      "1496: 4177 // 4177\n",
      "1497: 616 // 616\n",
      "1498: 506 // 506\n",
      "1499: 541 // 541\n",
      "1500: 465 // 465\n",
      "1501: 20367 // 20367\n",
      "1502: 4355 // 4355\n",
      "1503: 28725 // 28725\n",
      "1504: 4452 // 4452\n",
      "1505: 620 // 620\n",
      "1506: 324 // 324\n",
      "1507: 28765 // 28765\n",
      "1508: 13 // 13\n",
      "1509: 13 // 13\n",
      "1510: 28723 // 28723\n",
      "1511: 4382 // 4382\n",
      "1512: 7276 // 7276\n",
      "1513: 356 // 356\n",
      "1514: 5097 // 5097\n",
      "1515: 302 // 302\n",
      "1516: 5502 // 5502\n",
      "1517: 264 // 264\n",
      "1518: 304 // 304\n",
      "1519: 9981 // 9981\n",
      "1520: 1007 // 1007\n",
      "1521: 9396 // 9396\n",
      "1522: 298 // 298\n",
      "1523: 1736 // 1736\n",
      "1524: 541 // 541\n",
      "1525: 690 // 690\n",
      "1526: 28725 // 28725\n",
      "1527: 2475 // 2475\n",
      "1528: 438 // 438\n",
      "1529: 798 // 798\n",
      "1530: 272 // 272\n",
      "1531: 354 // 354\n",
      "1532: 7196 // 7196\n",
      "1533: 5864 // 5864\n",
      "1534: 680 // 680\n",
      "1535: 506 // 506\n",
      "1536: 993 // 993\n",
      "1537: 369 // 369\n",
      "1538: 1395 // 1395\n",
      "1539: 754 // 754\n",
      "1540: 18909 // 18909\n",
      "1541: 652 // 652\n",
      "1542: 555 // 555\n",
      "1543: 2104 // 2104\n",
      "1544: 369 // 369\n",
      "1545: 10086 // 10086\n",
      "1546: 304 // 304\n",
      "1547: 7028 // 7028\n",
      "1548: 23175 // 23175\n",
      "1549: 4681 // 4681\n",
      "1550: 2608 // 2608\n",
      "1551: 8019 // 8019\n",
      "1552: 8953 // 8953\n",
      "1553: 28723 // 28723\n",
      "1554: 798 // 798\n",
      "1555: 2952 // 2952\n",
      "1556: 272 // 272\n",
      "1557: 7949 // 7949\n",
      "1558: 829 // 829\n",
      "1559: 369 // 369\n",
      "1560: 10086 // 10086\n",
      "1561: 302 // 302\n",
      "1562: 9786 // 9786\n",
      "1563: 272 // 272\n",
      "1564: 288 // 288\n",
      "1565: 27988 // 27988\n",
      "1566: 486 // 486\n",
      "1567: 12832 // 12832\n",
      "1568: 7643 // 7643\n",
      "1569: 835 // 835\n",
      "1570: 541 // 541\n",
      "1571: 465 // 465\n",
      "1572: 20367 // 20367\n",
      "1573: 4355 // 4355\n",
      "1574: 28725 // 28725\n",
      "1575: 14586 // 14586\n",
      "1576: 1991 // 1991\n",
      "1577: 13 // 13\n",
      "1578: 13 // 13\n",
      "1579: 28723 // 28723\n",
      "1580: 3058 // 3058\n",
      "1581: 272 // 272\n",
      "1582: 297 // 297\n",
      "1583: 4893 // 4893\n",
      "1584: 798 // 798\n",
      "1585: 2804 // 2804\n",
      "1586: 1234 // 1234\n",
      "1587: 690 // 690\n",
      "1588: 28725 // 28725\n",
      "1589: 10755 // 10755\n",
      "1590: 798 // 798\n",
      "1591: 304 // 304\n",
      "1592: 18594 // 18594\n",
      "1593: 3352 // 3352\n",
      "1594: 22512 // 22512\n",
      "1595: 2824 // 2824\n",
      "1596: 302 // 302\n",
      "1597: 5679 // 5679\n",
      "1598: 264 // 264\n",
      "1599: 12652 // 12652\n",
      "1600: 304 // 304\n",
      "1601: 2426 // 2426\n",
      "1602: 3804 // 3804\n",
      "1603: 279 // 279\n",
      "1604: 3051 // 3051\n",
      "1605: 1303 // 1303\n",
      "1606: 858 // 858\n",
      "1607: 640 // 640\n",
      "1608: 851 // 851\n",
      "1609: 28723 // 28723\n",
      "1610: 16760 // 16760\n",
      "1611: 7391 // 7391\n",
      "1612: 1944 // 1944\n",
      "1613: 2419 // 2419\n",
      "1614: 562 // 562\n",
      "1615: 308 // 308\n",
      "1616: 3440 // 3440\n",
      "1617: 2108 // 2108\n",
      "1618: 302 // 302\n",
      "1619: 4268 // 4268\n",
      "1620: 297 // 297\n",
      "1621: 286 // 286\n",
      "1622: 24890 // 24890\n",
      "1623: 347 // 347\n",
      "1624: 993 // 993\n",
      "1625: 4799 // 4799\n",
      "1626: 442 // 442\n",
      "1627: 12179 // 12179\n",
      "1628: 1799 // 1799\n",
      "1629: 14033 // 14033\n",
      "1630: 28733 // 28733\n",
      "1631: 1489 // 1489\n",
      "1632: 272 // 272\n",
      "1633: 28725 // 28725\n",
      "1634: 19178 // 19178\n",
      "1635: 288 // 288\n",
      "1636: 7619 // 7619\n",
      "1637: 442 // 442\n",
      "1638: 21349 // 21349\n",
      "1639: 354 // 354\n",
      "1640: 6451 // 6451\n",
      "1641: 272 // 272\n",
      "1642: 460 // 460\n",
      "1643: 22295 // 22295\n",
      "1644: 304 // 304\n",
      "1645: 697 // 697\n",
      "1646: 1917 // 1917\n",
      "1647: 4355 // 4355\n",
      "1648: 1684 // 1684\n",
      "1649: 28723 // 28723\n",
      "1650: 2437 // 2437\n",
      "1651: 2708 // 2708\n",
      "1652: 4038 // 4038\n",
      "1653: 858 // 858\n",
      "1654: 640 // 640\n",
      "1655: 486 // 486\n",
      "1656: 349 // 349\n",
      "1657: 3345 // 3345\n",
      "1658: 798 // 798\n",
      "1659: 20777 // 20777\n",
      "1660: 465 // 465\n",
      "1661: 20367 // 20367\n",
      "1662: 4355 // 4355\n",
      "1663: 690 // 690\n",
      "1664: 297 // 297\n",
      "1665: 4342 // 4342\n",
      "1666: 2191 // 2191\n",
      "1667: 272 // 272\n",
      "1668: 302 // 302\n",
      "1669: 6167 // 6167\n",
      "1670: 13 // 13\n",
      "1671: 13 // 13\n",
      "1672: 28723 // 28723\n",
      "1673: 18909 // 18909\n",
      "1674: 652 // 652\n",
      "1675: 11499 // 11499\n",
      "1676: 298 // 298\n",
      "1677: 5823 // 5823\n",
      "1678: 798 // 798\n",
      "1679: 1413 // 1413\n",
      "1680: 11220 // 11220\n",
      "1681: 18594 // 18594\n",
      "1682: 395 // 395\n",
      "1683: 28725 // 28725\n",
      "1684: 5780 // 5780\n",
      "1685: 6202 // 6202\n",
      "1686: 304 // 304\n",
      "1687: 10423 // 10423\n",
      "1688: 1287 // 1287\n",
      "1689: 297 // 297\n",
      "1690: 308 // 308\n",
      "1691: 25371 // 25371\n",
      "1692: 347 // 347\n",
      "1693: 298 // 298\n",
      "1694: 10352 // 10352\n",
      "1695: 465 // 465\n",
      "1696: 20367 // 20367\n",
      "1697: 4355 // 4355\n",
      "1698: 28725 // 28725\n",
      "1699: 2421 // 2421\n",
      "1700: 13621 // 13621\n",
      "1701: 560 // 560\n",
      "1702: 28723 // 28723\n",
      "1703: 18909 // 18909\n",
      "1704: 652 // 652\n",
      "1705: 11499 // 11499\n",
      "1706: 298 // 298\n",
      "1707: 5823 // 5823\n",
      "1708: 798 // 798\n",
      "1709: 1307 // 1307\n",
      "1710: 304 // 304\n",
      "1711: 9800 // 9800\n",
      "1712: 1862 // 1862\n",
      "1713: 12888 // 12888\n",
      "1714: 369 // 369\n",
      "1715: 12155 // 12155\n",
      "1716: 4355 // 4355\n",
      "1717: 1813 // 1813\n",
      "1718: 298 // 298\n",
      "1719: 465 // 465\n",
      "1720: 20367 // 20367\n",
      "1721: 1307 // 1307\n",
      "1722: 9245 // 9245\n",
      "1723: 3014 // 3014\n",
      "1724: 297 // 297\n",
      "1725: 274 // 274\n",
      "1726: 10949 // 10949\n",
      "1727: 4355 // 4355\n",
      "1728: 28725 // 28725\n",
      "1729: 5445 // 5445\n",
      "1730: 362 // 362\n",
      "1731: 28774 // 28774\n",
      "1732: 28740 // 28740\n",
      "1733: 28705 // 28705\n",
      "1734: 272 // 272\n",
      "1735: 297 // 297\n",
      "1736: 28725 // 28725\n",
      "1737: 3335 // 3335\n",
      "1738: 1263 // 1263\n",
      "1739: 28723 // 28723\n",
      "1740: 3543 // 3543\n",
      "1741: 2969 // 2969\n",
      "1742: 272 // 272\n",
      "1743: 2490 // 2490\n",
      "1744: 28725 // 28725\n",
      "1745: 5780 // 5780\n",
      "1746: 1287 // 1287\n",
      "1747: 297 // 297\n",
      "1748: 495 // 495\n",
      "1749: 6170 // 6170\n",
      "1750: 660 // 660\n",
      "1751: 750 // 750\n",
      "1752: 659 // 659\n",
      "1753: 465 // 465\n",
      "1754: 20367 // 20367\n",
      "1755: 4355 // 4355\n",
      "1756: 28725 // 28725\n",
      "1757: 1944 // 1944\n",
      "1758: 271 // 271\n",
      "1759: 8690 // 8690\n",
      "1760: 13 // 13\n",
      "1761: 13 // 13\n",
      "1762: 28723 // 28723\n",
      "1763: 617 // 617\n",
      "1764: 2411 // 2411\n",
      "1765: 22303 // 22303\n",
      "1766: 304 // 304\n",
      "1767: 28725 // 28725\n",
      "1768: 12832 // 12832\n",
      "1769: 28725 // 28725\n",
      "1770: 2437 // 2437\n",
      "1771: 2708 // 2708\n",
      "1772: 4038 // 4038\n",
      "1773: 858 // 858\n",
      "1774: 640 // 640\n",
      "1775: 28725 // 28725\n",
      "1776: 3345 // 3345\n",
      "1777: 1167 // 1167\n",
      "1778: 302 // 302\n",
      "1779: 23798 // 23798\n",
      "1780: 304 // 304\n",
      "1781: 4045 // 4045\n",
      "1782: 272 // 272\n",
      "1783: 356 // 356\n",
      "1784: 13416 // 13416\n",
      "1785: 5864 // 5864\n",
      "1786: 553 // 553\n",
      "1787: 304 // 304\n",
      "1788: 9562 // 9562\n",
      "1789: 2372 // 2372\n",
      "1790: 798 // 798\n",
      "1791: 302 // 302\n",
      "1792: 10936 // 10936\n",
      "1793: 4118 // 4118\n",
      "1794: 601 // 601\n",
      "1795: 1127 // 1127\n",
      "1796: 660 // 660\n",
      "1797: 659 // 659\n",
      "1798: 5245 // 5245\n",
      "1799: 851 // 851\n",
      "1800: 28723 // 28723\n",
      "1801: 1982 // 1982\n",
      "1802: 297 // 297\n",
      "1803: 18594 // 18594\n",
      "1804: 354 // 354\n",
      "1805: 1760 // 1760\n",
      "1806: 442 // 442\n",
      "1807: 352 // 352\n",
      "1808: 11795 // 11795\n",
      "1809: 4355 // 4355\n",
      "1810: 652 // 652\n",
      "1811: 356 // 356\n",
      "1812: 2818 // 2818\n",
      "1813: 19550 // 19550\n",
      "1814: 19178 // 19178\n",
      "1815: 442 // 442\n",
      "1816: 7794 // 7794\n",
      "1817: 3058 // 3058\n",
      "1818: 395 // 395\n",
      "1819: 6992 // 6992\n",
      "1820: 288 // 288\n",
      "1821: 7619 // 7619\n",
      "1822: 302 // 302\n",
      "1823: 5245 // 5245\n",
      "1824: 272 // 272\n",
      "1825: 349 // 349\n",
      "1826: 465 // 465\n",
      "1827: 20367 // 20367\n",
      "1828: 745 // 745\n",
      "1829: 279 // 279\n",
      "1830: 5096 // 5096\n",
      "1831: 13 // 13\n",
      "1832: 28767 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1833: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1834: 11143 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1835: 489 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1836: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1837: 28789 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1838: 13 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1839: 28705 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1840: 2 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1841: 28723 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1842: 11354 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1843: 3332 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1844: 8598 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1845: 304 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1846: 28725 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1847: 7193 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1848: 1222 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1849: 28725 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1850: 5566 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1851: 25537 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1852: 7911 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1853: 395 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1854: 5643 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1855: 574 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1856: 582 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1857: 6795 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1858: 28723 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1859: 6092 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1860: 7087 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1861: 871 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1862: 3864 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1863: 12233 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1864: 438 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1865: 17939 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1866: 27319 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1867: 791 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1868: 4920 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1869: 1581 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1870: 302 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1871: 18014 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1872: 4628 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1873: 272 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1874: 8084 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1875: 304 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1876: 28725 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1877: 16985 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1878: 659 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1879: 465 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1880: 20367 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1881: 690 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1882: 297 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1883: 28713 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1884: 2758 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1885: 13621 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1886: 304 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1887: 10578 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1888: 272 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1889: 11772 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1890: 28723 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1891: 617 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1892: 2411 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1893: 22303 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1894: 304 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1895: 28725 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1896: 12832 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1897: 28725 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1898: 2437 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1899: 2708 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1900: 298 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1901: 5202 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1902: 4382 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1903: 298 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1904: 6516 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1905: 459 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1906: 562 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1907: 2490 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1908: 28725 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1909: 3345 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1910: 798 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1911: 302 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1912: 23798 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1913: 304 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1914: 4045 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1915: 272 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1916: 11180 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1917: 659 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1918: 465 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1919: 20367 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1920: 4355 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1921: 690 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1922: 297 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1923: 4342 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1924: 4118 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1925: 272 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1926: 21928 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1927: 966 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1928: 13 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1929: 28767 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1930: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1931: 1838 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1932: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1933: 28789 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1934: 13 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1935: 28705 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1936: 2 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1937: 10093 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1938: 10706 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1939: 10131 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1940: 264 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1941: 460 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1942: 1976 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1943: 13 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1944: 28767 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1945: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1946: 6574 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1947: 28766 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1948: 523 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1949: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1950: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1951: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1952: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1953: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1954: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1955: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1956: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1957: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1958: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1959: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1960: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1961: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1962: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1963: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1964: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1965: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1966: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1967: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1968: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1969: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1970: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1971: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1972: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1973: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1974: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1975: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1976: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1977: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1978: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1979: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1980: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1981: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1982: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1983: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1984: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1985: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1986: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1987: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1988: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1989: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1990: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1991: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1992: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1993: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1994: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1995: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1996: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1997: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1998: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "1999: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2000: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2001: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2002: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2003: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2004: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2005: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2006: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2007: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2008: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2009: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2010: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2011: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2012: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2013: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2014: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2015: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2016: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2017: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2018: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2019: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2020: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2021: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2022: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2023: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2024: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2025: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2026: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2027: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2028: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2029: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2030: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2031: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2032: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2033: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2034: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2035: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2036: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2037: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2038: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2039: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2040: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2041: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2042: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2043: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2044: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2045: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2046: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2047: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "2048: 1 // -100          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader.dataloader))\n",
    "\n",
    "input_ids = batch[\"input_ids\"]\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "num_tokens = 2048\n",
    "idx = 3\n",
    "\n",
    "print(input_ids[idx, -num_tokens:])\n",
    "print(tokenizer.decode(input_ids[idx, -num_tokens:]))\n",
    "for i in range(1, num_tokens + 1):\n",
    "    label = labels[idx, -i]\n",
    "    input_id = input_ids[idx, -i]\n",
    "    if label == -100:\n",
    "        print(f\"{i}: {input_id} // {label}          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "    else:\n",
    "        print(f\"{i}: {input_id} // {label}\")\n",
    "\n",
    "# print(labels[0, -300:])\n",
    "# print(input_ids[0, -300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- ROW IDX = 0 ----------------------\n",
      "tensor([ 9158, 22597,  7649,   852,   298,  9467,  2421, 28725,   304,   736,\n",
      "          460,  2856,  1581,  4514,   302,  7062, 11715,  3427,   304,  9773,\n",
      "        28723,  2909,   302,   272,  1080,  1162, 28733,  4717,   281,  2233,\n",
      "         3024,  5747,  2576,   385, 28725,   446,   282,   314,   270,   753,\n",
      "          385, 28725,   304,   268,  2262, 10270, 28723,  2957,   281,  2233,\n",
      "          460,  9566,  7885,   438,  6887, 23127, 28725, 12649, 24363, 28725,\n",
      "          304,   799,  5903,   697, 28723, 20438,   835,   659,  2856,  9158,\n",
      "        17937,   369,   460,  4842,   298,   272,  2939, 28725,  2490,   272,\n",
      "        16720, 28764,   280,  3430, 28725,   690,   349,   264,  1423,   286,\n",
      "        10623,  3684,   298,   264,  9915, 27545, 28725,   304,   272,   261,\n",
      "        28764,   423,   293, 28725,   690,   349,   264,  7000,  2751,   302,\n",
      "          272, 16720, 28764,   280,  3430, 28723,   560,  4518,   298,  7062,\n",
      "        11715,  3427,   304,  9773, 28725,   736,   460,   835,  2856,   799,\n",
      "         4514,   302,  3427,   369,   506,   750, 19927,   486, 11715,  5679,\n",
      "        28725,  2490,   312, 10200, 13370, 28725,   690,   349,   264,  1212,\n",
      "          302,  3427,   369,  6202,   297, 20438,   297,   272,  2935, 28705,\n",
      "        28750, 28734,   362,  5445,   304,   659,  5176,   302, 23612, 28725,\n",
      "         9952, 28725,   304,  8307,  3427, 28723, 21013, 28725, 20438,  5751,\n",
      "          264,  6708,   304, 12836,  2819,   302,  7062,  3427,   304,  9773,\n",
      "        12078,   369,   460,  1162,  4407, 20998, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 1 ----------------------\n",
      "tensor([28725,  2490, 13631,   304,  7069,  1307, 28723,   816,   913,  3814,\n",
      "          298,  9788,   477,   368, 28808,  -100, 28705,    13,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "           13, 28741, 28747,  7812,   368,   354,   574,   297, 18831, 28723,\n",
      "         5592, 28725,   272, 14693,   349,  1269,   302, 21023,  1503,  9445,\n",
      "        28723,  1047,   368,   506,   707,   799,  4224,   442, 10864, 28725,\n",
      "         4665,   511,   459, 10816,  9647,   298,  1460, 28723,  -100, 28705,\n",
      "           13,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,    13, 28741, 28747, 22557, 28808,  7812,   368,   354,   574,\n",
      "         2145,   297,   813, 14693, 28713, 28723,   415,  4144,   660,  5028,\n",
      "          349,   429, 28734, 28723, 28782, 28734, 28725,   304,   272,  7968,\n",
      "        15069,   368,   541,  1745,   349, 28705, 28750, 28734,  7769,   325,\n",
      "          538,  7450,  3890,   609,  1047,   368,   506,   707,  3629,  4224,\n",
      "        28725,  4665,  1601,  1933,   298,  1460, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 2 ----------------------\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,    13,  2324, 21163,   575,   754, 28705, 28740,\n",
      "        28781,  4007, 21264,   395,  5757,  1492,   465,   354,   272,   312,\n",
      "        16239,   466,  2488, 28723,  -100, 28705,    13,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "           13, 22099, 28808,   816,  4198,   395,   393,   416,  1919,   298,\n",
      "         5663,  5804,  1492,   465,   354,   272,  1712,  7380, 11201,   404,\n",
      "          390,   744,   302,   272, 14469, 12270, 13293,   282,  2488, 28723,\n",
      "          816,  5682,   304, 11540, 20202, 14461,   304,  1486, 28733, 14817,\n",
      "         1492,   465,   369,  9943, 23581,   395,   272,  9702, 28809,  4638,\n",
      "        13811,   304, 20583, 13580, 28723,   415,  5804,  1492,   465,   478,\n",
      "        11448,  4658,  1486, 28733, 24035,  7212, 10090,   354,  1162, 28733,\n",
      "         4717,  4799,  1259,   390,   393,   416,  1919, 28725,   382, 28735,\n",
      "         4738, 28725,   367, 23046, 28725,   304,   524,  8597, 28777, 28723,\n",
      "         3489,  6503,   403,   298,  2231,   264,  5804,   286,  4466,   369,\n",
      "          403,  1560,  1717,  1323, 14714,   304, 26739,   925, 28725,   754,\n",
      "        13377,   272, 20497, 14469,  7212,  1081, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 3 ----------------------\n",
      "tensor([28725,  2708,  2437, 28725,   304, 13269,   294, 17002,   297,   320,\n",
      "         4879,  4877, 28723,   415, 10947,  4827,  6064,   659,   750, 28166,\n",
      "          297, 21325,   272,   938,   302,  2809,  2708,  2437,  7040,  1259,\n",
      "          390,  4105,  5377,  8326,   288,   304,  3618,  7420, 24073,   298,\n",
      "         9009,   798,  3345,   304,  2400, 10755,  2708,   522, 28723,    13,\n",
      "           13, 28781, 28723,   415,  7575, 11919,  2437, 27838,   288,   297,\n",
      "        20915,  1379,  1651,  7759, 28747,   851,  2488, 28725, 13492,   486,\n",
      "          272, 11304,   354,  4271, 18208,   472,   325, 28743,  2665,   557,\n",
      "          349,   264,  3681,   302,  6259,  6190,  9909, 28725, 26197, 28725,\n",
      "          304,  3618,  3338,  2739,   298, 11711,  1203, 23767,   304,  2708,\n",
      "         2437,   297,   798, 13053,   482,   466,   297, 20915,  1379,  1651,\n",
      "        28723,   415,  2488,   659,   750,  2358,   298, 26411, 13290,   302,\n",
      "        21363,   304,  8015,  4920,  9861, 28713, 17939,   438, 17032, 20367,\n",
      "          465,   297,   798, 13053,   482,   466, 28723,    13,    13,   657,\n",
      "          544,   302,  1167,  4469, 28725,  6259,  6190,  9909,   506,   750,\n",
      "         2358,   298, 10130,  8035,  4355, 20367,   465,   486, 21325,  1203,\n",
      "        23767, 28725,  2708,  2437, 28725,   304,  9893, 28742,  5038, 17002,\n",
      "          297,   272,  2411,   617,  1759, 28723,  2957,   791, 27319,   506,\n",
      "         4163,   298, 11957,  2372,  9562, 28725,  9397, 21363, 28725,   304,\n",
      "        21733,   798,  4893,   297, 22303, 12638, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 4 ----------------------\n",
      "tensor([  369,   624,   302,   272,   275, 15868, 28725,   418,   830,   412,\n",
      "         1343, 28706, 28725,   349,  6311, 13999,   304, 10658,   298,  3054,\n",
      "          272,  6337,   395, 10532,  2235, 28723, 13445, 28725, 17821,   349,\n",
      "        20958,   486,   272,   275, 15868, 28725,   304,   590, 26129,   713,\n",
      "          304, 14919,  2673,   272, 28594,   390, 17821,   304, 14919,  1464,\n",
      "          298,  8957, 28723,    13,    13,  3260, 12851,   403,   264,  4387,\n",
      "         3817,   354, 13708, 28713,   298,  3324,   684,  1096,   378,   403,\n",
      "          264,  3659,  8114,   354,   516,   275, 12032,   304,  8075,   294,\n",
      "         2838, 28707,  3572,  3238, 28723,   650,   403,  2358,   298,   938,\n",
      "          272,  2838,   298, 11418, 18978,   302,  2544,  2045, 28725, 16372,\n",
      "          352, 28725,   304,   272,  2195, 17568, 28725,  1312,   835, 19082,\n",
      "          288,   272, 16067,   395, 19345,  5704, 28713,   304,  1997, 15138,\n",
      "        13875, 28723,    13,    13,  1991, 14586, 28725,   272, 16067, 24345,\n",
      "        13708, 28713, 28742, 10532,  2235,   395,  1862,  5074,  4784,   431,\n",
      "          304,   516,  2016,   302, 13359, 17717,  1411, 28723, 13708, 28713,\n",
      "          403,   264,  9393,  2522,  1649,  1294,   693, 20177,   272, 21703,\n",
      "          304,  5679,   302,   516,  3153, 13942, 28725,   304,   345, 28738,\n",
      "          314,   451, 28742,  1981, 27513, 28739,   349,   264,  3659,  2757,\n",
      "          302,   516, 13106,   298, 16176,   288,   304,  5903,  1077, 17717,\n",
      "         5074,  4784,   431,   304,  1942,  2827, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 5 ----------------------\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,    13, 28737,   511,   459,   506,  2735,   298,\n",
      "          574,  4723,  1871, 28723,  2993, 28725,   736,   460,  2856,  4342,\n",
      "          368,   541,  1300,  1862, 16177,   442, 11574,   369,  6112,   295,\n",
      "        27443,  2245,  3429, 28747,    13,    13, 28740, 28723, 11147,  3270,\n",
      "          354,   345, 16913,   566,  7372,  6720, 28739,   442,   345, 16913,\n",
      "          566,  2245,  3429, 28739,   395,   574,  2990,   442,  4424,  1141,\n",
      "          298,  1300,   707,  1862,   442, 10396, 16177,   369,  6112,  1259,\n",
      "         4907, 28723, 28705, 28750, 28723,  4914,   395,  1862, 10455, 10858,\n",
      "        28725,  6311,  1395,   369,  2841,   653,   297, 18433,   442,  7062,\n",
      "         7372,  6720, 28725,   590,   993,   506,   295, 27443,  7372,  6720,\n",
      "          442,   993,   347,  2358,   298,  1745,   706,   354,   368, 28723,\n",
      "        28705, 28770, 28723,  4914,  3270,  2668,   452,  2089,   737,   911,\n",
      "         3467,   442,  5818,   266,   304,  5531,  2903,   486,  4723,   298,\n",
      "         1300,  6112,   404,   297,   574,  2698, 28723, 28705, 28781, 28723,\n",
      "        19500,   354, 18892,   477,   272,  1862,   390,   753,  8932,  9909,\n",
      "          442,  3884,  1443, 15217,   297,   574,  2990,   442,  3736, 28723,\n",
      "          315,  3317,  1167, 17278,   460, 10865, 28808,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 6 ----------------------\n",
      "tensor([  297, 28725,   272,  2768,  7319,  7296, 23756,   782,   682,  2231,\n",
      "          396,  4870,  4297, 22326,   297,   272,  8613, 28725,  6818,  5414,\n",
      "         3427,   298,   272,  8049,   304,  7099,   369,  4142,   298,   272,\n",
      "        19834,   302,   272,  7296, 28723,    13,    13,  1014,  2061,   682,\n",
      "         2268,  2818,   356,   272,   727,   302,  1370, 28745,   390,   272,\n",
      "         4376,  3125,   298, 25132, 28725,  7160,   281, 10206,  2910,   272,\n",
      "        18259, 28725,   264,  3987, 28725,  6100, 22998,  9522, 14610,   272,\n",
      "         8613, 28723,   415, 14545,   304,  9684,   295,  1139,   682,  1272,\n",
      "         7273,   272,  8049,   302,   272,  7099,   304,  1579,  2053, 28725,\n",
      "        13098,   272,  8613,   298,   913,  1019,   680,  1201,  1860,   304,\n",
      "        20927, 28723,    13,    13,  1313,  2723, 28742, 28707,   521,  5348,\n",
      "          354,   272,  8086,   298,   347,   521, 23077,   522, 28725,   395,\n",
      "         4376,  1780,   304,  3296,   668,  8855, 21640,   486,  2936, 25752,\n",
      "        22239, 28713,   369,   408,  2569, 16307, 28717,   723, 28725,   395,\n",
      "         5917, 22838, 14912,  1905,  1059,   272,  8049,   304, 25752,   533,\n",
      "         1882,   369,  3894,   286,  1059,   272,  8613, 28723,    13,    13,\n",
      "         2565,   528, 28725,   272,   727,   302,  1370,   304,  8086,  4331,\n",
      "        21733,   272,  9693,   302,   456,  2841,  1633, 28723,   661,  1269,\n",
      "         1012,  2470,  5223,   297,   272,  8613,  1581, 28725,  9055,   264,\n",
      "         4842,   304, 27661,  2659,  1012,   727, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 7 ----------------------\n",
      "tensor([ 2623,   298,  3251, 16818,   293, 23018, 28725,   272,   475,  1555,\n",
      "          282,   794,  2535,   930, 10113,   297,  5353,   682,   347,   396,\n",
      "         8099, 14416,   298,  2659, 28723,   560,  2485, 28725,   272,  1489,\n",
      "          727,   298,  3251,  5558,   682,  3289,   356,   574, 10299,   304,\n",
      "          767,   368,   947,   298,  2659, 28723,  -100, 28705,    13,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,    13, 28737, 28742, 28719,  8200,   369,   315,   829,  1316,\n",
      "        28723,   415,   475,  1555,   282,   794,  2535,   930, 10113,   349,\n",
      "         6304,  4407, 20998, 28725,   304,   315, 28742, 28719,  1864,   368,\n",
      "        28742,   584,  3555,   378, 28723, 16818,   293, 23018,   659,   264,\n",
      "         6708,  8932, 22597,   304,   349,  2651,   354,   871,  1524, 28725,\n",
      "         3427, 28725,   304,  3181,  1007, 12649, 24363, 28725,   579,  1038,\n",
      "         1864,   298, 11418,   272,  4424,   304,  3251,   741,   302,   272,\n",
      "         8376,  2533, 17181,   737,   272, 13863,  5922, 28725,  4785,  2089,\n",
      "        28725,   304,  1637,  2815,  1938,   574,  3079, 28723,  1047,   368,\n",
      "          927,   707,   680,  1316,   297,  7394,   574,  6596,   298,  5558,\n",
      "        28725,  1601,  1933,   298,  1460,   528, 28808,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 8 ----------------------\n",
      "tensor([ 3316,  1159,   521,  7067,  3925,   304,  8169,   303,   288, 28723,\n",
      "         1136,   354, 19718,   356,   272, 11495, 12031,  2760,  6592, 10056,\n",
      "        14833, 28725,  1236,   460,   264,  1664, 17278, 28747,    13,    13,\n",
      "        28733,  1670,  5200,  3690, 14812, 10056,   334,   621, 28747,  1791,\n",
      "          967,  4210, 21606,   304,  8478,   302, 16859, 28725,   967, 28705,\n",
      "        28740, 28733, 28750, 24789,  2345,  1053,   302,   277,  2594, 22686,\n",
      "          304,   264,  9550,   338,   302,  9246, 28719,   641,   298,   272,\n",
      "         7994, 28723,   387, 21853,  3690, 14812, 10056,   334,   621, 28747,\n",
      "         3301,   272,   686,   374,   302, 28705, 28740, 28733, 28750,   442,\n",
      "         5897,   298,   272,  7994,   304,   938, 14545, 18342,  3519,   302,\n",
      "        21375,  4749,   442,  3075, 21375,  4749,   354,   264,  5963, 17355,\n",
      "        28724, 20746, 28723,   387,  3838,  1067, 14233,  3690, 14812, 10056,\n",
      "          334,   621, 28747,   401,   738,   741, 28393,  6138, 18393, 28363,\n",
      "          778,   272, 14833,  7994,  1159, 22411,   354,   264, 15925,   472,\n",
      "         9230, 28723,   387,   334, 24859,   329,  3690, 14812, 10056,   334,\n",
      "          621, 28747,  3301, 28705, 28740, 28748, 28781,  6211,   302,  7221,\n",
      "         2106,   480,   893,  3802, 14609,   266,   329,   298,   272,  7994,\n",
      "          354,   264, 21257, 16859, 28723,  2957, 19718,   541,   347, 23799,\n",
      "          778,   272, 14833,  7994, 28725,  8169,   303,   288,   442,  5211,\n",
      "          742, 28725,  4771,   298,   574, 22731, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 9 ----------------------\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,    13,  5604,  3059,   298,   272,  2245, 28725,   390,   302,\n",
      "          272, 20990,  1269,   486,   393,   493, 11527,  2799,  8819,  4776,\n",
      "         6927,   279,   520, 10947,  1150,   276,   356, 11463, 28725,  5085,\n",
      "         5558,   622,  1149,   272,  5611,  9314,   477,  1756,   431,   298,\n",
      "        20071, 28768,   912,   297,  3999, 28725,   562,   272,  3459,  3608,\n",
      "          403,  2783,   298,   347,  6321, 28723,  -100, 28705,    13,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,    13,  5604,  3059,   298,   272,  2245, 28725,  1756,   431,\n",
      "        14400, 28742, 28713, 27864,   297,   272, 14400,  2372,  4045,  6593,\n",
      "          742,   654,  7004,   438,   272,  2944,  9887,  1987,   486,   393,\n",
      "          493, 11527,  2799,  8819,  4776,  6927,   279,   520, 10947,  1150,\n",
      "          276, 28723,  2993, 28725,   272,  2245,  2368, 28742, 28707,  3084,\n",
      "          680,  4162,   356,   272,  6593,   742, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 10 ----------------------\n",
      "tensor([   13,  6325,   368,  4665,  3084,   528,   395,   264, 14060,   302,\n",
      "          272,  5447,   356,  2079,   418,  6800,  7408,   541, 28742, 28707,\n",
      "          347,   272, 25606,   302,  3897,  2783, 28804,  -100, 28705,    13,\n",
      "        28789, 28766,   489, 11143, 28766, 28767,    13,  1014,  5447,  3342,\n",
      "          274,   910,   418,  6800,  7408, 28742, 28713,   633,  6945, 17767,\n",
      "         2372, 28725,  2404, 19714,  7929, 28780, 28725,   349,   865,  2632,\n",
      "         1059,   418,  6800,  7408, 28742, 28713,  8604, 28737, 11709,  2005,\n",
      "          302,  8309, 28725,   690,   659,   264, 10102,  2859, 20646, 28725,\n",
      "        27931,   272,  2188,  2624,   354,  2404, 19714,  7929, 28780, 28723,\n",
      "          415,  5447,  1629,  1139,   369,   354,  2404, 19714,  7929, 28780,\n",
      "          298,  8168,   871,  2173,  4628,   304,  2727,   272,   345,  4428,\n",
      "        22401,   302,  3897,   862,   418,  6800,  7408,  3208,   298,  1038,\n",
      "          272,  2372, 14494,  1059,   264, 25941,  2819,   302,  8309,   304,\n",
      "        19245, 28723, 21963,  6308, 28725, 25606,   659,   750,  2358,   298,\n",
      "         2333,   871,  2188,  2624,   486,  1250, 14494,   356,   264,  5335,\n",
      "         2819,   302, 14926,   304,  8309, 28723,   415,  5447, 12308,   369,\n",
      "         1312,   418,  6800,  7408, 28742, 28713,  8604, 28737, 11709,  8309,\n",
      "         3688,   298,  6710,   652,  4563, 28725,   418,  6800,  7408,  3208,\n",
      "          298,  3232,   356, 20767,   272, 18858,   302,  2404, 19714,  7929,\n",
      "        28780,   298,  4563,   264, 17525,  7938, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 11 ----------------------\n",
      "tensor([ 4295,  1096,  8886,   442, 10506,   302,  8376,   905,  6842,   264,\n",
      "         1486,  5637,   477,   272,  4077, 28725,   390,  1162,   390,   272,\n",
      "         2952,   798, 28723,   851,  5637,   349, 10624,   286,   486,   272,\n",
      "         5903,   472,  5679,   369,   659,  6202,   297,  1287,  5099,   302,\n",
      "          272,  1526,   369,   659,  4163,   298,   905, 12647,   298,   873,\n",
      "          680,   684,   272,  3327,  4621,   302,  8376,  6992, 28723, 28705,\n",
      "           13,    13,  1014,  4077,   835,  7196,   477,   272,  5593, 21722,\n",
      "          369,  1567,   395, 20449,  8886,   442, 10506,   302,  4387,  5207,\n",
      "        24508,   497, 28723, 11302, 28725,  2809,  4077, 14926,   737, 11690,\n",
      "        28725, 18351, 28725,   304,  8488,   506,  1269,   378,  7089,   354,\n",
      "        13207,   905,   298, 10094,   304,  6049, 28606,   495,  8886,   442,\n",
      "        10506,  3214,   486, 12930,   283,   941,  4295, 28725,   690,   506,\n",
      "         3629,  7483,   272,  5637,   354,  1259,  3036, 28723,    13,    13,\n",
      "        23475,   741,  5207, 24508,   497,   993,  3555,   272,  4501,   442,\n",
      "         7949,   477,   272,  7483,   798,   472, 28725,  2663,  1300,   378,\n",
      "          396, 18793,   302,   652, 12917,   304,   264,  5957,  7677,  3623,\n",
      "          778,   652,  3327,  4621, 28723,   415,  5957,  4501,   541,   835,\n",
      "         4244,  6727, 28725, 12758,   304,  1019,  1658,   706,   297,  9259,\n",
      "        11846, 28725,   390, 12930,   283,   941,  4295,  2608,   938, 18925,\n",
      "        23532,   298, 11319,  6203,   302,   706, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 12 ----------------------\n",
      "tensor([  356, 28725,   304, 12652,  1372, 28733,   715,  9796,   354,  6292,\n",
      "        20559, 28723,    13,    13, 28770, 28723, 24223, 11931,  3698, 15613,\n",
      "        28747,  6412,   438,   910,   272, 14678,   829,  4916,  6346, 15613,\n",
      "        28723,   330,  1162, 28733, 15873,   286, 14678,   541,  1316,   272,\n",
      "         3698,   298,  9087,  3754,   395,  5688,   304,  4916,   272,  7544,\n",
      "         6346,  2659, 28723,    13,    13, 28781, 28723,  2597,  3275,   272,\n",
      "         5088,   356, 12832, 28747, 11772,   272,  5088,   369,   272, 14678,\n",
      "          682,   506,   356,   272,  3698, 28742, 28713, 21197, 12832, 28723,\n",
      "        18208,  1077,  4918,   829,  4602,   272,  3698,   264,  5864,  3558,\n",
      "          302,   727, 28725,  4607,  1077, 15561,  9537,  1259,   390,  1178,\n",
      "         4905,   304, 14438, 28723,    13,    13, 28782, 28723,  8349,   272,\n",
      "         7544,  2434, 28747,  8126, 28725,  1917,   272,  7544,  2434,   302,\n",
      "          272, 14678, 28725,  2490,   272,  4099, 28725,  4154, 28725,   304,\n",
      "        15260, 12364, 28723, 25217,   369,   272, 14678,   349, 23581,   395,\n",
      "          272,  3698, 28742, 28713,  8326,   304,   272,  7196,   378,   622,\n",
      "         2405,   298,   272,  1955,   298, 16205,   272,  6966, 28723,    13,\n",
      "           13,  1930,  4681,   279,  3864,   272,  4507,   697,  2818,   356,\n",
      "          652,  5088,   356,   272,  3698, 28742, 28713,  1955, 28725,   368,\n",
      "          541,  1316,   706,  6619,   652, 22134, 28725,  4454,  1081,   652,\n",
      "         9537, 28725,   304,  5247,   652, 14008, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 13 ----------------------\n",
      "tensor([  477,  6904,   288,   442, 21325, 21668,  2373,   272, 10036,  6259,\n",
      "         6153,   302,  1102,  2374, 11021,   442,  2211, 11021, 28723,   851,\n",
      "         2309,   659,  6481,   298,  2231,   264,   680,  3051,   279, 28733,\n",
      "         5527,  1587,   354,   798,  6904,  1339,   297,   272,  2939, 28723,\n",
      "           13,    13,  2820,  1039,  2757,   349,  5558, 28742, 28713, 12045,\n",
      "         5836,  7596, 28725,   690,  5976,   354,  1203, 17556,   302,  6259,\n",
      "        21567,  1012,  1664,  1267,   297,  1745,   298,  5297, 10755,   477,\n",
      "         2719,  1368,  8450,   297,   264,  2552,  2840,   304, 20345,  2967,\n",
      "          544, 20904,   442, 12167,   369,  1736,   298,   435, 14676,  1443,\n",
      "        28723,    13,    13, 26813, 13544,   659,   835, 13492,   396, 23331,\n",
      "        28733,  9903, 12992,   304, 12045, 12744,  9238, 28725,   690,  6694,\n",
      "         1002,  9278,   302,   435, 14676,  1443,   304, 21363,  2373,   798,\n",
      "        12638,   304,   659,   272,  1982,   298,  5704,   789,  1395,  1419,\n",
      "        14227, 28723,    13,    13, 18171,   460,   776,   264,  1664,  9254,\n",
      "        28725,   562,   590, 16499,   369,   736,   460,  4118, 12108,   369,\n",
      "          541,   347, 22427,   298,  2962,   435, 14676,  1443,   297,  1581,\n",
      "         2758, 28713, 28723,   661,   622,   347,  2278,   354, 11715,  8038,\n",
      "          298,  3881,   272,  2288,   274,   304, 25394,   302,  6891, 28733,\n",
      "          485, 14676,  1443, 10582,   297,   799,  5780,   304,  2822,   477,\n",
      "          706,   739, 10423,   652,  1216, 12108, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 14 ----------------------\n",
      "tensor([ 1038,   516,  1069,   298,   272,  3536,  1526, 28723,  2993, 28725,\n",
      "          390,   400,  2774,  1060,   272, 12997, 28725,   400,   829,  3934,\n",
      "         3331, 16005,  3894,   288,   582,  5083,   713, 28723,    13,    13,\n",
      "        22922,   294,   808,   852,   297,   390,   400,  7185,   516,  8957,\n",
      "         1659,   347,   306, 11328,   286, 28723,   650,  4377,  1699, 10322,\n",
      "          852,   582,   272, 12997,   304, 18750,   356,   264, 14378, 28725,\n",
      "         6265,   390,   516,  7900,  3807, 17070,   272, 12997, 28723,   650,\n",
      "         2723, 28742, 28707,  1864,   767,   298,   511,  1679, 28725,  3992,\n",
      "          442,  9314, 28725,   562,   400,  2580,  2477,  3551,   829,   347,\n",
      "        20174, 28723,    13,    13,  1014,   676,  2056,   264,  3534,  5276,\n",
      "          304,  4857,   582, 28725, 16486,   298,  3517,   739,   400, 16188,\n",
      "          477,   516, 16622,  1633, 28725,   562,   767,   400,  2672,  1679,\n",
      "         1269,   713,  2115,   297,   516, 11637, 28723,   330,  2971, 28725,\n",
      "          704,   265,   959,   286,   304, 23889, 28725, 16188,   477,   272,\n",
      "        20519,  5458, 28725,   304,   400,  7185,   590,   654,  1560,   297,\n",
      "          272,  1348,  4620, 28723,    13,    13,  1014,   989, 24305,   439,\n",
      "        11247,  4813,  1007,  1272,  2233,   304,  4377,  7185,   652,   865,\n",
      "         3317,   354, 16856,   403,   298,   771,  2553,   298,  8957, 28723,\n",
      "         1306,  5317,  1060,   272, 12997, 28725,  4313,   354,  2424,   272,\n",
      "         7689,   553,   297,  4143,   354,   706, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 15 ----------------------\n",
      "tensor([ 1300,  2905,   477, 11576, 19572,   298, 13894, 19824, 28723,    13,\n",
      "           13, 28750, 28723,   413,  1074, 28724,   387,   413,  1074, 28724,\n",
      "          349,   264,  2668,  2912,   354,  1021, 15248,   304, 28465,  4907,\n",
      "        28725,  2490,  3493, 27261, 28723,   995,   541,  5531,   574,  3472,\n",
      "          486,  3238, 28725,  3181, 28725,   304,  4144,   298,  1300, 27261,\n",
      "          369, 16395,   574,  8326, 28723,    13,    13, 28770, 28723,   351,\n",
      "        27700,   387,   351, 27700,   349,   264,  2668,  2912,   354,  7126,\n",
      "        10611,   298,  6112,   652, 13580, 28725,  2490, 27261,   354,   574,\n",
      "         8133, 28723,  1306,  2405,   264,  6677,   302, 12078, 28725,   477,\n",
      "        11495,   298,  4638, 28725,   304,   368,   541,  1745, 25831,  1104,\n",
      "         3000,   442,   521,   898,  3000, 28723,    13,    13, 28781, 28723,\n",
      "         3951, 28723,   675,   387,  3951, 28723,   675,  5751,   264,  5401,\n",
      "         7495,   302, 16662,  1524, 25831, 28725,  1704,   404, 28725,   304,\n",
      "        17806, 28723,   995,   541,  3472,   486,  3238, 28725,  7325, 28725,\n",
      "          442,  3817,  3209,   298,  1300, 27261,   369,  9019,   574,  8059,\n",
      "        28723,    13,    13, 28782, 28723,  7506,   855, 28710,  3951,   387,\n",
      "         7506,   855, 28710,  3951,   349,   264,  1191,   601,  1524,  2668,\n",
      "         2912,   369,  4190,  3493, 27261,   477, 20053, 10611,  1401,   272,\n",
      "         1526, 28723,   995,   541,  5531,   574,  3472,   486,  4144,   298,\n",
      "         1300, 16662,  7769,   354,   574,  9384, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 16 ----------------------\n",
      "tensor([ 9496,  5949,   541, 20765,   272,  1222,   304,  6387, 22239,  4628,\n",
      "         7151,   354, 24643,   272,  4620, 28723, 21596, 28733, 10045,   288,\n",
      "         2522,   269, 11041, 28747,    13,    13, 28740, 28723,  5812,   547,\n",
      "         2822,   404,   778, 12690,   304,  2111,  1430,  5964,   264, 23324,\n",
      "         9165, 13406,   298,   960,   575, 28723, 26268,   829,  3024,   264,\n",
      "        25511,  6346,   693,  6755, 18925,   442,   264, 12072,   580,   263,\n",
      "          693,  6755, 12143,   578,   534,  6657, 28723,  2530,  1430, 13406,\n",
      "          349,  7885, 28725,   272,  2071,   541,  3342,   272,  1489, 14915,\n",
      "          298,   272,  4620, 28723, 28705, 28750, 28723,  5938,   264,  3905,\n",
      "        28733,  1674,   288, 13406,   298,  1290,  6212,   264, 23324,  9165,\n",
      "         4620,   304,  5245, 26167,   298,   378, 28723,  9496,  5949,   541,\n",
      "         1388,  8617,  4543,  1581, 12426, 28725,  1259,   390,   272, 12788,\n",
      "          271, 28725,   272, 12354, 28725,   304,   272,   486,  1793,   263,\n",
      "        28723, 28705, 28770, 28723,  8290,  2822,   404,  3905, 28733,  1674,\n",
      "          396,  5038,  6041,   263, 13406, 28723, 18847,   297, 12690,   442,\n",
      "         1741,  4938, 28725,  2822,   404,   541,  5245,  1413, 25469, 10223,\n",
      "        15251, 28725,  2843,  1491,  7836,  9804, 28725,   304,  1008, 28733,\n",
      "         1270,  1058,  9804, 28723,  2957, 22969,  6290,   622,  1316,  2822,\n",
      "          404,   298,  1950,   272,  6266,  4892,   298,  5297, 28725,  9051,\n",
      "        28725,   304,  9421,   298, 23324,  9165, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 17 ----------------------\n",
      "tensor([ 7267, 10833,  4669,   484,   361, 28720,  3119,  1059,   272,   799,\n",
      "         6470,   304, 11547,   586,  4501,   298,   264,  2128, 11969,   712,\n",
      "         5721,   660,  1572,   356,   264,  4718,  9799, 28723,   661,   403,\n",
      "          390,   513,   378,   403, 10681,   272,  2470,   395,   592, 28725,\n",
      "        16269,   272,  1370,   776,   390,  1188,   390,   478,   654, 28723,\n",
      "           13,    13,  2198,   315,  7715,   582,   264,   484, 13327, 28725,\n",
      "          315,   829,  1601,   871,  3713,  1467,   304,  4336,   297,   586,\n",
      "         1021, 28725,   304,   272, 14024, 28742, 28713,  5799,  1467,   739,\n",
      "          315, 24215,   586,  9033,   778,   378, 28723,   415,  5666,  1214,\n",
      "        11700,   395, 16859,   297,   586,  5108,   387,  7221,   304, 19373,\n",
      "          438,   272,  1348,   727, 28723,   415,  9230,  6209,   286,   528,\n",
      "          852,   298,   586, 13159,   739,   315,   682,  1316,   586,  3487,\n",
      "         4083,  3088, 18707,  2040,   477,   516, 13409, 28723,   415, 24048,\n",
      "          302, 19098, 12394,   515,  1269,   272,  2470,   369,  1188,  4017,\n",
      "         1623, 28723,    13,    13,  3260,  1370, 14795,   528,   369,   741,\n",
      "          302,   272,  1489,  1722,   297,  1411,  5405, 28742, 28707,  3388,\n",
      "         1672,  8642,   442,  3487, 27864, 28725,   562,  3519, 28725,   272,\n",
      "         3588,  9235,  5223,   395,   905,   478,  2016, 28723,   315,  2770,\n",
      "        11977,   298,   506,   456,  2470,   304, 15131,   354,   272, 12607,\n",
      "          369,   682,  1432,   528,   264, 14927, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 18 ----------------------\n",
      "tensor([  304,  9811, 18454, 28723,  1306,  3246, 27556,   286,   297,   272,\n",
      "        13463,  3239,  1526, 28725,   395,  2887, 20557,   304,  5207, 24508,\n",
      "          497,   972,   629,   288,   298,   652,  9926,   298,  7324,   652,\n",
      "         1317,   697, 28723,  1092,  1019,   390,   652, 24665,  8108, 28725,\n",
      "          590,  1484,  9400,   272,  2611,   590,  5970,   297,  2016,   297,\n",
      "          272,   907,  1633,   387,   652,  6431,  8773,   354, 13198, 28723,\n",
      "         1306,  5317,   298,  6878,   304,  8403,   380, 28725, 13884,   272,\n",
      "        16824,   302,   767,   403,  2572,   395, 13506,   304,  9804, 28723,\n",
      "         1015,  1059,   378,   544, 28725,  4537, 28722, 15089,   304,  4537,\n",
      "        28722, 15303,   515,  7568,   486,  1430,   799, 28742, 28713,  2081,\n",
      "        28725, 11787,  1430,   799,  1059,   272,   582, 28713,   304,  1060,\n",
      "        28713,   302,  1411, 28723,  1306,   553,  1419,  1545,  7297,  2841,\n",
      "          387,   264,  2016,   369,   403,   712,  4618,   297,   652,  6431,\n",
      "         8773,   354,  2887, 28725,   562,   690,   553, 10039,   778,  1545,\n",
      "         1188, 13551,   304,   680, 19258, 28723,  1136,   590,  2382,   852,\n",
      "          356,   652,  8123, 28725,   590,  2580,   369,   590,   553,  3859,\n",
      "         1545,  7297,  4842,   304,  2841,   387,   264, 13463,  3239, 17424,\n",
      "          369,   682, 26649,   997,  2547,   304,  2887, 24883,   354,  1267,\n",
      "          298,  1567, 28723,  1015,   390,   590,  2382,   298,   272,  3437,\n",
      "        28725,   590,  2580,   369,   590,   553,  1430,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 19 ----------------------\n",
      "tensor([ 6665,   272, 12249,  1303,   304,  6203,   302,   272, 16578, 28725,\n",
      "          690,   272,   315,  1798,   905,  1032,   390,   264,  1863,  3517,\n",
      "          356,   652, 16415,   304,  1069,   302,  1411, 28723,    13,    13,\n",
      "        28770, 28723,   415, 14300,   302,  8307,  5362, 28747,   415,  4409,\n",
      "        20150,  3058,   835,  4180,  1377,  8307,  5362, 28725,   690,   349,\n",
      "         2598,   390,   264,  1069,   298,  1155, 25568,   380,   272,   315,\n",
      "         1798,   905,   778,  4409,  5679, 28723,   851, 12652,   264, 21556,\n",
      "         2373,   272,  3618, 28725,   390,   741, 20318,   272,  5362,  1312,\n",
      "         2663,  1032,   378,   390,   264,  5483,   298,   652,  7062,  1069,\n",
      "          302,  1411, 28723,    13,    13, 28781, 28723,  6504,   266, 28729,\n",
      "          809, 28809, 28713, 12568,   298,  2268, 28747,  6504,   266, 28729,\n",
      "          809,   349,   605, 11143,   298, 20150,  8015,   304, 12477,   378,\n",
      "          390,  6334,   304,  1397,   366,  4296, 28723,  2993, 28725,   739,\n",
      "          516,   639,  5356,  1966, 28725,   418,   809,  7187, 28725,  9105,\n",
      "         1074,   298, 22266, 28725,  6504,   266, 28729,   809,   704,   656,\n",
      "        28713,   713, 28723,    13,    13,  2675,   455, 28725,   272,   533,\n",
      "         1029,  1444,   272,   315,  1798,  5679,   304,  8307,  8015,   349,\n",
      "         2248, 16152,   390,   264, 11371,   354,  1982,   304,  8208, 28725,\n",
      "          390,   272,   315,  1798,   905,   460,  7207,   298, 12773,   272,\n",
      "         4435,  4248,   356,   486, 20150,  1443, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 20 ----------------------\n",
      "tensor([18851,  3024,   847,   601,   367,  1785,   274,   276, 28725, 10227,\n",
      "          277,   887, 22421, 28725,  1439, 12802,   285,  1632, 28725,   442,\n",
      "        10558, 28724,   576,   270, 13628, 28723,  2957,   541,   967,   264,\n",
      "         5543, 15362, 28724,   442,  3823,   884, 16859,   298,   272, 18851,\n",
      "          304,   541,  1038,   378,  1019,   680, 15992, 28723,  4003,   460,\n",
      "          741, 10636,   354,  2492, 28431,   770,  1439,   406,  1053, 28747,\n",
      "        28705,    13, 28740, 28723,  7043,   395,  1370, 28733,   738, 10714,\n",
      "          442,  6138, 10714,   369,   659,   750,  1749,   575,   298,  6964,\n",
      "          354,   264,  1664,  3316, 28723, 28705, 28750, 28723, 18514,   574,\n",
      "        10714,   778, 13563,   274,   325, 14146, 28705, 28740, 17013, 28731,\n",
      "          304, 13647,   706,   395, 24403,  5503,   304,   707, 11785,  3302,\n",
      "          742,   737, 25795, 14761,   442, 10088,  3302,   288, 28723, 28705,\n",
      "        28770, 28723,  1670,   923,   272, 10714,  1019,   346,   356,   264,\n",
      "        22411, 12173,   304,   287,   621,   438, 28705, 28770, 28782, 28734,\n",
      "        28902, 28765,   354, 28705, 28740, 28734, 28733, 28740, 28782,  3486,\n",
      "        28725,   442,  1996,   590,   460, 13863,  9060,   304,  8578,  2834,\n",
      "        28723, 28705, 28781, 28723,  3169,   272,  1439,   406,  1053,  5106,\n",
      "          304,  4143,   706,   297,   396,   264,  2262,   454,  7640,   354,\n",
      "          582,   298,   264,  1819, 28723,   315,  3317,   368,  1300,  1167,\n",
      "        10636, 10865, 28808, 22823,   574, 18851, 28808,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 21 ----------------------\n",
      "tensor([28808,  4003,   460,   741,  4870,  2157, 10730,   369,  3232,   356,\n",
      "         1008, 28733,  1013,  1784,   304,  3327,  6057, 28747,    13,    13,\n",
      "         4068, 28705, 28781, 28782, 28733, 28781, 28784, 28747,  5235,  4343,\n",
      "        15239,   980,   274, 28705,    13, 28733,   330,  9915,  4575,  2621,\n",
      "          356,   624,  2884,   304,   264, 11510,   356,   272,   799, 28705,\n",
      "           13, 28733,   981,  1227,   655,   302,   264,  8035,   368,   506,\n",
      "        12565,   297,   272,  2609, 28723,  1602,   863,   368, 17132,   378,\n",
      "        28804,  1824,   863,   368,  2822,   477,   272,  2659, 10584, 28705,\n",
      "           13,    13,  4068, 28705, 28781, 28787, 28733, 28781, 28783, 28747,\n",
      "        14683,  1007, 12511,  5938, 28705,    13, 28733,   330,  9915,  4575,\n",
      "         2621,   356,   624,  2884,   304,   264, 11510,   356,   272,   799,\n",
      "        28705,    13, 28733,   981,  2601,   535,   910,  2608,   368,   938,\n",
      "         5514,  5473,   272,  1370, 28723,  1691,   378,  8538,   442, 21413,\n",
      "         2131,   574,  8057,   304, 10526,  1162, 28733, 15823, 10584, 28705,\n",
      "           13,    13,  4068, 28705, 28781, 28774, 28733, 28782, 28734, 28747,\n",
      "        23433,  3187,   973, 28705,    13, 28733,   330,  9915,  4575,  2621,\n",
      "          356,   624,  2884,   304,   264, 11510,   356,   272,   799, 28705,\n",
      "           13, 28733,   981,  1227,   655,   302,   264,  5541,   368,   947,\n",
      "          298,  6619,   297,   272,  1679,   879, 28723,  1824,  5944,   541,\n",
      "          368,  1388,   298,  1038,   378,  4804, 10584,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 22 ----------------------\n",
      "tensor([  440,   369,  7263,  5321,   272,  4759,   477, 12507,  6727,   734,\n",
      "         1259,   390, 27702,   304, 25231,   408,   748, 28723,   661,   835,\n",
      "         7263,   298,  6833,   269,   272,  4630,   296,   304,  7643,   272,\n",
      "         9293,   302,  3199, 16961,   304, 11503, 28720,   326,   466,   352,\n",
      "        28723, 16569, 28725, 11781,  8255,   334,   659,   750,  4894,   298,\n",
      "        10974,  3495,  5786,  4885, 28725,   690,   541,  1316,  4916,   272,\n",
      "         4759, 28742, 28713, 14024,   304,  7643,   272,  9293,   302,  4433,\n",
      "         4715,   304,  1425,   655,   867, 28723,  1047,   368,   506,   289,\n",
      "         1106,  4759, 28725,   315,   682,  6557,  1413,   272,  1500, 28802,\n",
      "        28828, 14510,   457, 28725, 17104,   263, 28725,   304,  6885,   392,\n",
      "          324,  4024, 28723,   415,  3587,   457,   304,  6339,   263,  1316,\n",
      "          298,  7873,   272,  4759, 28742, 28713,  4229,   284, 28769,  6157,\n",
      "          304,  7643,  5503,  4885, 28725,  1312,   272, 17261,   324,  4024,\n",
      "         5312,  7974,  6521,  2922,   352,  1671,   533,   476,  3080,  1627,\n",
      "          411, 28723, 13695, 28725,   272,  4229, 13506,  1307,   297,  1167,\n",
      "         4076,  1259,   390,   389,  3679,  1429, 28708,   304,  5821,  9524,\n",
      "        28708,  5503,   541,  1316,   579, 15203,   304,   307,   423,   789,\n",
      "          272,  4759,  1671, 13098, 16418,   352,   442,  1721,  8508, 28723,\n",
      "          315,  3317,   456,  7263, 28808,  3169,   528,   873,   513,   368,\n",
      "          506,   707,   799,  4224,   442, 10864, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 23 ----------------------\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,    13, 22099, 28808,   560,   415, 23147,   895, 22279, 28725,\n",
      "        14717,   304, 19150,  5197,   272,  6128, 16841, 28723,   382,  7933,\n",
      "          367,   643, 28711,   485,  9021, 14717,  1096,   630,   349, 19200,\n",
      "          480,  3000,   304,  1997,  2689,   601,   354,   901,  3257,   616,\n",
      "          353, 15527, 28725,  1312,  1298,   343,   416, 12185, 18752,  4137,\n",
      "        21352,  9021, 19150,  1096,   400, 13270,   973,   516,  5450,   477,\n",
      "          272,   798,   304,  8315, 26491, 18835,  3358,   390,   264,  1204,\n",
      "        28723, 14115,   689,  8317, 11360,  9021, 19150,   354,   516, 25824,\n",
      "        28733, 27369, 13313,  2677,   302,   272,  1298,   343,   416, 28725,\n",
      "         5374,   298,   516, 11054,   304,  5277, 17468, 28723, 16658,   365,\n",
      "         3572,  3520,  9021, 14717,   739,   400,   349,   340,  9110,   486,\n",
      "          689,  8317, 11360,   778,  4195,   369,   382,  7933,   304, 25895,\n",
      "          460,  5762,   294,  6894, 28725,  4622,   264,  3367,   302,  1997,\n",
      "        22758,   438,  1250,  4052,  1006, 28723, 21013, 28725,   272,  7092,\n",
      "         1949,  3626,   272,  1581,  4342,   297,   690,  6128,  2659,   304,\n",
      "         3215,   395, 14717,   304, 19150, 28725, 12144,   288,   272, 18835,\n",
      "         5088,   302,  1167, 13855,   356,  6992, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 24 ----------------------\n",
      "tensor([ 2324, 28742,   584,   379, 10376,   544, 28725,   395,   544,   813,\n",
      "         3992,    13,  2946, 17758,   302,   305,   594, 28725,   297,   813,\n",
      "         7739,    13, 15265, 20756, 28725, 26575,  2270,  6833,    13,    13,\n",
      "         3400,   394,   366,   982, 28724, 28742, 28713,  5430, 28725,   298,\n",
      "         7223,  3414,    13, 15265, 25957,   622,  3894, 28725,  1484,   298,\n",
      "          347,   281, 14335,    13,  1733,  1345,   297,   813,  8773, 28725,\n",
      "          579, 19327,    13, 28738, 11553, 28725,   813, 11115,   622,   605,\n",
      "          687,    13,    13,  1209, 18235, 28747,    13, 14724,  1418,  5783,\n",
      "        28725,   395,  4813,  1503,  1659,    13,  2324, 28742,   584,   379,\n",
      "        10376,   544, 28725,   395,   544,   813,  3992,    13,  2946, 17758,\n",
      "          302,   305,   594, 28725,   297,   813,  7739,    13, 15265, 20756,\n",
      "        28725, 26575,  2270,  6833,    13,    13,  2946,  1430,  5541, 28725,\n",
      "          478, 28742,   584,   712,   283,   395,   319, 12553,    13, 15265,\n",
      "         2016,   354,   272,  2039, 28725,   354,   544,   298,  1032,    13,\n",
      "        15265,  8773, 28725, 10739,  6107,   304,  1933,    13,  2565,  5783,\n",
      "        28725,   813, 11115,    13,    13,  1209, 18235, 28747,    13, 14724,\n",
      "         1418,  5783, 28725,   395,  4813,  1503,  1659,    13,  2324, 28742,\n",
      "          584,   379, 10376,   544, 28725,   395,   544,   813,  3992,    13,\n",
      "         2946, 17758,   302,   305,   594, 28725,   297,   813,  7739,    13,\n",
      "        15265, 20756, 28725, 26575,  2270,  6833, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 25 ----------------------\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,    13,  5816,\n",
      "          460,  1287,  1598, 25773, 13784, 13464, 14666,  1401,   272,  1526,\n",
      "        28725,   579,   378,   682,  3289,   356,   970,   368,   460,  5651,\n",
      "        28723,  2993, 28725,   741,  4387,  9926, 20496,   369,  7031, 25773,\n",
      "        13784, 13464,  4014, 26704,  3024, 28747,    13,    13, 28740, 28723,\n",
      "          367, 28723, 28765, 28723,   689,   602, 28742, 28713,    13, 28750,\n",
      "        28723,   394,   357,   314,  2786,    13, 28770, 28723,   418,  2567,\n",
      "        28742, 28713,    13, 28781, 28723,   689,   336,  8328,    13, 28782,\n",
      "        28723,  1972, 28709,    13, 28784, 28723,   542,  1585, 16722,    13,\n",
      "        28787, 28723, 14859,  2238,  1337,  2015,    13,    13,  3381,   368,\n",
      "          460,  2526,   354,   264,   680, 18250,   442,  4337, 28733,   416,\n",
      "        14129,  2659, 28725,   368,  1659,   947,   298,  1464,   741,  1862,\n",
      "          442,  7126,  2877, 28723,   330,  1179,  1069,   298,  1300, 18892,\n",
      "          349,   298,  1460, 23237,   442,  3472,  3270,   354, 10237, 28723,\n",
      "          995,   541,   835,  1877,   575,  2887, 27855,   442,  4530, 25146,\n",
      "          354, 18892,   356, 14666,   298,  1464, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 26 ----------------------\n",
      "tensor([  298,  4098,   652,   771,   304,  8248,   380,   395,   624,  1698,\n",
      "          297,  4342,   369,   682,   506,   750,  8313,   776,   264,  1664,\n",
      "        10073,  3584, 28723,  7575,  4077, 14926,   737,  2622,  8180, 28725,\n",
      "         6297,  1575, 28725,   304,  1140,  6806,  1914, 10611,   298, 11711,\n",
      "          652,   771,   304,  5789,   395,  8504,   356,   264,  3526,  5657,\n",
      "        28723, 13770, 10466,   771,   303,   697, 28725,   442,   281,  6195,\n",
      "        28725,   506, 11029,  1332,   272,  1069,   369,  3427,   349,  6763,\n",
      "        28725,  9836, 10611,   298,  2231,   304, 13313,  6212,  7258,   297,\n",
      "         4342,   369,   654,  2327,  8313,  1671,  9212,  9659,  7108, 28723,\n",
      "           13,    13,   270,   272,  1348,   727, 28725,   736,   349,  1309,\n",
      "         1545,  4139,  6954,   522,   684,   272,  2105, 28733,   532, 28733,\n",
      "         1797, 14983,   369,   506,  1043,   750,   264,  7224,  3325,   302,\n",
      "          272, 20925,  1759, 28723,  6025, 21895,  1309,  5273,   298,   771,\n",
      "          297,  5277,  1362,  2806, 28725,  1413,  7062, 17937,   304,  7108,\n",
      "        28725,   304,   298,  8248,   380,   395,   799, 21895,   297,  1338,\n",
      "        28723,    13,    13,   517,  9807, 28725,  5514,   659,  4923,   582,\n",
      "          633, 18053,   354, 20925, 15609,   304,  8877, 28725,   562,   378,\n",
      "          659,   459,  8939,   272,   927,   354,   272, 10698,   302,  3327,\n",
      "        12284,   304,  9811,   439, 17330,   369,   506,  1743,   750,   438,\n",
      "          272,  3031,   302,   272, 20925,  1759, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 27 ----------------------\n",
      "tensor([20497,  8044,   302,   272,  4229,  9689,   302,   272, 26678, 28723,\n",
      "          415,  5998,   302,   456,  2533,   403,   264,  5864,  2120, 18557,\n",
      "          297,   272,  4747, 28728,   473,  3092,  1815, 28713,  8280,   298,\n",
      "        18018,   272,  1565,  2764,   356,   272, 26678,   354,  3437, 16806,\n",
      "        28723,    13,    13,  1014,  5998,   302,   272,  2533,   403,  1269,\n",
      "         2572,   486,   264, 21088,  8704,  1444,   272,  4747, 28728,   473,\n",
      "         3092, 28723,  1015,   272, 12952, 28725,   690,  6740,   264, 10782,\n",
      "          354,   272, 16176,   352,   304,  5411,   302,   272,  2533, 28723,\n",
      "          415,  8704,  5532, 22321,   354,   272, 27243,   304, 12364,   302,\n",
      "          272,  4229, 20878, 28725,  8147,   302,   948,   602,  2092,  7018,\n",
      "        28725,   304,  4099,   302, 14165,   304,   937,   267,  1249,  7034,\n",
      "          369,  1914, 12085,   298,  2659,   272,  4229,  9689,   302,   272,\n",
      "         2698, 28723,    13,    13,  3260,  5998,   403,  5864,  1096,   378,\n",
      "        10727,   272, 15151,   302,   272,  4747, 28728,   473,  3092,  1815,\n",
      "        28713,  5541,   298, 18018,   304,  5321, 28705, 28782, 28734, 28725,\n",
      "        28734, 28734, 28734, 20647,   302,   272, 28705, 28774, 28770, 28725,\n",
      "        28734, 28734, 28734, 28733, 20383, 26678, 28723,   661,   835,  3956,\n",
      "         2330,   369,   272,  1565,  2764,  5020,   302,   272, 26678,   682,\n",
      "          347,  4443,   477,  4099, 28725, 16176,   288,   264, 12302,  4229,\n",
      "         3715,   354,  3437, 16806,   298,  3555, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 28 ----------------------\n",
      "tensor([  910,   630,   754, 28717,   433,   559,  4813,   302,  7719,   486,\n",
      "          312, 28733,   898,  6133,   378,   390,   264,  4892,   744,   302,\n",
      "          272,  5168,  1759, 28725,   304,   486, 18319,   356,   272,  1722,\n",
      "          630,   829,  2602,   325, 18210,   390,   559,  1216,  6768,   304,\n",
      "        27044,   609, 28705, 28782, 28723,  6389, 21174,   394,  8879,  6439,\n",
      "        28747,  6389, 21174,   394,  8879,  6439,   754, 28717,   433,   264,\n",
      "        14361, 13159,   304,  9696,   808, 19076,   298,  2727,   624,   302,\n",
      "          272,  1080,  6053,  4077,  3327,  1218,   302,   544,   727, 28723,\n",
      "          985,   659, 14382,   684,   910,   630, 17159,   601,   264,  6057,\n",
      "         2273,   673,   486, 18319,   356,  5168,   304,  1008, 28733,   321,\n",
      "          729, 10894, 28725,   304,   486, 12028,  4746,   395,  5278,   304,\n",
      "        27729,   905, 28723,   394,  8879,  6439,   835, 10574,  1332,   272,\n",
      "         9545,   302,  3344, 13507, 14679,   304,  1250,  7813,   298,  3747,\n",
      "          297,  1745,   298,  6619,  1598,  1722, 28723,  2957,  6053,  6992,\n",
      "          544,   553,  1581, 13945,   298,   754,  4343,   652,  4813,   302,\n",
      "         7719,   304,  3344, 13507, 14679, 28725,   562,   590,  6431,   264,\n",
      "         1664,  1945, 15559, 28747,   264,  7813,  1467,   298,  2822,   477,\n",
      "        17179, 28725,   264,  3232,   356,  1008, 28733,   321,   729, 10894,\n",
      "          304,  6057, 28725,   304,   264,  3534,  3367,   302,  8773,   304,\n",
      "         6032,   369,  3313,  6099,   706,  3814, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 29 ----------------------\n",
      "tensor([28723,    13,    13, 28770, 28723,   451, 28727,   895,   387,   451,\n",
      "        28727,   895,  2870,  7455,  5982, 27987,   395, 27810,   369,  3508,\n",
      "         3031,  4338,   304, 21058,  6157, 28723,   415,  1178,   349,  2662,\n",
      "          298,   264,  7455,  6690,   954, 28725,   690,   389,  9916,  4386,\n",
      "          513,  2424,  3969,   534,  8027, 28723,    13,    13, 28781, 28723,\n",
      "        17285, 21329, 19649,   387, 17285, 21329, 19649,  2870,  7455, 13278,\n",
      "          395, 27810,   369,  3508, 12588, 10090,   737,  3031,  4338, 28725,\n",
      "        14232, 28725,   304,  2187,  7641, 28723,   415,  1178,   349,  2662,\n",
      "          298,   264,  7455,  6690,   954, 28725,   690,  5312,  1353, 28733,\n",
      "         1536, 12139,   356,  7544,  2528,   304,  1162,  1467, 28723,    13,\n",
      "           13, 28782, 28723,   650, 28744,   385,  7510,   387,   650, 28744,\n",
      "          385,  7510,  2870,  7455,   480,  9338,   395, 27810,   369,  3508,\n",
      "        12588, 10090,   737,  3031,  4338, 28725, 14232, 28725,   304,  6355,\n",
      "         6157, 28723,   415,  1178,   349,  2662,   298,   264,  7455,  6690,\n",
      "          954, 28725,   690,  5312, 10537, 20715,   778,  7544,  2528,   304,\n",
      "         1162,  1467, 28723,    13,    13, 18171,   460,   776,   264,  1664,\n",
      "         9254,   302,   272,  1287,  4799,   304,  4076,   369,   460,  5489,\n",
      "        28464, 20007, 27810,   297, 13278, 28723,  1136,   272,  5514, 10352,\n",
      "          298,  1950, 28725,   478,   541,  1675,   298,  1032,   680,   304,\n",
      "          680, 16827,  8429,   297,   456,  2764, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 30 ----------------------\n",
      "tensor([28725,  1854,   315,   325, 28740, 28731,   349,  6838,  1024,   550,\n",
      "          325, 28782, 28731,   304,  3886,   298,   378, 28723,    13, 28733,\n",
      "         9738, 28725,  6084,  3069,   541,   347,  4429,   582,   486,  5683,\n",
      "         1077,  7000,  3069, 28723,  1263,  2757, 28725, 21027, 10651,   272,\n",
      "         1474, 28705, 28770, 28734, 28725,  1854,  1500,   325, 28740, 28734,\n",
      "        28731,   349,  9332,  1712,  2421, 28723,    13,    13,  1014,  2611,\n",
      "         2079,  2552,  5551,   460, 10483,   395,  2948, 26350,   302,  9181,\n",
      "          349, 12282, 10578, 28723,   415, 26081,   302,   272,  8325,  5539,\n",
      "          282,  1587,   460,   521,  6206, 28725,   562,   378,   349,  1654,\n",
      "          298,   506,  6202,   477,   272,   413,   434,   381,  4230,  5539,\n",
      "          282,  1587, 28723,  5235,   727, 28725,   272,  1587,   403,  1792,\n",
      "         1311,   304, 14708, 28725,   395,   633, 16212,  1250,  3886,   354,\n",
      "         6084,  5551, 28723, 28705,    13,    13,   657,  2952, 28725,   272,\n",
      "         8325,  5539,   282,  1587,   349,  2108,  9096,   821,   272, 25888,\n",
      "         1587,   478,   938,  3154, 28725,  1854,   378,  6948,   680, 16212,\n",
      "          298,  2904,   272,  1348,  2819,   302,  5551, 28723,  2993, 28725,\n",
      "          378,  7520,   264,  4387,   304,  4169, 11552,  1069,   302, 14030,\n",
      "         5551, 28725,   304,   871,   938,   349,  1309,  1419,   297,  1287,\n",
      "         5020,   302,  1411, 28725,  1259,   390,  9917, 10190, 28725,  1820,\n",
      "        25310, 28725,   304,  5994,  5331,  1190, 28723,  -100, 28705,    13])\n",
      "\n",
      "---------------------- ROW IDX = 31 ----------------------\n",
      "tensor([  356,   272,  8147,   302,  2533,   304,  2130,  5823,   298, 11711,\n",
      "         7783, 27374, 28713,   304, 28043,  6189,  1449, 28723,  5400,  8719,\n",
      "          263,  8812,  3024,  8005, 12364, 28725, 28606,   495,  7018, 17078,\n",
      "        28725,   304, 16882,   302,  1862, 28043, 19439, 28723, 28705, 28781,\n",
      "        28723,  9553,  1054, 28709,   385, 28747,  6025, 20372,   385,   297,\n",
      "          574,  2698,  2405, 21311,  8812,  9045,   356,  8527,  1656,   304,\n",
      "         5362, 28723,  5400,  8719,   404,   541,  1316,   395,  9796,   737,\n",
      "        16486,  8527,   890,  1468, 28725, 11906,  8527,   481, 10413,  1238,\n",
      "        28725,   304,  5374, 23274,   442, 14165,  7034, 28723, 28705, 28782,\n",
      "        28723, 16155, 27454,   282, 16011, 18015, 28747,  2957,  9909,  3084,\n",
      "         1656,   304,   312, 23005,  5174,   354, 15714, 22983, 27454,   973,\n",
      "          737,  6163,   261,  3525,   867, 28725,   427,   973, 28725,   304,\n",
      "        13824,   721,  1126, 28723,  5400,  8719,   263,  8812,  3024,  8527,\n",
      "         1656, 28725, 12671, 12364, 28725,   304, 14165,   575, 18932, 28723,\n",
      "         2957,   460,   776,   741,  9254,   302,   272,  4514,   302,  8527,\n",
      "        21088,  9909,   368,  1659,  1300,   297,   574,  1862,  2698, 28723,\n",
      "         3995,  2570,   356,   574,  2948, 10299, 28725,   368,  1659,  1300,\n",
      "         8812,   298, 21311,   395,   799,  4514,   302,  9909, 28725,  2490,\n",
      "         1395,  9045,   356,   948,   602,  2092,  7018, 28725,  8527, 21607,\n",
      "        28725,   442,  8527, 11881,  2426,  8280, 28723,  -100, 28705,    13])\n"
     ]
    }
   ],
   "source": [
    "labels = next(iter(dataloader.dataloader))[\"labels\"]\n",
    "\n",
    "for row_idx in range(labels.shape[0]):\n",
    "    print(f\"\\n---------------------- ROW IDX = {row_idx} ----------------------\")\n",
    "    print(labels[row_idx,-200:])\n",
    "\n",
    "    # for i in range(20):\n",
    "    #     print(f\"i={i}: {labels[row_idx,i*100:(i+1)*100]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
