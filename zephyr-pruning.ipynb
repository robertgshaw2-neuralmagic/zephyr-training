{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ./sparseml[transformers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92214c8c78b643279abd69cf61962a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"HuggingFaceH4/mistral-7b-sft-beta\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train_sft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "def add_system_prompt(batch):\n",
    "    system_prompt = {\n",
    "        \"content\": \"You are a friendly chatbot\",\n",
    "        \"role\": \"system\"\n",
    "    }\n",
    "\n",
    "    updated_messages = []\n",
    "    for element in batch[\"messages\"]:\n",
    "        updated_messages.append([system_prompt] + element)\n",
    "\n",
    "    return {\"messages_with_sys_prompt\": updated_messages}\n",
    "\n",
    "dataset = dataset.map(\n",
    "    add_system_prompt,\n",
    "    batched=True,\n",
    "    num_proc=32,\n",
    "    batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from typing import List, Union, Any, Dict\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "class DataCollatorForChatLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(\n",
    "        self,\n",
    "        response_template: List[int],\n",
    "        instruction_template: List[int],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, mlm=False, **kwargs)\n",
    "\n",
    "        self.instruction_token_ids = instruction_template\n",
    "        self.response_token_ids = response_template\n",
    "        self.ignore_index = -100\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "            response_token_ids_idxs = []\n",
    "            human_token_ids_idxs = []\n",
    "\n",
    "            for assistant_idx in np.where(batch[\"labels\"][i] == self.response_token_ids[0])[0]:\n",
    "                # find the indexes of the start of a response.\n",
    "                if (\n",
    "                    self.response_token_ids\n",
    "                    == batch[\"labels\"][i][assistant_idx : assistant_idx + len(self.response_token_ids)].tolist()\n",
    "                ):\n",
    "                    response_token_ids_idxs.append(assistant_idx + len(self.response_token_ids))\n",
    "\n",
    "            if len(response_token_ids_idxs) == 0:\n",
    "                warnings.warn(\n",
    "                    f\"Could not find response key `{self.response_token_ids}` in the \"\n",
    "                    f'following instance: {self.tokenizer.decode(batch[\"input_ids\"][i])} '\n",
    "                    f\"This instance will be ignored in loss calculation. \"\n",
    "                    f\"Note, if this happens often, consider increasing the `max_seq_length`.\"\n",
    "                )\n",
    "                batch[\"labels\"][i, :] = self.ignore_index\n",
    "\n",
    "            human_token_ids = self.instruction_token_ids\n",
    "            for human_idx in np.where(batch[\"labels\"][i] == human_token_ids[0])[0]:\n",
    "                # find the indexes of the start of a human answer.\n",
    "                if human_token_ids == batch[\"labels\"][i][human_idx : human_idx + len(human_token_ids)].tolist():\n",
    "                    human_token_ids_idxs.append(human_idx)\n",
    "\n",
    "            if len(human_token_ids_idxs) == 0:\n",
    "                warnings.warn(\n",
    "                    f\"Could not find instruction key `{self.instruction_token_ids}` in the \"\n",
    "                    f'following instance: {self.tokenizer.decode(batch[\"input_ids\"][i])} '\n",
    "                    f\"This instance will be ignored in loss calculation. \"\n",
    "                    f\"Note, if this happens often, consider increasing the `max_seq_length`.\"\n",
    "                )\n",
    "                batch[\"labels\"][i, :] = self.ignore_index\n",
    "\n",
    "            for idx, (start, end) in enumerate(zip(human_token_ids_idxs, response_token_ids_idxs)):\n",
    "                print(f\"start = {start} / end = {end}\")\n",
    "                assert start < end\n",
    "                # Make pytorch loss function ignore all non response tokens\n",
    "                if idx != 0:\n",
    "                    batch[\"labels\"][i, start:end] = self.ignore_index\n",
    "                else:\n",
    "                    batch[\"labels\"][i, :end] = self.ignore_index\n",
    "            \n",
    "            # assert follows user // assistant back + forth with equal number of query // response pairs\n",
    "            assert len(response_token_ids_idxs) == len(human_token_ids_idxs)\n",
    "            print(human_token_ids_idxs[0])\n",
    "            # mask out everything before the first user prompt (i.e. the system prompt)\n",
    "            batch[\"labels\"][i, :human_token_ids_idxs[0]] = self.ignore_index\n",
    "            \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28789, 28766, 489, 11143, 28766, 28767]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|assistant|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template_ids = tokenizer.encode(\"/n<|assistant|>\", add_special_tokens=False)[2:]\n",
    "print(response_template_ids)\n",
    "tokenizer.decode(response_template_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28789, 28766, 1838, 28766, 28767]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|user|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_template_ids = tokenizer.encode(\"/n<|user|>\", add_special_tokens=False)[2:]\n",
    "print(instruction_template_ids)\n",
    "tokenizer.decode(instruction_template_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# collator = DataCollatorForChatLM(\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template_ids,\n",
    "    instruction_template=instruction_template_ids,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "def apply_chat_template(tokenizer, messages_col, batch):\n",
    "    strs = []\n",
    "    for example in batch[messages_col]:\n",
    "        strs.append(tokenizer.apply_chat_template(example, tokenize=False))\n",
    "\n",
    "    return strs\n",
    "\n",
    "chat_formatting_func = partial(apply_chat_template, tokenizer, \"messages_with_sys_prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255170e3f66046408b62d62c3b85e293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/rshaw/zephyr-pruning/.venv/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    chat_format = chat_formatting_func(element)\n",
    "\n",
    "    outputs = tokenizer(\n",
    "        chat_format,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=2048,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "    )\n",
    "\n",
    "    return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"], \"chat_format\": chat_format}\n",
    "\n",
    "tokenized_dataset = dataset.select(range(10000)).map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    num_proc=32,\n",
    "    batch_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_removed = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col not in [\"input_ids\", \"attention_mask\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": 1,\n",
    "    \"collate_fn\": collator,\n",
    "}\n",
    "\n",
    "dataloader = DataLoader(tokenized_dataset_removed, **dataloader_params)\n",
    "\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,   523, 28766,  6574, 28766, 28767,    13,  1976,   460,   264,\n",
      "        10131, 10706, 10093,     2, 28705,    13, 28789, 28766,  1838, 28766,\n",
      "        28767,    13, 18171, 11382,  5580,   298,  4211, 28733,  5527, 18978,\n",
      "          325,  1146, 13532,   495, 28705, 28784, 28723, 28734, 28806, 28725,\n",
      "         8337,  1380, 28705, 28781, 28723, 28734, 28806, 28725,  2316,   455,\n",
      "          897, 28705, 28770, 28723, 28734, 28806,  6372,  1798, 28705, 28750,\n",
      "        28723, 28734, 28806, 28725,   351,   598, 16712, 28705, 28782, 28723,\n",
      "        28734, 28806,   609,  1824,  7335,  2751,   837,   315,  1413, 28804,\n",
      "           13,  2486,   574, 27395,  6718,   567, 22114, 28715, 27395, 12458,\n",
      "        28725,   368,   541,  5061,  1347,   272, 13461,  3469,   302,   264])\n",
      "<|system|>\n",
      "You are a friendly chatbot</s>\n",
      "<|user|>\n",
      "These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n",
      "On your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\n",
      "Your Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\n",
      "Does this feature apply to all sections of the theme or just specific ones as listed in the text material?</s>\n",
      "<|assistant|>\n",
      "This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.</s>\n",
      "<|user|>\n",
      "Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?</s>\n",
      "<|assistant|>\n",
      "Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n",
      "\n",
      "1. Log in to your Shopify account and go to your Online Store.\n",
      "2. Click on Customize theme for the section-based theme you are using.\n",
      "3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n",
      "4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n",
      "5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n",
      "6. If available, select 'Show secondary image on hover'.\n",
      "7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n",
      "\n",
      "If you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.</s>\n",
      "<|user|>\n",
      "Can you provide me with a link to the documentation for my theme?</s>\n",
      "<|assistant|>\n",
      "I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.</s>\n",
      "<|user|>\n",
      "Can you confirm if this feature also works for the Quick Shop section of my theme?</s>\n",
      "<|assistant|>\n",
      "The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n",
      "\n",
      "1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.tensor(tokenized_dataset[0][\"input_ids\"][:100]))\n",
    "print(tokenized_dataset[0][\"chat_format\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 28705, 13, 28789, 28766, 489, 11143, 28766, 28767, 13, 3260, 4480, 865, 15588, 298, 13079, 6718, 304, 22114, 28715, 27395, 12458, 302, 272, 4211, 28733, 5527, 18978, 9206, 297, 272, 2245, 3388, 28723]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"\\n<|assistant|>\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 28705, 13, 3260, 4480, 865, 15588, 298, 13079, 6718]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"\\nThis feature only applies to Collection pages\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.</s>'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([13,  3260,  4480,   865, 15588,   298, 13079,  6718,   304, 22114, 28715, 27395, 12458,   302,   272,  4211, 28733,  5527, 18978,  9206,   297,   272,  2245,  3388, 28723,     2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch[\"labels\"][0]\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    if label == -100:\n",
    "        labels[idx]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot</s>\n",
      "<|user|>\n",
      "These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n",
      "On your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\n",
      "Your Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\n",
      "Does this feature apply to all sections of the theme or just specific ones as listed in the text material?</s>\n",
      "<|assistant|>\n",
      "This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.</s>\n",
      "<|user|>\n",
      "Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?</s>\n",
      "<|assistant|>\n",
      "Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n",
      "\n",
      "1. Log in to your Shopify account and go to your Online Store.\n",
      "2. Click on Customize theme for the section-based theme you are using.\n",
      "3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n",
      "4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n",
      "5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n",
      "6. If available, select 'Show secondary image on hover'.\n",
      "7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n",
      "\n",
      "If you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.</s>\n",
      "<|user|>\n",
      "Can you provide me with a link to the documentation for my theme?</s>\n",
      "<|assistant|>\n",
      "I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.</s>\n",
      "<|user|>\n",
      "Can you confirm if this feature also works for the Quick Shop section of my theme?</s>\n",
      "<|assistant|>\n",
      "The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n",
      "\n",
      "1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0][\"chat_format\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<s> \n",
      "<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n",
      "\n",
      "1. Log in to your Shopify account and go to your Online Store.\n",
      "2. Click on Customize theme for the section-based theme you are using.\n",
      "3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n",
      "4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n",
      "5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n",
      "6. If available, select 'Show secondary image on hover'.\n",
      "7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n",
      "\n",
      "If you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<s> \n",
      "<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<s> \n",
      "<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n",
      "\n",
      "1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<s> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eef958e8e346bc88037d2496ca1606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 15.74 GiB total capacity; 0 bytes already allocated; 201.19 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/rshaw/zephyr-pruning/zephyr-pruning.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bproduct/home/rshaw/zephyr-pruning/zephyr-pruning.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrl\u001b[39;00m \u001b[39mimport\u001b[39;00m SFTTrainer\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bproduct/home/rshaw/zephyr-pruning/zephyr-pruning.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m sft_trainer \u001b[39m=\u001b[39m SFTTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bproduct/home/rshaw/zephyr-pruning/zephyr-pruning.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bproduct/home/rshaw/zephyr-pruning/zephyr-pruning.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mselect(\u001b[39mrange\u001b[39;49m(\u001b[39m10000\u001b[39;49m)),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bproduct/home/rshaw/zephyr-pruning/zephyr-pruning.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     data_collator\u001b[39m=\u001b[39;49mcollator,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bproduct/home/rshaw/zephyr-pruning/zephyr-pruning.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     formatting_func\u001b[39m=\u001b[39;49mchat_formatting_func,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bproduct/home/rshaw/zephyr-pruning/zephyr-pruning.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     max_seq_length\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bproduct/home/rshaw/zephyr-pruning/zephyr-pruning.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[0;32m~/zephyr-pruning/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:219\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m tokenizer\u001b[39m.\u001b[39mpadding_side \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m tokenizer\u001b[39m.\u001b[39mpadding_side \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mright\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moverflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = \u001b[39m\u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` to your code.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m     )\n\u001b[0;32m--> 219\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    220\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    221\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    222\u001b[0m     data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[1;32m    223\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset,\n\u001b[1;32m    224\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49meval_dataset,\n\u001b[1;32m    225\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m    226\u001b[0m     model_init\u001b[39m=\u001b[39;49mmodel_init,\n\u001b[1;32m    227\u001b[0m     compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,\n\u001b[1;32m    228\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    229\u001b[0m     optimizers\u001b[39m=\u001b[39;49moptimizers,\n\u001b[1;32m    230\u001b[0m     preprocess_logits_for_metrics\u001b[39m=\u001b[39;49mpreprocess_logits_for_metrics,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmax_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m packing:\n\u001b[1;32m    234\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    235\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m     )\n",
      "File \u001b[0;32m~/zephyr-pruning/.venv/lib/python3.10/site-packages/transformers/trainer.py:481\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[39m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    478\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_model_on_device\n\u001b[1;32m    479\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mquantization_method\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    480\u001b[0m ):\n\u001b[0;32m--> 481\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_move_model_to_device(model, args\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    483\u001b[0m \u001b[39m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/zephyr-pruning/.venv/lib/python3.10/site-packages/transformers/trainer.py:716\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_move_model_to_device\u001b[39m(\u001b[39mself\u001b[39m, model, device):\n\u001b[0;32m--> 716\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    717\u001b[0m     \u001b[39m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mparallel_mode \u001b[39m==\u001b[39m ParallelMode\u001b[39m.\u001b[39mTPU \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mtie_weights\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/zephyr-pruning/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m     \u001b[39mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2267\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2268\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2269\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m         )\n\u001b[0;32m-> 2271\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/zephyr-pruning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/zephyr-pruning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/zephyr-pruning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/zephyr-pruning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/zephyr-pruning/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 15.74 GiB total capacity; 0 bytes already allocated; 201.19 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset.select(range(10000)),\n",
    "    data_collator=collator,\n",
    "    formatting_func=chat_formatting_func,\n",
    "    max_seq_length=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start = 1648 / end = 2186\n",
      "start = 2263 / end = 2322\n",
      "start = 2442 / end = 2510\n",
      "start = 2659 / end = 2715\n",
      "start = 16 / end = 47\n",
      "start = 852 / end = 901\n",
      "start = 1498 / end = 1563\n",
      "start = 2142 / end = 2211\n",
      "start = 1524 / end = 2246\n",
      "start = 2379 / end = 2426\n",
      "start = 2610 / end = 2642\n",
      "start = 1482 / end = 1574\n",
      "start = 2581 / end = 2647\n",
      "start = 910 / end = 994\n",
      "start = 1712 / end = 1753\n",
      "start = 2044 / end = 2092\n",
      "start = 2384 / end = 2448\n",
      "start = 2008 / end = 2091\n",
      "start = 2542 / end = 2575\n",
      "start = 1882 / end = 2389\n",
      "start = 2520 / end = 2558\n",
      "start = 2699 / end = 2748\n",
      "start = 1077 / end = 1171\n",
      "start = 1761 / end = 1818\n",
      "start = 2098 / end = 2141\n",
      "start = 2430 / end = 2487\n",
      "start = 828 / end = 855\n",
      "start = 1711 / end = 1749\n",
      "start = 2030 / end = 2099\n",
      "start = 1423 / end = 1567\n",
      "start = 2223 / end = 2266\n",
      "start = 2416 / end = 2471\n",
      "start = 1621 / end = 1696\n",
      "start = 1858 / end = 1920\n",
      "start = 2118 / end = 2179\n",
      "start = 2465 / end = 2535\n",
      "start = 1043 / end = 1099\n",
      "start = 1371 / end = 1408\n",
      "start = 1749 / end = 1791\n",
      "start = 2078 / end = 2122\n",
      "start = 2433 / end = 2475\n",
      "start = 1170 / end = 1264\n",
      "start = 1610 / end = 1648\n",
      "start = 2025 / end = 2063\n",
      "start = 2410 / end = 2455\n",
      "start = 460 / end = 616\n",
      "start = 1741 / end = 1798\n",
      "start = 1649 / end = 2106\n",
      "start = 2225 / end = 2276\n",
      "start = 2385 / end = 2436\n",
      "start = 2589 / end = 2638\n",
      "start = 1945 / end = 2138\n",
      "start = 2574 / end = 2604\n",
      "start = 1616 / end = 1654\n",
      "start = 2010 / end = 2059\n",
      "start = 2371 / end = 2416\n",
      "start = 2615 / end = 2666\n",
      "start = 1947 / end = 2045\n",
      "start = 2213 / end = 2272\n",
      "start = 2527 / end = 2576\n",
      "start = 1261 / end = 1295\n",
      "start = 1840 / end = 1887\n",
      "start = 2332 / end = 2382\n",
      "start = 1171 / end = 1207\n",
      "start = 1547 / end = 1592\n",
      "start = 2058 / end = 2098\n",
      "start = 2443 / end = 2494\n",
      "start = 2167 / end = 2396\n",
      "start = 2488 / end = 2517\n",
      "start = 1895 / end = 1924\n",
      "start = 2463 / end = 2513\n",
      "start = 1034 / end = 1126\n",
      "start = 1762 / end = 1815\n",
      "start = 2035 / end = 2095\n",
      "start = 2095 / end = 2231\n",
      "start = 2272 / end = 2340\n",
      "start = 2596 / end = 2646\n",
      "start = 2003 / end = 2392\n",
      "start = 2577 / end = 2639\n",
      "start = 2084 / end = 2121\n",
      "start = 2403 / end = 2443\n",
      "start = 2519 / end = 2574\n",
      "start = 1511 / end = 1606\n",
      "start = 1856 / end = 1894\n",
      "start = 2147 / end = 2209\n",
      "start = 2499 / end = 2563\n",
      "start = 1775 / end = 1830\n",
      "start = 2420 / end = 2474\n",
      "start = 1748 / end = 1848\n",
      "start = 2450 / end = 2494\n",
      "start = 2415 / end = 2549\n",
      "start = 2593 / end = 2632\n",
      "start = 219 / end = 384\n",
      "start = 1546 / end = 1605\n",
      "start = 2102 / end = 2162\n",
      "start = 2327 / end = 2419\n",
      "start = 2610 / end = 2673\n",
      "start = 828 / end = 972\n",
      "start = 1826 / end = 1873\n",
      "start = 2506 / end = 2565\n",
      "start = 1572 / end = 2073\n",
      "start = 2233 / end = 2281\n",
      "start = 2437 / end = 2509\n",
      "start = 2771 / end = 2810\n",
      "start = 1749 / end = 1839\n",
      "start = 2067 / end = 2122\n",
      "start = 2297 / end = 2345\n",
      "start = 2507 / end = 2571\n",
      "start = 327 / end = 439\n",
      "start = 1381 / end = 1420\n",
      "start = 2543 / end = 2584\n",
      "start = 930 / end = 1035\n",
      "start = 2078 / end = 2127\n",
      "start = 2596 / end = 2648\n",
      "start = 1559 / end = 1679\n",
      "start = 2615 / end = 2669\n",
      "start = 1180 / end = 1217\n",
      "start = 1522 / end = 1563\n",
      "start = 1842 / end = 1878\n",
      "start = 2158 / end = 2197\n",
      "start = 2252 / end = 2293\n",
      "start = 2565 / end = 2612\n",
      "start = 2939 / end = 3006\n",
      "start = 2153 / end = 2210\n",
      "start = 2236 / end = 2275\n",
      "start = 2467 / end = 2504\n",
      "start = 2768 / end = 2802\n",
      "start = 1211 / end = 1286\n",
      "start = 1775 / end = 1818\n",
      "start = 2159 / end = 2208\n",
      "start = 2588 / end = 2629\n",
      "start = 1449 / end = 1659\n",
      "start = 2078 / end = 2120\n",
      "start = 2613 / end = 2651\n",
      "start = 1701 / end = 1733\n",
      "start = 2029 / end = 2070\n",
      "start = 2399 / end = 2427\n",
      "start = 2742 / end = 2780\n",
      "start = 1501 / end = 1601\n",
      "start = 1908 / end = 1968\n",
      "start = 2353 / end = 2393\n",
      "start = 2680 / end = 2714\n",
      "start = 1502 / end = 1614\n",
      "start = 2111 / end = 2174\n",
      "start = 2590 / end = 2659\n",
      "start = 1629 / end = 2537\n",
      "start = 2647 / end = 2682\n",
      "start = 2793 / end = 2830\n",
      "start = 1553 / end = 1642\n",
      "start = 2124 / end = 2176\n",
      "start = 2359 / end = 2403\n",
      "start = 2677 / end = 2741\n",
      "start = 16 / end = 124\n",
      "start = 886 / end = 946\n",
      "start = 1641 / end = 1684\n",
      "start = 2300 / end = 2358\n",
      "start = 2827 / end = 2930\n",
      "start = 2996 / end = 3039\n",
      "start = 2821 / end = 2909\n",
      "start = 2935 / end = 2974\n",
      "start = 1717 / end = 1757\n",
      "start = 2160 / end = 2214\n",
      "start = 2638 / end = 2691\n",
      "start = 2586 / end = 2673\n",
      "start = 2835 / end = 2877\n",
      "start = 2418 / end = 2788\n",
      "start = 2923 / end = 2957\n",
      "start = 2489 / end = 2739\n",
      "start = 2768 / end = 2811\n",
      "start = 2892 / end = 2924\n",
      "start = 2948 / end = 2980\n",
      "start = 1648 / end = 1677\n",
      "start = 2060 / end = 2098\n",
      "start = 2470 / end = 2512\n",
      "start = 2800 / end = 2848\n",
      "start = 1434 / end = 2397\n",
      "start = 2793 / end = 2838\n",
      "start = 2253 / end = 2298\n",
      "start = 2748 / end = 2810\n",
      "start = 2464 / end = 2701\n",
      "start = 2747 / end = 2780\n",
      "start = 2829 / end = 2860\n",
      "start = 2961 / end = 2996\n",
      "start = 2447 / end = 2884\n",
      "start = 2949 / end = 2983\n",
      "start = 3013 / end = 3050\n",
      "start = 2337 / end = 2492\n",
      "start = 2909 / end = 2942\n",
      "start = 2390 / end = 2584\n",
      "start = 2649 / end = 2687\n",
      "start = 2962 / end = 3003\n",
      "start = 523 / end = 684\n",
      "start = 1343 / end = 1433\n",
      "start = 1914 / end = 2005\n",
      "start = 2454 / end = 2565\n",
      "start = 1569 / end = 1659\n",
      "start = 2139 / end = 2172\n",
      "start = 2574 / end = 2629\n",
      "start = 2351 / end = 2415\n",
      "start = 2864 / end = 2920\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(sft_trainer.get_train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100], device='cuda:0')\n",
      "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100], device='cuda:0')\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,    13, 13617,   404,   356,\n",
      "         6081,  7303,  5801,  9381, 28742, 28713,  3187, 28765,   915,  7121,\n",
      "         2884,   506, 26812,   559,  2808,  1771,   297,  5587,   582,   272,\n",
      "         3360,   520,  7242, 28723,  5801,  9381,   659,  9256,   369,   378,\n",
      "         7033,  1074,   559,   298,  5556, 17584,   304,   369,   630, 11638,\n",
      "        28742, 28707, 12527,   707,   302,   272,  2445,  6333, 28723,   985,\n",
      "          659,   835,  2261, 12085,   298,   559,  2884,   298,  1917,   949,\n",
      "         1077,   298,   799,  6812, 15829,  1059,   272,  6409,  3254, 28723,\n",
      "         -100, 28705,    13,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,    13,  5816,   659,   750,  1560,  1760,   304, 17584,\n",
      "          354,  6081,  7303,  5801,  9381, 28742, 28713,  3187, 28765,   915,\n",
      "         7121,  2884, 28723,  2909,  6992,   506,   750,  4142,   486,   559,\n",
      "         2838,   304,   506,  1350,  5019,   949,   601,   298,   559,  4244,\n",
      "        28725, 10503,   297,   559, 12436,   288,   559,  5541,   302,   429,\n",
      "        28782, 28725, 28734, 28734, 28734,  2373,   264,  1664,  2202, 28723,\n",
      "        19038,   506,  8394,  1332,   559,  5161,   298,  1460,   354,  1316,\n",
      "          304,   506, 26812,   559,  2808,  1771, 28723,  5801,  9381,   659,\n",
      "        15249,   486, 13964,  3080,   272, 17584,   304,  4072,   288, 26485,\n",
      "          354,   272,   949,   697,   630,   659,  3874, 28723,   985,   659,\n",
      "          835,  2261, 12085,   298,   559,  2884,   298,  1917,   949,  1077,\n",
      "          298,   799,  6812, 15829,  1059,   272,  6409,  3254, 28723,  -100,\n",
      "        28705,    13,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "           13, 22387,  7303,  5801,  9381,   659,   750, 11180,   297,  2856,\n",
      "         4342,   486,   272, 10473,  3058,  6409,  3254,  5298,  2461,   298,\n",
      "         6112,   559, 28141,   304,   808,   582,   264,  3187, 28765,   915,\n",
      "         7121,  2884, 28723,  2326,   272,  6409,  3254, 24266,   354,   754,\n",
      "          264,  2102, 28725,  5801,  9381, 28742, 28713,  1432,  8432,  2136,\n",
      "         2888,   403, 11793,   754,   272, 20973, 28725, 13098,   559,   298,\n",
      "        11371,   298,  1038,  9675,  2647, 28723,   985,   659,   835,   553,\n",
      "          298,  3119,   852,   356, 16972,   304,  1658,   805,  4892, 25596,\n",
      "        28725, 27817,   559,   304,   559,  2005, 28742, 28713,  4045,   302,\n",
      "         1411, 28723, 16569, 28725,   272,  6409,  3254,   659,   835, 11180,\n",
      "          559,   771, 28725,   395,   559,  1250,  7207,   298,  1388,   356,\n",
      "          521, 26283,  3530,   390,   264,  1204,   302,   272,   285,  2179,\n",
      "          900, 28713, 28723, 21013, 28725,   272,  6409,  3254,   659, 11117,\n",
      "         5088,   286,  5801,  9381, 28742, 28713,   854,  2233, 28725,  6790,\n",
      "         1411, 28725,   304,  2389,  4908, 28723,  -100, 28705,    13,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,    13, 22387,  7303,  5801,  9381,\n",
      "        28742, 28713,  3618,   659,  6373,   559,  4118,  6967,   302,  1760,\n",
      "         1938,   272, 10473,  3058,  6409,  3254, 28723,  1263,  2757, 28725,\n",
      "          272,  4386,   302,   559, 28705, 28740, 28787, 28733,  4395, 28733,\n",
      "          738,  1966, 28742, 28713,  1918, 15123,   304,  3282,   438,   367,\n",
      "         2594,  7338,  4556,  5907,   354,   272, 22088,   771,   559,  1966,\n",
      "         3236, 28723, 16569, 28725,   272,   798,   659, 15249, 14139,  2260,\n",
      "          298,   559,  3187, 28765,   915,  7121,  5774, 28725,   395,   949,\n",
      "          734, 25885,   754,   429, 28740, 28784, 28725, 28734, 28734, 28734,\n",
      "          298,  1316,   559,   304,   559,  2005,  1059,   456,  3796,   727,\n",
      "        28723, 10191,   741,  7087,  7616,   356,   559,  2884, 28725,   736,\n",
      "          659,   750,   396, 18894,  3558,   302,  1760,  4894,   298,  5801,\n",
      "         9381, 28725,   395,  1287,   905,  1250,  4142,   486,   559,  2838,\n",
      "          304, 12647,   298,  1316,   297,   707,  1069,   590,   541, 28723,\n",
      "         -100, 28705,    13], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"labels\"][0][0:1000])\n",
    "print(batch[\"labels\"][0][1000:2000])\n",
    "print(batch[\"labels\"][0][2000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 523, 28766, 1838, 28766, 28767, 13, 18171, 11382, 5580, 298, 4211, 28733, 5527, 18978, 325, 1146, 13532, 495, 28705, 28784, 28723, 28734, 28806, 28725, 8337, 1380, 28705, 28781, 28723, 28734, 28806, 28725, 2316, 455, 897, 28705, 28770, 28723, 28734, 28806, 6372, 1798, 28705, 28750, 28723, 28734, 28806, 28725, 351, 598, 16712, 28705, 28782, 28723, 28734, 28806, 609, 1824, 7335, 2751, 837, 315, 1413, 28804, 13, 2486, 574, 27395, 6718, 567, 22114, 28715, 27395, 12458, 28725, 368, 541, 5061, 1347, 272, 13461, 3469, 302, 264, 2093, 356, 18848, 486, 25748, 624, 302, 272, 7335, 28742, 28713, 4429, 28733, 262, 6472, 28808, 13, 11159, 13079, 6718, 567, 22114, 28715, 27395, 12458, 622, 1055, 4249, 272, 13461, 2093, 3469, 776, 486, 18848, 288, 754, 369, 2093, 3469, 15762, 21418, 28723, 13, 20510, 456, 4480, 5580, 298, 544, 12458, 302, 272, 7335, 442, 776, 2948, 4413, 390, 9206, 297, 272, 2245, 3388, 28804, 2, 28705, 13, 28789, 28766, 489, 11143, 28766, 28767, 13, 3260, 4480, 865, 15588, 298, 13079, 6718, 304, 22114, 28715, 27395, 12458, 302, 272, 4211, 28733, 5527, 18978, 9206, 297, 272, 2245, 3388, 28723, 2, 28705, 13, 28789, 28766, 1838, 28766, 28767, 13, 6325, 368, 8327, 528, 1059, 272, 1759, 302, 25748, 272, 13461, 3469, 18848, 4480, 356, 586, 13079, 6718, 304, 22114, 28715, 27395, 12458, 28804, 2, 28705, 13, 28789, 28766, 489, 11143, 28766, 28767, 13, 22099, 28725, 1236, 460, 272, 5944, 298, 8234, 272, 13461, 3469, 18848, 4480, 356, 574, 13079, 6718, 304, 22114, 28715, 27395, 12458, 28747, 13, 13, 28740, 28723, 5545, 297, 298, 574, 14142, 1575, 2708, 304, 576, 298, 574, 10634, 13229, 28723, 13, 28750, 28723, 10213, 356, 7973, 653, 7335, 354, 272, 4211, 28733, 5527, 7335, 368, 460, 1413, 28723, 13, 28770, 28723, 9363, 11590, 298, 272, 13079, 6718, 442, 22114, 28715, 27395, 4211, 970, 368, 947, 298, 8234, 272, 13461, 3469, 18848, 4480, 28723, 13, 28781, 28723, 2326, 272, 4211, 1565, 28725, 6046, 356, 272, 4211, 28742, 28713, 5587, 325, 490, 283, 28731, 9460, 297, 272, 1830, 28733, 1828, 6581, 28723, 13, 28782, 28723, 560, 272, 6472, 8357, 369, 8045, 28725, 913, 354, 396, 3551, 25430, 464, 4176, 4249, 28742, 442, 464, 4176, 18848, 4135, 13, 28784, 28723, 1047, 2632, 28725, 5339, 464, 8398, 13461, 3469, 356, 18848, 4135, 13, 28787, 28723, 15226, 272, 4435, 304, 25203, 272, 13079, 28748, 11636, 28715, 13079, 2884, 298, 1032, 272, 2030, 28723, 13, 13, 3381, 368, 28742, 267, 2461, 7598, 7484, 272, 5587, 28725, 272, 1489, 1970, 298, 511, 349, 3295, 298, 574, 7335, 28742, 28713, 12905, 28725, 1854, 272, 4723, 304, 3870, 288, 302, 6472, 541, 11204, 1444, 18978, 28723, 2, 28705, 13, 28789, 28766, 1838, 28766, 28767, 13, 6325, 368, 3084, 528, 395, 264, 3062, 298, 272, 12905, 354, 586, 7335, 28804, 2, 28705, 13, 28789, 28766, 489, 11143, 28766, 28767, 13, 28737, 949, 28742, 28707, 506, 2735, 298, 574, 4143, 28742, 28713, 7335, 1871, 28723, 2993, 28725, 368, 541, 4312, 1300, 272, 12905, 354, 574, 7335, 486, 1404, 298, 272, 5126, 1575, 7335, 4143, 28725, 7484, 574, 7335, 304, 24675, 356, 272, 464, 6658, 28742, 3062, 5651, 297, 272, 5859, 1103, 6581, 302, 272, 2884, 28723, 16677, 6308, 28725, 368, 541, 511, 264, 15682, 3472, 354, 272, 1141, 302, 574, 7335, 4961, 486, 464, 7200, 352, 28742, 442, 464, 1838, 8327, 4135, 2, 28705, 13, 28789, 28766, 1838, 28766, 28767, 13, 6325, 368, 8735, 513, 456, 4480, 835, 3791, 354, 272, 16652, 14142, 4211, 302, 586, 7335, 28804, 2, 28705, 13, 28789, 28766, 489, 11143, 28766, 28767, 13, 1014, 13461, 3469, 18848, 4480, 993, 442, 993, 459, 771, 354, 574, 16652, 14142, 4211, 28725, 10085, 356, 272, 7427, 302, 574, 7335, 28723, 2909, 18978, 3024, 456, 4480, 297, 272, 16652, 14142, 4211, 486, 2369, 28725, 1312, 2663, 993, 2699, 4870, 2460, 1837, 28723, 1791, 1877, 513, 456, 4480, 349, 2632, 354, 272, 16652, 14142, 4211, 302, 574, 7335, 28725, 1372, 1167, 5944, 28747, 13, 13, 28740, 28723, 3187, 298, 272, 16652, 14142, 4211, 970, 368, 682, 737, 298, 8234, 272, 4480, 28723, 28705, 28750, 28723, 10213, 356, 272, 16652, 14142, 6472, 9460, 325, 490, 283, 9460, 28731, 304, 913, 354, 464, 4176, 4249, 28742, 442, 464, 4176, 18848, 4135, 28705, 28770, 28723, 1047, 2632, 28725, 5339, 464, 8398, 13461, 3469, 356, 18848, 4135, 28705, 28781, 28723, 15226, 272, 4435, 28723, 1047, 456, 3551, 349, 459, 2632, 297, 574, 16652, 14142, 4211, 6472, 28725, 368, 993, 927, 298, 4563, 575, 298, 574, 7335, 21782, 354, 11611, 395, 2460, 3864, 574, 16652, 14142, 4211, 298, 3024, 456, 4480, 28723, 2, 28705, 13]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_removed = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col not in [\"input_ids\", \"attention_mask\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages', 'input_ids', 'attention_mask', 'chat_format'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(tokenized_dataset_removed, **dataloader_params)\n",
    "\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 26588, 28725,  ...,     2, 28705,    13],\n",
       "        [    1,  9305, 28731,  ...,     2, 28705,    13],\n",
       "        [    1, 18413,   302,  ...,     2, 28705,    13],\n",
       "        ...,\n",
       "        [    2,     2,     2,  ...,     2, 28705,    13],\n",
       "        [    1,   829,   427,  ...,     2, 28705,    13],\n",
       "        [    2,     2,     2,  ...,     2, 28705,    13]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  -100, 28705,    13],\n",
       "        [ -100,  -100,  -100,  ...,  -100, 28705,    13],\n",
       "        [ -100,  -100,  -100,  ...,  -100, 28705,    13],\n",
       "        ...,\n",
       "        [ -100,  -100,  -100,  ...,  -100, 28705,    13],\n",
       "        [ -100,  -100,  -100,  ...,  -100, 28705,    13],\n",
       "        [ -100,  -100,  -100,  ...,  -100, 28705,    13]])}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 351\n",
    "collator.response_token_ids == batch[\"input_ids\"][0][idx : idx + len(collator.response_token_ids)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28789, 28766,   489, 11143, 28766, 28767,    13, 13617,   404,   356])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"][0][351:351+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch[\"input_ids\"][0][:1000] == 28789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train_sft\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|user|>\\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?</s>\\n<|assistant|>\\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.</s>\\n<|user|>\\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?</s>\\n<|assistant|>\\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.</s>\\n<|user|>\\nCan you provide me with a link to the documentation for my theme?</s>\\n<|assistant|>\\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.</s>\\n<|user|>\\nCan you confirm if this feature also works for the Quick Shop section of my theme?</s>\\n<|assistant|>\\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.</s>\\n\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[0][\"chat_format\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 732, 28711, 28789, 28766, 489, 11143, 28766, 28767]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 523, 28766, 1838, 28766, 28767, 13, 18171, 11382, 5580, 298, 4211, 28733, 5527, 18978, 325, 1146, 13532, 495, 28705, 28784, 28723, 28734, 28806, 28725, 8337, 1380, 28705, 28781, 28723, 28734, 28806, 28725, 2316, 455, 897, 28705, 28770, 28723, 28734, 28806, 6372, 1798, 28705, 28750, 28723, 28734, 28806, 28725, 351, 598, 16712, 28705, 28782, 28723, 28734, 28806, 609, 1824, 7335, 2751, 837, 315, 1413, 28804, 13, 2486, 574, 27395, 6718, 567, 22114, 28715, 27395, 12458, 28725, 368, 541, 5061, 1347, 272, 13461, 3469, 302, 264, 2093, 356, 18848, 486, 25748, 624, 302, 272, 7335, 28742, 28713, 4429, 28733, 262, 6472, 28808, 13, 11159, 13079, 6718, 567, 22114, 28715, 27395, 12458, 622, 1055, 4249, 272, 13461, 2093, 3469, 776, 486, 18848, 288, 754, 369, 2093, 3469, 15762, 21418, 28723, 13, 20510, 456, 4480, 5580, 298, 544, 12458, 302, 272, 7335, 442, 776, 2948, 4413, 390, 9206, 297, 272, 2245, 3388, 28804, 2, 28705, 13, 28789, 28766, 489, 11143, 28766, 28767, 13, 3260, 4480, 865, 15588, 298, 13079, 6718, 304, 22114, 28715, 27395, 12458, 302, 272, 4211, 28733, 5527, 18978, 9206, 297, 272, 2245, 3388, 28723, 2, 28705, 13, 28789, 28766, 1838, 28766, 28767, 13, 6325, 368, 8327, 528, 1059, 272, 1759, 302, 25748, 272, 13461, 3469, 18848, 4480, 356, 586, 13079, 6718, 304, 22114, 28715, 27395, 12458, 28804, 2, 28705, 13, 28789, 28766, 489, 11143, 28766, 28767, 13, 22099, 28725, 1236, 460, 272, 5944, 298, 8234, 272, 13461, 3469, 18848, 4480, 356, 574, 13079, 6718, 304, 22114, 28715, 27395, 12458, 28747, 13, 13, 28740, 28723, 5545, 297, 298, 574, 14142, 1575, 2708, 304, 576, 298, 574, 10634, 13229, 28723, 13, 28750, 28723, 10213, 356, 7973, 653, 7335, 354, 272, 4211, 28733, 5527, 7335, 368, 460, 1413, 28723, 13, 28770, 28723, 9363, 11590, 298, 272, 13079, 6718, 442, 22114, 28715, 27395, 4211, 970, 368, 947, 298, 8234, 272, 13461, 3469, 18848, 4480, 28723, 13, 28781, 28723, 2326, 272, 4211, 1565, 28725, 6046, 356, 272, 4211, 28742, 28713, 5587, 325, 490, 283, 28731, 9460, 297, 272, 1830, 28733, 1828, 6581, 28723, 13, 28782, 28723, 560, 272, 6472, 8357, 369, 8045, 28725, 913, 354, 396, 3551, 25430, 464, 4176, 4249, 28742, 442, 464, 4176, 18848, 4135, 13, 28784, 28723, 1047, 2632, 28725, 5339, 464, 8398, 13461, 3469, 356, 18848, 4135, 13, 28787, 28723, 15226, 272, 4435, 304, 25203, 272, 13079, 28748, 11636, 28715, 13079, 2884, 298, 1032, 272, 2030, 28723, 13, 13, 3381, 368, 28742, 267, 2461, 7598, 7484, 272, 5587, 28725, 272, 1489, 1970, 298, 511, 349, 3295, 298, 574, 7335, 28742, 28713, 12905, 28725, 1854, 272, 4723, 304, 3870, 288, 302, 6472, 541, 11204, 1444, 18978, 28723, 2, 28705, 13, 28789, 28766, 1838, 28766, 28767, 13, 6325, 368, 3084, 528, 395, 264, 3062, 298, 272, 12905, 354, 586, 7335, 28804, 2, 28705, 13, 28789, 28766, 489, 11143, 28766, 28767, 13, 28737, 949, 28742, 28707, 506, 2735, 298, 574, 4143, 28742, 28713, 7335, 1871, 28723, 2993, 28725, 368, 541, 4312, 1300, 272, 12905, 354, 574, 7335, 486, 1404, 298, 272, 5126, 1575, 7335, 4143, 28725, 7484, 574, 7335, 304, 24675, 356, 272, 464, 6658, 28742, 3062, 5651, 297, 272, 5859, 1103, 6581, 302, 272, 2884, 28723, 16677, 6308, 28725, 368, 541, 511, 264, 15682, 3472, 354, 272, 1141, 302, 574, 7335, 4961, 486, 464, 7200, 352, 28742, 442, 464, 1838, 8327, 4135, 2, 28705, 13, 28789, 28766, 1838, 28766, 28767, 13, 6325, 368, 8735, 513, 456, 4480, 835, 3791, 354, 272, 16652, 14142, 4211, 302, 586, 7335, 28804, 2, 28705, 13, 28789, 28766, 489, 11143, 28766, 28767, 13, 1014, 13461, 3469, 18848, 4480, 993, 442, 993, 459, 771, 354, 574, 16652, 14142, 4211, 28725, 10085, 356, 272, 7427, 302, 574, 7335, 28723, 2909, 18978, 3024, 456, 4480, 297, 272, 16652, 14142, 4211, 486, 2369, 28725, 1312, 2663, 993, 2699, 4870, 2460, 1837, 28723, 1791, 1877, 513, 456, 4480, 349, 2632, 354, 272, 16652, 14142, 4211, 302, 574, 7335, 28725, 1372, 1167, 5944, 28747, 13, 13, 28740, 28723, 3187, 298, 272, 16652, 14142, 4211, 970, 368, 682, 737, 298, 8234, 272, 4480, 28723, 28705, 28750, 28723, 10213, 356, 272, 16652, 14142, 6472, 9460, 325, 490, 283, 9460, 28731, 304, 913, 354, 464, 4176, 4249, 28742, 442, 464, 4176, 18848, 4135, 28705, 28770, 28723, 1047, 2632, 28725, 5339, 464, 8398, 13461, 3469, 356, 18848, 4135, 28705, 28781, 28723, 15226, 272, 4435, 28723, 1047, 456, 3551, 349, 459, 2632, 297, 574, 16652, 14142, 4211, 6472, 28725, 368, 993, 927, 298, 4563, 575, 298, 574, 7335, 21782, 354, 11611, 395, 2460, 3864, 574, 16652, 14142, 4211, 298, 3024, 456, 4480, 28723, 2, 28705, 13]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
