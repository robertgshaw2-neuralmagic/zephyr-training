{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/home/paperspace/zephyr-training/data/llama-7b-chat-50sparse-int8',)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/paperspace/zephyr-training/evaluation/generation-example.ipynb Cell 1\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/zephyr-training/evaluation/generation-example.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/zephyr-training/evaluation/generation-example.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/paperspace/zephyr-training/data/llama-7b-chat-50sparse-int8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/zephyr-training/evaluation/generation-example.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m SparseAutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/zephyr-training/evaluation/generation-example.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     model_path, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/zephyr-training/evaluation/generation-example.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/zephyr-training/evaluation/generation-example.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/zephyr-training/evaluation/generation-example.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpaperspace/home/paperspace/zephyr-training/evaluation/generation-example.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m~/zephyr-training/evaluation/sparseautomodel.py:16\u001b[0m, in \u001b[0;36mSparseAutoModelForCausalLM.from_pretrained\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mloading model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     18\u001b[0m     model_id \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m     recipe_file \u001b[39m=\u001b[39m model_id \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/recipe.yaml\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/zephyr-training/env/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m~/zephyr-training/env/lib/python3.9/site-packages/transformers/modeling_utils.py:3307\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3297\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3298\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3300\u001b[0m     (\n\u001b[1;32m   3301\u001b[0m         model,\n\u001b[1;32m   3302\u001b[0m         missing_keys,\n\u001b[1;32m   3303\u001b[0m         unexpected_keys,\n\u001b[1;32m   3304\u001b[0m         mismatched_keys,\n\u001b[1;32m   3305\u001b[0m         offload_index,\n\u001b[1;32m   3306\u001b[0m         error_msgs,\n\u001b[0;32m-> 3307\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3308\u001b[0m         model,\n\u001b[1;32m   3309\u001b[0m         state_dict,\n\u001b[1;32m   3310\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3311\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3312\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3313\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3314\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3315\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3316\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3317\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3318\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3319\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3320\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3321\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3322\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3323\u001b[0m     )\n\u001b[1;32m   3325\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3326\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/zephyr-training/env/lib/python3.9/site-packages/transformers/modeling_utils.py:3559\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3557\u001b[0m     set_initialized_submodules(model, _loaded_keys)\n\u001b[1;32m   3558\u001b[0m     \u001b[39m# This will only initialize submodules that are not marked as initialized by the line above.\u001b[39;00m\n\u001b[0;32m-> 3559\u001b[0m     model\u001b[39m.\u001b[39;49mapply(model\u001b[39m.\u001b[39;49m_initialize_weights)\n\u001b[1;32m   3561\u001b[0m \u001b[39m# Set some modules to fp32 if any\u001b[39;00m\n\u001b[1;32m   3562\u001b[0m \u001b[39mif\u001b[39;00m keep_in_fp32_modules \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/zephyr-training/env/lib/python3.9/site-packages/torch/nn/modules/module.py:884\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m \n\u001b[1;32m    882\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 884\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[1;32m    885\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[1;32m    886\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/zephyr-training/env/lib/python3.9/site-packages/torch/nn/modules/module.py:884\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m \n\u001b[1;32m    882\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 884\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[1;32m    885\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[1;32m    886\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.apply at line 884 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/zephyr-training/env/lib/python3.9/site-packages/torch/nn/modules/module.py:884\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m \n\u001b[1;32m    882\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 884\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[1;32m    885\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[1;32m    886\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/zephyr-training/env/lib/python3.9/site-packages/torch/nn/modules/module.py:885\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m    884\u001b[0m     module\u001b[39m.\u001b[39mapply(fn)\n\u001b[0;32m--> 885\u001b[0m fn(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    886\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/zephyr-training/env/lib/python3.9/site-packages/transformers/modeling_utils.py:1388\u001b[0m, in \u001b[0;36mPreTrainedModel._initialize_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, \u001b[39m\"\u001b[39m\u001b[39m_is_hf_initialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1387\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_weights(module)\n\u001b[1;32m   1389\u001b[0m module\u001b[39m.\u001b[39m_is_hf_initialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/zephyr-training/env/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:743\u001b[0m, in \u001b[0;36mLlamaPreTrainedModel._init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    741\u001b[0m std \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39minitializer_range\n\u001b[1;32m    742\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, nn\u001b[39m.\u001b[39mLinear):\n\u001b[0;32m--> 743\u001b[0m     module\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mnormal_(mean\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, std\u001b[39m=\u001b[39;49mstd)\n\u001b[1;32m    744\u001b[0m     \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    745\u001b[0m         module\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mzero_()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sparseautomodel import SparseAutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"/home/paperspace/zephyr-training/data/llama-7b-chat-50sparse-int8\"\n",
    "model = SparseAutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(tokenizer, dataset, messages_column_name=\"messages\", num_prompts=1):\n",
    "    messages = dataset.shuffle().select(range(num_prompts))[messages_column_name]\n",
    "    system_prompt = {\n",
    "        \"content\": \"\",\n",
    "        \"role\": \"system\"\n",
    "    }\n",
    "    prompts = []\n",
    "    for message in messages:\n",
    "        convo = [system_prompt] + message[:-1]\n",
    "        prompt = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=True)\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-training/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"/home/paperspace/zephyr-training/data/llama-7b-chat-50sparse-int8\"\n",
    "dataset_path = \"HuggingFaceH4/ultrachat_200k\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-training/evaluation\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def print_layer_sparsity(model: torch.nn.Module) -> None:\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            mask = torch.where(module.weight == 0, torch.tensor(0, dtype=torch.uint8), torch.tensor(1, dtype=torch.uint8))\n",
    "            print(f\"[Layer {name} sparsity = {torch.sum(mask == 0)/mask.numel()}]\")\n",
    "        else:\n",
    "            print_layer_sparsity(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Layer q_proj sparsity = 0.5000032782554626]\n",
      "[Layer k_proj sparsity = 0.5000081062316895]\n",
      "[Layer v_proj sparsity = 0.5000081062316895]\n",
      "[Layer o_proj sparsity = 0.5000024437904358]\n",
      "[Layer gate_proj sparsity = 0.5000011324882507]\n",
      "[Layer up_proj sparsity = 0.500001072883606]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000019073486328]\n",
      "[Layer k_proj sparsity = 0.5000078678131104]\n",
      "[Layer v_proj sparsity = 0.5000078678131104]\n",
      "[Layer o_proj sparsity = 0.5000019669532776]\n",
      "[Layer gate_proj sparsity = 0.5000009536743164]\n",
      "[Layer up_proj sparsity = 0.5000007748603821]\n",
      "[Layer down_proj sparsity = 0.5000020861625671]\n",
      "[Layer q_proj sparsity = 0.500002384185791]\n",
      "[Layer k_proj sparsity = 0.5000083446502686]\n",
      "[Layer v_proj sparsity = 0.5000076293945312]\n",
      "[Layer o_proj sparsity = 0.5000024437904358]\n",
      "[Layer gate_proj sparsity = 0.5000009536743164]\n",
      "[Layer up_proj sparsity = 0.500001072883606]\n",
      "[Layer down_proj sparsity = 0.5000019073486328]\n",
      "[Layer q_proj sparsity = 0.5000022053718567]\n",
      "[Layer k_proj sparsity = 0.5000076293945312]\n",
      "[Layer v_proj sparsity = 0.5000081062316895]\n",
      "[Layer o_proj sparsity = 0.5000020861625671]\n",
      "[Layer gate_proj sparsity = 0.5000009536743164]\n",
      "[Layer up_proj sparsity = 0.5000007152557373]\n",
      "[Layer down_proj sparsity = 0.5000019669532776]\n",
      "[Layer q_proj sparsity = 0.5000021457672119]\n",
      "[Layer k_proj sparsity = 0.5000083446502686]\n",
      "[Layer v_proj sparsity = 0.5000083446502686]\n",
      "[Layer o_proj sparsity = 0.5000020265579224]\n",
      "[Layer gate_proj sparsity = 0.5000005960464478]\n",
      "[Layer up_proj sparsity = 0.5000008940696716]\n",
      "[Layer down_proj sparsity = 0.5000019073486328]\n",
      "[Layer q_proj sparsity = 0.5000020265579224]\n",
      "[Layer k_proj sparsity = 0.5000081062316895]\n",
      "[Layer v_proj sparsity = 0.5000078678131104]\n",
      "[Layer o_proj sparsity = 0.5000022649765015]\n",
      "[Layer gate_proj sparsity = 0.5000012516975403]\n",
      "[Layer up_proj sparsity = 0.500001072883606]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000025033950806]\n",
      "[Layer k_proj sparsity = 0.5000076293945312]\n",
      "[Layer v_proj sparsity = 0.5000076293945312]\n",
      "[Layer o_proj sparsity = 0.5000026226043701]\n",
      "[Layer gate_proj sparsity = 0.5000007152557373]\n",
      "[Layer up_proj sparsity = 0.5000010132789612]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000019669532776]\n",
      "[Layer k_proj sparsity = 0.5000078678131104]\n",
      "[Layer v_proj sparsity = 0.5000083446502686]\n",
      "[Layer o_proj sparsity = 0.5000021457672119]\n",
      "[Layer gate_proj sparsity = 0.5000008344650269]\n",
      "[Layer up_proj sparsity = 0.5000006556510925]\n",
      "[Layer down_proj sparsity = 0.5000019669532776]\n",
      "[Layer q_proj sparsity = 0.5000020861625671]\n",
      "[Layer k_proj sparsity = 0.5000083446502686]\n",
      "[Layer v_proj sparsity = 0.5000076293945312]\n",
      "[Layer o_proj sparsity = 0.5000019669532776]\n",
      "[Layer gate_proj sparsity = 0.5000011324882507]\n",
      "[Layer up_proj sparsity = 0.5000008940696716]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000019073486328]\n",
      "[Layer k_proj sparsity = 0.5000076293945312]\n",
      "[Layer v_proj sparsity = 0.5000078678131104]\n",
      "[Layer o_proj sparsity = 0.5000020861625671]\n",
      "[Layer gate_proj sparsity = 0.5000008940696716]\n",
      "[Layer up_proj sparsity = 0.5000008940696716]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000022649765015]\n",
      "[Layer k_proj sparsity = 0.5000081062316895]\n",
      "[Layer v_proj sparsity = 0.5000085830688477]\n",
      "[Layer o_proj sparsity = 0.5000019073486328]\n",
      "[Layer gate_proj sparsity = 0.5000011920928955]\n",
      "[Layer up_proj sparsity = 0.5000008344650269]\n",
      "[Layer down_proj sparsity = 0.5000021457672119]\n",
      "[Layer q_proj sparsity = 0.5000020265579224]\n",
      "[Layer k_proj sparsity = 0.5000078678131104]\n",
      "[Layer v_proj sparsity = 0.5000078678131104]\n",
      "[Layer o_proj sparsity = 0.5000021457672119]\n",
      "[Layer gate_proj sparsity = 0.5000008344650269]\n",
      "[Layer up_proj sparsity = 0.5000007152557373]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000019669532776]\n",
      "[Layer k_proj sparsity = 0.5000081062316895]\n",
      "[Layer v_proj sparsity = 0.5000078678131104]\n",
      "[Layer o_proj sparsity = 0.5000027418136597]\n",
      "[Layer gate_proj sparsity = 0.5000009536743164]\n",
      "[Layer up_proj sparsity = 0.5000008940696716]\n",
      "[Layer down_proj sparsity = 0.5000021457672119]\n",
      "[Layer q_proj sparsity = 0.5000025033950806]\n",
      "[Layer k_proj sparsity = 0.5000083446502686]\n",
      "[Layer v_proj sparsity = 0.5000076293945312]\n",
      "[Layer o_proj sparsity = 0.5000026822090149]\n",
      "[Layer gate_proj sparsity = 0.5000007748603821]\n",
      "[Layer up_proj sparsity = 0.5000012516975403]\n",
      "[Layer down_proj sparsity = 0.5000019669532776]\n",
      "[Layer q_proj sparsity = 0.5000019073486328]\n",
      "[Layer k_proj sparsity = 0.5000076293945312]\n",
      "[Layer v_proj sparsity = 0.5000083446502686]\n",
      "[Layer o_proj sparsity = 0.5000022053718567]\n",
      "[Layer gate_proj sparsity = 0.5000006556510925]\n",
      "[Layer up_proj sparsity = 0.5000011324882507]\n",
      "[Layer down_proj sparsity = 0.5000020861625671]\n",
      "[Layer q_proj sparsity = 0.500002384185791]\n",
      "[Layer k_proj sparsity = 0.5000078678131104]\n",
      "[Layer v_proj sparsity = 0.5000081062316895]\n",
      "[Layer o_proj sparsity = 0.5000019073486328]\n",
      "[Layer gate_proj sparsity = 0.5000008344650269]\n",
      "[Layer up_proj sparsity = 0.5000011324882507]\n",
      "[Layer down_proj sparsity = 0.5000019073486328]\n",
      "[Layer q_proj sparsity = 0.5000024437904358]\n",
      "[Layer k_proj sparsity = 0.5000076293945312]\n",
      "[Layer v_proj sparsity = 0.5000076293945312]\n",
      "[Layer o_proj sparsity = 0.5000021457672119]\n",
      "[Layer gate_proj sparsity = 0.5000007748603821]\n",
      "[Layer up_proj sparsity = 0.5000007152557373]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000024437904358]\n",
      "[Layer k_proj sparsity = 0.5000078678131104]\n",
      "[Layer v_proj sparsity = 0.5000076293945312]\n",
      "[Layer o_proj sparsity = 0.5000022649765015]\n",
      "[Layer gate_proj sparsity = 0.5000011920928955]\n",
      "[Layer up_proj sparsity = 0.5000014305114746]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000020265579224]\n",
      "[Layer k_proj sparsity = 0.5000081062316895]\n",
      "[Layer v_proj sparsity = 0.5000081062316895]\n",
      "[Layer o_proj sparsity = 0.5000020265579224]\n",
      "[Layer gate_proj sparsity = 0.5000008940696716]\n",
      "[Layer up_proj sparsity = 0.5000007152557373]\n",
      "[Layer down_proj sparsity = 0.5000021457672119]\n",
      "[Layer q_proj sparsity = 0.5000025629997253]\n",
      "[Layer k_proj sparsity = 0.5000076293945312]\n",
      "[Layer v_proj sparsity = 0.5000081062316895]\n",
      "[Layer o_proj sparsity = 0.5000026226043701]\n",
      "[Layer gate_proj sparsity = 0.5000008940696716]\n",
      "[Layer up_proj sparsity = 0.5000012516975403]\n",
      "[Layer down_proj sparsity = 0.5000019073486328]\n",
      "[Layer q_proj sparsity = 0.5000023245811462]\n",
      "[Layer k_proj sparsity = 0.5000078678131104]\n",
      "[Layer v_proj sparsity = 0.5000081062316895]\n",
      "[Layer o_proj sparsity = 0.5000022649765015]\n",
      "[Layer gate_proj sparsity = 0.5000006556510925]\n",
      "[Layer up_proj sparsity = 0.5000009536743164]\n",
      "[Layer down_proj sparsity = 0.5000019669532776]\n",
      "[Layer q_proj sparsity = 0.5000025033950806]\n",
      "[Layer k_proj sparsity = 0.5000081062316895]\n",
      "[Layer v_proj sparsity = 0.5000078678131104]\n",
      "[Layer o_proj sparsity = 0.5000025033950806]\n",
      "[Layer gate_proj sparsity = 0.5000011324882507]\n",
      "[Layer up_proj sparsity = 0.5000005960464478]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000028014183044]\n",
      "[Layer k_proj sparsity = 0.5000081062316895]\n",
      "[Layer v_proj sparsity = 0.5000076293945312]\n",
      "[Layer o_proj sparsity = 0.5000019073486328]\n",
      "[Layer gate_proj sparsity = 0.5000011324882507]\n",
      "[Layer up_proj sparsity = 0.5000007152557373]\n",
      "[Layer down_proj sparsity = 0.5000019073486328]\n",
      "[Layer q_proj sparsity = 0.5000022649765015]\n",
      "[Layer k_proj sparsity = 0.5000076293945312]\n",
      "[Layer v_proj sparsity = 0.5000085830688477]\n",
      "[Layer o_proj sparsity = 0.5000019073486328]\n",
      "[Layer gate_proj sparsity = 0.5000006556510925]\n",
      "[Layer up_proj sparsity = 0.5000011920928955]\n",
      "[Layer down_proj sparsity = 0.5000020861625671]\n",
      "[Layer q_proj sparsity = 0.5000019669532776]\n",
      "[Layer k_proj sparsity = 0.5000078678131104]\n",
      "[Layer v_proj sparsity = 0.5000081062316895]\n",
      "[Layer o_proj sparsity = 0.5000020861625671]\n",
      "[Layer gate_proj sparsity = 0.5000007152557373]\n",
      "[Layer up_proj sparsity = 0.5000008344650269]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000023245811462]\n",
      "[Layer k_proj sparsity = 0.5000083446502686]\n",
      "[Layer v_proj sparsity = 0.5000078678131104]\n",
      "[Layer o_proj sparsity = 0.500002384185791]\n",
      "[Layer gate_proj sparsity = 0.5000011324882507]\n",
      "[Layer up_proj sparsity = 0.500001072883606]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000022053718567]\n",
      "[Layer k_proj sparsity = 0.5000081062316895]\n",
      "[Layer v_proj sparsity = 0.5000078678131104]\n",
      "[Layer o_proj sparsity = 0.5000019669532776]\n",
      "[Layer gate_proj sparsity = 0.5000007748603821]\n",
      "[Layer up_proj sparsity = 0.5000008344650269]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000019669532776]\n",
      "[Layer k_proj sparsity = 0.5000078678131104]\n",
      "[Layer v_proj sparsity = 0.5000081062316895]\n",
      "[Layer o_proj sparsity = 0.5000019073486328]\n",
      "[Layer gate_proj sparsity = 0.5000011920928955]\n",
      "[Layer up_proj sparsity = 0.5000011324882507]\n",
      "[Layer down_proj sparsity = 0.5000020861625671]\n",
      "[Layer q_proj sparsity = 0.5000020861625671]\n",
      "[Layer k_proj sparsity = 0.5000088214874268]\n",
      "[Layer v_proj sparsity = 0.5000076293945312]\n",
      "[Layer o_proj sparsity = 0.5000020265579224]\n",
      "[Layer gate_proj sparsity = 0.5000011920928955]\n",
      "[Layer up_proj sparsity = 0.5000008344650269]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000020265579224]\n",
      "[Layer k_proj sparsity = 0.5000081062316895]\n",
      "[Layer v_proj sparsity = 0.5000076293945312]\n",
      "[Layer o_proj sparsity = 0.5000025033950806]\n",
      "[Layer gate_proj sparsity = 0.500001072883606]\n",
      "[Layer up_proj sparsity = 0.5000005960464478]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000023245811462]\n",
      "[Layer k_proj sparsity = 0.5000081062316895]\n",
      "[Layer v_proj sparsity = 0.5000078678131104]\n",
      "[Layer o_proj sparsity = 0.5000022053718567]\n",
      "[Layer gate_proj sparsity = 0.5000011324882507]\n",
      "[Layer up_proj sparsity = 0.5000008344650269]\n",
      "[Layer down_proj sparsity = 0.5000020265579224]\n",
      "[Layer q_proj sparsity = 0.5000024437904358]\n",
      "[Layer k_proj sparsity = 0.5000088214874268]\n",
      "[Layer v_proj sparsity = 0.5000078678131104]\n",
      "[Layer o_proj sparsity = 0.5000020861625671]\n",
      "[Layer gate_proj sparsity = 0.5000008344650269]\n",
      "[Layer up_proj sparsity = 0.5000008940696716]\n",
      "[Layer down_proj sparsity = 0.5000021457672119]\n",
      "[Layer lm_head sparsity = 5.882263394596521e-06]\n"
     ]
    }
   ],
   "source": [
    "print_layer_sparsity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Layer lm_head sparsity = 5.882263394596521e-06]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_path, split=\"train_sft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|system|>\n",
      "You are a helpful chatbot</s> \n",
      "<|user|>\n",
      "Here is a piece of text: Tour of the crop trials at the AHDB's Strategic Potato (SPot) Farm initiative at the Elveden Estate. Pictured: Agronomist Graham Tomalin explains the herbicide trials. Picture: Chris Hill.\n",
      "East Anglia’s potato-growing knowhow is being coupled with the region’s scientific expertise in a bid to improve crop performance across the sector.\n",
      "Visitors were invited to a farm walk at the Elveden Estate to discuss the progress of crop trials in the third year of the Strategic Potato (SPot) Farm initiative, co-ordinated by the Agriculture and Horticulture Development Board’s potatoes division (AHDB Potatoes).\n",
      "They were told that while academic research and anecdotal evidence from farmers can both give valuable insights – neither can provide the whole answer to improve yields and soil health.\n",
      "So the SPot Farm project combines research trials and demonstrations with industry best practice at exemplars such as the 22,500-acre Elveden Estate near Thetford – and allows other growers in to assess which approaches would work within their own system, soil type and climate.\n",
      "Dr Elizabeth Stockdale, head of farming systems at NIAB (National Institute of Agricultural Botany), said: “It is a really tricky thing for scientists to unpick anecdotal evidence. Farmers will tell us that something worked, but we struggle to put a number on it.\n",
      "“So scientists need to go to that farm and see what is happening. We know there is stuff we can do in research trials, and things we cannot. What we cannot do is look at systems where people have changed more than two or three things. If you change your cultivation system you may also change your timings and your seed variety. You change a lot of things in farming but research trials, when properly done, are rubbish at that.\n",
      "Dr Mark Stalham of NIAB CUF (Cambridge University Farm) introduced several trials and demonstrations at the farm including an assessment of the effect of compost before a potato crop. The trial field is split into thirds – one given a standard dose of 43t/ha, one given double at 86t/ha, and one receiving none at all.\n",
      "Graham Tomalin, an agronomist for Vegetable Consultancy Services (VCS) also outlined trials for residual and contact herbicides, new varieties and chemicals to combat potato cyst nematodes (PCN), and “trap cropping” – the use of other plants to attract pests away from the main commercial crop.\n",
      "Visitors were also shown trials for a new herbicide called Aclonifen, which agro-chemical firm Bayer hopes will gain UK approval next year to fill part of the gap left by Linuron, a “mainstay” chemical for potato protection, which failed to win re-approval earlier this year.\n",
      "“We have been doing a lot of work with residual chemicals, due to the impending loss of Linuron, which has changed the whole perspective on sandy lands,” said Mr Tomalin.\n",
      "The herbicide trials include one plot where 26 potato varieties have been planted in parallel rows, which were cross-sprayed in perpendicular lines with 18 different chemical treatments – creating 468 mini trial plots to demonstrate the result of each combination.\n",
      "Due to the delayed cultivations after a wet start to spring, Mr Tomalin said many of the Breckland weeds have not yet appeared, so the full effect of these treatments will be better assessed by the time of Elveden’s SPot Farm Open Day on July 6.\n",
      "\n",
      "Based on the text material above, generate the response to the following quesion or instruction: What is the significance of combining academic research and industry best practice in the SPot Farm project?</s> \n",
      "<|assistant|>\n",
      "The SPot Farm project combines academic research and industry best practice in order to improve crop performance across the potato-growing sector. This initiative allows other growers to assess which approaches would work within their own system, soil type, and climate. While academic research and anecdotal evidence from farmers can both give valuable insights, neither can provide the whole answer to improve yields and soil health. Therefore, the SPot Farm project combines research trials and demonstrations with industry best practice to provide a comprehensive solution.</s> \n",
      "<|user|>\n",
      "Can you provide more details on the trials for residual and contact herbicides, new varieties and chemicals to combat potato cyst nematodes (PCN), and “trap cropping” discussed at the farm walk?</s> \n",
      "<|assistant|>\n",
      "At the farm walk, agronomist Graham Tomalin outlined trials for residual and contact herbicides, new varieties and chemicals to combat potato cyst nematodes (PCN), and “trap cropping” – the use of other plants to attract pests away from the main commercial crop. The trials for residual and contact herbicides were conducted due to the impending loss of Linuron, a “mainstay” chemical for potato protection, which failed to win re-approval earlier this year. The trials included one plot where 26 potato varieties were planted in parallel rows, which were cross-sprayed in perpendicular lines with 18 different chemical treatments – creating 468 mini trial plots to demonstrate the result of each combination. The trials for new varieties and chemicals to combat PCN were not further described in the text. The “trap cropping” trial involved the use of other plants to attract pests away from the main commercial crop.</s>\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "prompts = generate_prompts(tokenizer, dataset, num_prompts=1)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for prompt in prompts:\n",
    "    inps = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    for key in inps:\n",
    "        inps[key] = inps[key].to(\"cuda\")\n",
    "    \n",
    "    outputs_tokens = model.generate(**inps, max_new_tokens=1000)\n",
    "\n",
    "    print(tokenizer.batch_decode(outputs_tokens)[0])\n",
    "    print(\"\\n\\n\\n\")\n",
    "torch.cuda.synchronize()\n",
    "end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens = 1266\n",
      "new_tokens = 213\n",
      "num_seconds = 10.281893671490252\n",
      "new tok/sec = 20.7160282731389\n"
     ]
    }
   ],
   "source": [
    "start_tokens = inps[\"input_ids\"].shape[1]\n",
    "total_tokens = outputs_tokens.shape[1]\n",
    "\n",
    "print(f\"total_tokens = {total_tokens}\")\n",
    "print(f\"new_tokens = {total_tokens - start_tokens}\")\n",
    "print(f\"num_seconds = {end - start}\")\n",
    "print(f\"new tok/sec = {(total_tokens - start_tokens) / (end - start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-training/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map={'':0}\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/zephyr-training/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fastchat.model.model_adapter.ZephyrBetaAdapter object at 0x7fe693d9c850>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:27<00:00,  3.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from fastchat.model import load_model, get_conversation_template\n",
    "import torch\n",
    "\n",
    "# model_path = \"/home/rshaw/zephyr-training/pruning/data/zephyr-50sparse-fp16-one-shot-v0\"\n",
    "model_path = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "model_id = \"zephyr-beta\"\n",
    "\n",
    "model, tokenizer = load_model(\n",
    "    model_path,\n",
    "    device=\"cuda\",\n",
    "    num_gpus=1,\n",
    "    dtype=torch.float16,\n",
    "    load_8bit=False,\n",
    "    cpu_offloading=False,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_id': 83, 'category': 'writing', 'turns': ['Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.', 'Take your previous response and rephrase it as a limerick.']}\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from FastChat.fastchat.llm_judge.common import load_questions, temperature_config\n",
    "\n",
    "question_file = \"FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl\"\n",
    "questions = load_questions(question_file, None, None)\n",
    "question = questions[2]\n",
    "print(question)\n",
    "\n",
    "conv = get_conversation_template(model_id)\n",
    "\n",
    "turns = []\n",
    "for j in range(len(question[\"turns\"])):\n",
    "    qs = question[\"turns\"][j]\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    do_sample=True\n",
    "    temperature=0.7\n",
    "    print(prompt)\n",
    "    \n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        torch.as_tensor(input_ids).cuda(),\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|system|>\n",
      "</s> \n",
      "<|user|>\n",
      "Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.</s> \n",
      "<|assistant|>\n",
      "Title: Smartphone Showdown: iPhone 12 vs. Samsung Galaxy S21\n",
      "\n",
      "Introduction:\n",
      "\n",
      "Smartphones have become a vital part of our daily lives, and with new models coming out every year, it can be overwhelming to decide which one to pick. In this blog post, we will compare two popular smartphone models, the iPhone 12, and Samsung Galaxy S21. We will take a closer look at their features, performance, and user experience to help you make an informed decision.\n",
      "\n",
      "1. Design and Display\n",
      "\n",
      "- iPhone 12: The iPhone 12 comes with a new design, featuring a flat-edged rectangular shape. It has a 6.1-inch Super Retina XDR OLED display with a 2532 x 1170 resolution. The display also supports HDR content and Dolby Vision.\n",
      "- Samsung Galaxy S21: The Samsung Galaxy S21 also comes with a modern design, featuring a sleek, flat-screen design. It has a 6.2-inch Dynamic AMOLED 2X display with a 2400 x 1080 resolution. The display also supports HDR10+ content.\n",
      "\n",
      "2. Processor and Performance\n",
      "\n",
      "- iPhone 12: The iPhone 12 is powered by the A14 Bionic chip, which is a 64-bit, six-core processor. It also has a four-core GPU and 4GB of RAM. The A14 Bionic chip is known for its fast performance and efficiency.\n",
      "- Samsung Galaxy S21: The Samsung Galaxy S21 is powered by the Snapdragon 888 processor in the US and the Exynos 2100 processor in other regions. It also has an eight-core GPU and 8GB of RAM. The Snapdragon 888 processor is known for its fast performance and excellent gaming capabilities.\n",
      "\n",
      "3. Camera\n",
      "\n",
      "- iPhone 12: The iPhone 12 comes with a dual-camera setup, featuring a 12-megapixel ultra-wide-angle lens and a 12-megapixel wide-angle lens. The camera also supports Night mode, Deep Fusion, and ProRAW.\n",
      "- Samsung Galaxy S21: The Samsung Galaxy S21 comes with a triple-camera setup, featuring a 12-megapixel wide-angle lens, a 12-megapixel ultra-wide-angle lens, and a 64-megapixel telephoto lens. The camera also supports Night mode and Pro mode.\n",
      "\n",
      "4. Battery Life and Charging\n",
      "\n",
      "- iPhone 12: The iPhone 12 has a 2815mAh battery, which is smaller than the iPhone 11's battery. It also supports 20W fast charging (through a USB-C to Lightning cable and adapter sold separately) and wireless charging.\n",
      "- Samsung Galaxy S21: The Samsung Galaxy S21 has a 4000mAh battery, which is larger than the Galaxy S20's battery. It also supports 25W fast charging and wireless charging.\n",
      "\n",
      "5. User Experience\n",
      "\n",
      "- iPhone 12: The iPhone 12 runs on iOS 14, which is known for its simplicity and user-friendliness. It also has a responsive touch screen and Face ID for secure biometric authentication.\n",
      "- Samsung Galaxy S21: The Samsung Galaxy S21 runs on Android 11, which is customizable and user-friendly. It also has a responsive touch screen and an in-display fingerprint sensor for secure biometric authentication.\n",
      "\n",
      "Conclusion:\n",
      "\n",
      "In conclusion, both the iPhone 12 and Samsung Galaxy S21 are excellent smartphone models, and choosing between them depends on your personal preferences. If you prefer iOS and a smaller battery, the iPhone 12 might be the better choice. If you prefer Android and a larger battery, the Samsung Galaxy S21 might be the better choice. Ultimately, both smartphones offer top-of-the-line features, performance, and user experience.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(output_ids)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
